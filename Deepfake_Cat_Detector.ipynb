{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JL5tIX4c9z6s"
      },
      "source": [
        "<div style=\"margin-bottom: 15px; padding: 15px; color: #31708f; background-color: #d9edf7; border: 1px solid #bce8f1; border-radius: 5px;\">\n",
        "    \n",
        "<b><font size=+2>Enter your information below:</font></b></br></br>\n",
        "\n",
        "  <b>(full) Name</b>: Vismaya Anand Bolbandi\n",
        "  </br>\n",
        "  <b>Student ID Number</b>:  862548529\n",
        "  </br></br>\n",
        "    \n",
        "<b>By submitting this notebook, I assert that the work below is my own work, completed for this course.  Except where explicitly cited, none of the portions of this notebook are duplicated from anyone else's work or my own previous work.</b>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hsywQWEI8pzj"
      },
      "source": [
        "# Overview\n",
        "In this assignment you will implement some classifiers to predict whether or not images of cats are \"deepfakes\", i.e., generated by AI. (I used SD 1.5, and down-sampled to match CIFAR-10, which we use for real images.)\n",
        "\n",
        "For this assignment we will use the functionality of PyTorch, HuggingFace \"transformers\" library for getting pretrained models, scikit-learn (for cross validation utility and for baseline logistic regression), matplotlib for visualization. Before you start, make sure you have installed all those packages in your local Jupyter instance. Or use Google Colab (which has everything you need pre-installed).\n",
        "\n",
        "Read **all** cells carefully and answer **all** parts (both text and missing code). You will complete all the parts marked `TODO` and print desired results. (In some cases, this just means getting the code to work so the TODO section prints the correct result.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NM7-Cx4H-jTt"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "# The following functions are discussed in week 5 demo\n",
        "import torch.nn as nn  # neural net layers and activations\n",
        "from torch.optim import SGD  # Our chosen optimizer\n",
        "from torch.utils.data import DataLoader, TensorDataset  # Super useful data utilities!\n",
        "\n",
        "# We discussed all these in week 4 demo:\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Used for visualization\n",
        "import torchvision.utils as vutils\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "# Turn off some annoying convergence warnings from sklearn\n",
        "from warnings import simplefilter\n",
        "from sklearn.exceptions import ConvergenceWarning\n",
        "simplefilter(\"ignore\", category=ConvergenceWarning)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VR37r18FtEPo"
      },
      "source": [
        "## **Obtain and inspect data [3 points]**\n",
        "You can download the data file here:\n",
        " https://elearn.ucr.edu/courses/169673/files/17302822/download?download_frd=1\n",
        "You'll have to make them available locally or upload them to your colab instance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p2-zIA5vYOHA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 820
        },
        "outputId": "11ffc025-e68a-4b2f-b70a-b3c4ee9c192f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-39-c3db27e095f2>:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  X, y = torch.load('hw2_data.pt')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data shapes before flattening:\n",
            "X: torch.Size([2000, 3, 32, 32])\n",
            "y: torch.Size([2000])\n",
            "X shape after flattening: torch.Size([2000, 3072])\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x800 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Load dataset and visualize\n",
        "X, y = torch.load('hw2_data.pt')\n",
        "\n",
        "print('Data shapes before flattening:')\n",
        "print('X:', X.shape)  # 2000, 3, 32, 32, 2000 images, channel, height width\n",
        "print('y:', y.shape)  # 2000 binary labels 0 is real, 1 is fake\n",
        "\n",
        "# Print examples from each class\n",
        "grid = vutils.make_grid(X[y==0][:8], nrow=4, padding=2, normalize=True)\n",
        "fig, axs = plt.subplots(2, 1, figsize=(8, 8))\n",
        "axs[0].axis('off')\n",
        "axs[0].set_title('REAL Cat images')\n",
        "axs[0].imshow(grid.numpy().transpose((1, 2, 0)))\n",
        "\n",
        "grid = vutils.make_grid(X[y==1][:8], nrow=4, padding=2, normalize=True)\n",
        "axs[1].axis('off')\n",
        "axs[1].set_title('FAKE Cat images')\n",
        "axs[1].imshow(grid.numpy().transpose((1, 2, 0)))\n",
        "\n",
        "\n",
        "X = X.flatten(start_dim=1)  # From now on, we work with the flattened vector\n",
        "print(f\"X shape after flattening: {X.shape}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M4ZHC6mqM8kt",
        "outputId": "f9667509-83bd-4eb9-84bc-4d55e7ea3cbe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy and standard error:\t 0.992 +/- 0.001\n",
            "Validation Accuracy and standard error:\t 0.633 +/- 0.011\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import KFold\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "import torch\n",
        "\n",
        "n_folds = 5\n",
        "kf = KFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
        "\n",
        "val_accs = []  # Store validation accuracy for each fold\n",
        "train_accs = []  # Store training accuracy for each fold\n",
        "\n",
        "# Iterate over folds\n",
        "for train_index, val_index in kf.split(X):\n",
        "    X_train, X_val = X[train_index], X[val_index]\n",
        "    y_train, y_val = y[train_index], y[val_index]\n",
        "\n",
        "    # Fit logistic regression model on training data\n",
        "    model = LogisticRegression(max_iter=1000)\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Compute and store accuracy on train data\n",
        "    train_acc = accuracy_score(y_train, model.predict(X_train))\n",
        "    train_accs.append(train_acc)\n",
        "\n",
        "    # Compute and store accuracy on validation data\n",
        "    val_acc = accuracy_score(y_val, model.predict(X_val))\n",
        "    val_accs.append(val_acc)\n",
        "\n",
        "# Compute mean and standard deviation of accuracies\n",
        "train_std, train_mean = torch.std_mean(torch.tensor(train_accs))\n",
        "val_std, val_mean = torch.std_mean(torch.tensor(val_accs))\n",
        "\n",
        "# Standard error is standard deviation / sqrt(n_folds)\n",
        "rootn = torch.sqrt(torch.tensor(n_folds))\n",
        "print(f'Train Accuracy and standard error:\\t {train_mean:.3f} +/- {train_std / rootn:.3f}')\n",
        "print(f'Validation Accuracy and standard error:\\t {val_mean:.3f} +/- {val_std / rootn:.3f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hVNKfNQKM8kt"
      },
      "source": [
        "## Define the model [3 points]\n",
        "\n",
        "- As always, implement an __init__ function and a forward function\n",
        "- Use Linear layers with ReLU activations for the hidden layers\n",
        "- 2 layers of hidden units. First layer has 128 hidden units, second layer has 64 hidden units.\n",
        "- Output represents *binary* logits (must have correct shape to do that!)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "265qk4h2YUrc"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class MyMLP(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MyMLP, self).__init__()\n",
        "\n",
        "        # Define the layers\n",
        "        self.layer1 = nn.Linear(in_features=3*32*32, out_features=128)  # First hidden layer (128 units)\n",
        "        self.layer2 = nn.Linear(in_features=128, out_features=64)  # Second hidden layer (64 units)\n",
        "        self.output = nn.Linear(in_features=64, out_features=1)  # Output layer (binary classification)\n",
        "\n",
        "        # Define the activation function\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Flatten the input to 1D vector (batch_size, 3*32*32)\n",
        "        x = x.view(x.size(0), -1)  # Flatten the input image\n",
        "\n",
        "        # Apply the first hidden layer and ReLU activation\n",
        "        x = self.relu(self.layer1(x))\n",
        "\n",
        "        # Apply the second hidden layer and ReLU activation\n",
        "        x = self.relu(self.layer2(x))\n",
        "\n",
        "        # Output layer (logits for binary classification)\n",
        "        x = self.output(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tUdMenOwM8kt"
      },
      "source": [
        "## Train function [6 points]\n",
        "Make a function to train your neural net, following week 5 example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pvXYTplTM8kt"
      },
      "outputs": [],
      "source": [
        " import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "def train(model, train_loader, val_loader, n_epochs, optimizer, criterion, verbose=False):\n",
        "    \"\"\"Train model using data from train_loader over n_epochs,\n",
        "    using a Pytorch \"optimizer\" object (e.g., Adam or SGD) and \"criterion\" as the loss function.\n",
        "    \"\"\"\n",
        "    for epoch in range(n_epochs):\n",
        "        # Set the model to training mode\n",
        "        model.train()\n",
        "\n",
        "        # Training loop\n",
        "        running_loss = 0.0\n",
        "        for inputs, labels in train_loader:\n",
        "            optimizer.zero_grad()  # Clear previous gradients\n",
        "\n",
        "            # Forward pass: Compute predicted y by passing x to the model\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs.squeeze(), labels.float())  # Calculate the loss\n",
        "            loss.backward()  # Backpropagate the loss\n",
        "            optimizer.step()  # Update the weights\n",
        "\n",
        "            running_loss += loss.item()  # Accumulate loss\n",
        "\n",
        "        # After each epoch, print the training loss (optional)\n",
        "        print(f\"Epoch [{epoch+1}/{n_epochs}], Training loss: {running_loss / len(train_loader):.4f}\")\n",
        "\n",
        "        if verbose:\n",
        "            # Set the model to evaluation mode to prevent gradient calculation\n",
        "            model.eval()\n",
        "\n",
        "            # Validation loop (optional: check the performance on validation set)\n",
        "            val_loss = 0.0\n",
        "            correct_preds = 0\n",
        "            total_preds = 0\n",
        "            with torch.no_grad():  # No need to calculate gradients for validation\n",
        "                for inputs, labels in val_loader:\n",
        "                    # Forward pass: Compute predicted y by passing x to the model\n",
        "                    outputs = model(inputs)\n",
        "                    loss = criterion(outputs.squeeze(), labels.float())  # Calculate the loss\n",
        "                    val_loss += loss.item()  # Accumulate validation loss\n",
        "\n",
        "                    # Compute accuracy for the validation set\n",
        "                    preds = torch.sigmoid(outputs).round()  # Get binary predictions\n",
        "                    correct_preds += (preds.squeeze() == labels).sum().item()  # Count correct predictions\n",
        "                    total_preds += labels.size(0)  # Total samples\n",
        "\n",
        "            # Print validation loss and accuracy\n",
        "            val_accuracy = correct_preds / total_preds\n",
        "            print(f\"Validation loss: {val_loss / len(val_loader):.4f}, Validation accuracy: {val_accuracy:.4f}\")\n",
        "\n",
        "            # Set the model back to training mode for the next epoch\n",
        "            model.train()\n",
        "\n",
        "    print(\"Training complete.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JM4ijd56M8kt"
      },
      "source": [
        "Loop over hyper-parameters and do 5-fold cross-validation for each setting, saving the train and validation mean accuracy and standard error."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GFtknV8AM8kt",
        "outputId": "5f36d57f-28cb-4fa7-b590-83cffca0bc63"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-43-08011d4dd45a>:32: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  train_dataset = TensorDataset(torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.long))\n",
            "<ipython-input-43-08011d4dd45a>:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  val_dataset = TensorDataset(torch.tensor(X_val, dtype=torch.float32), torch.tensor(y_val, dtype=torch.long))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/100], Training loss: 0.6947\n",
            "Epoch [2/100], Training loss: 0.6937\n",
            "Epoch [3/100], Training loss: 0.6928\n",
            "Epoch [4/100], Training loss: 0.6919\n",
            "Epoch [5/100], Training loss: 0.6910\n",
            "Epoch [6/100], Training loss: 0.6901\n",
            "Epoch [7/100], Training loss: 0.6892\n",
            "Epoch [8/100], Training loss: 0.6883\n",
            "Epoch [9/100], Training loss: 0.6874\n",
            "Epoch [10/100], Training loss: 0.6865\n",
            "Epoch [11/100], Training loss: 0.6856\n",
            "Epoch [12/100], Training loss: 0.6847\n",
            "Epoch [13/100], Training loss: 0.6838\n",
            "Epoch [14/100], Training loss: 0.6829\n",
            "Epoch [15/100], Training loss: 0.6821\n",
            "Epoch [16/100], Training loss: 0.6812\n",
            "Epoch [17/100], Training loss: 0.6803\n",
            "Epoch [18/100], Training loss: 0.6794\n",
            "Epoch [19/100], Training loss: 0.6786\n",
            "Epoch [20/100], Training loss: 0.6777\n",
            "Epoch [21/100], Training loss: 0.6768\n",
            "Epoch [22/100], Training loss: 0.6759\n",
            "Epoch [23/100], Training loss: 0.6751\n",
            "Epoch [24/100], Training loss: 0.6742\n",
            "Epoch [25/100], Training loss: 0.6732\n",
            "Epoch [26/100], Training loss: 0.6723\n",
            "Epoch [27/100], Training loss: 0.6714\n",
            "Epoch [28/100], Training loss: 0.6705\n",
            "Epoch [29/100], Training loss: 0.6695\n",
            "Epoch [30/100], Training loss: 0.6686\n",
            "Epoch [31/100], Training loss: 0.6676\n",
            "Epoch [32/100], Training loss: 0.6666\n",
            "Epoch [33/100], Training loss: 0.6656\n",
            "Epoch [34/100], Training loss: 0.6646\n",
            "Epoch [35/100], Training loss: 0.6636\n",
            "Epoch [36/100], Training loss: 0.6626\n",
            "Epoch [37/100], Training loss: 0.6615\n",
            "Epoch [38/100], Training loss: 0.6605\n",
            "Epoch [39/100], Training loss: 0.6594\n",
            "Epoch [40/100], Training loss: 0.6583\n",
            "Epoch [41/100], Training loss: 0.6572\n",
            "Epoch [42/100], Training loss: 0.6561\n",
            "Epoch [43/100], Training loss: 0.6549\n",
            "Epoch [44/100], Training loss: 0.6538\n",
            "Epoch [45/100], Training loss: 0.6526\n",
            "Epoch [46/100], Training loss: 0.6514\n",
            "Epoch [47/100], Training loss: 0.6502\n",
            "Epoch [48/100], Training loss: 0.6490\n",
            "Epoch [49/100], Training loss: 0.6478\n",
            "Epoch [50/100], Training loss: 0.6466\n",
            "Epoch [51/100], Training loss: 0.6453\n",
            "Epoch [52/100], Training loss: 0.6440\n",
            "Epoch [53/100], Training loss: 0.6427\n",
            "Epoch [54/100], Training loss: 0.6414\n",
            "Epoch [55/100], Training loss: 0.6401\n",
            "Epoch [56/100], Training loss: 0.6387\n",
            "Epoch [57/100], Training loss: 0.6374\n",
            "Epoch [58/100], Training loss: 0.6360\n",
            "Epoch [59/100], Training loss: 0.6346\n",
            "Epoch [60/100], Training loss: 0.6331\n",
            "Epoch [61/100], Training loss: 0.6317\n",
            "Epoch [62/100], Training loss: 0.6302\n",
            "Epoch [63/100], Training loss: 0.6288\n",
            "Epoch [64/100], Training loss: 0.6273\n",
            "Epoch [65/100], Training loss: 0.6257\n",
            "Epoch [66/100], Training loss: 0.6242\n",
            "Epoch [67/100], Training loss: 0.6227\n",
            "Epoch [68/100], Training loss: 0.6211\n",
            "Epoch [69/100], Training loss: 0.6195\n",
            "Epoch [70/100], Training loss: 0.6179\n",
            "Epoch [71/100], Training loss: 0.6163\n",
            "Epoch [72/100], Training loss: 0.6147\n",
            "Epoch [73/100], Training loss: 0.6131\n",
            "Epoch [74/100], Training loss: 0.6114\n",
            "Epoch [75/100], Training loss: 0.6097\n",
            "Epoch [76/100], Training loss: 0.6080\n",
            "Epoch [77/100], Training loss: 0.6064\n",
            "Epoch [78/100], Training loss: 0.6047\n",
            "Epoch [79/100], Training loss: 0.6029\n",
            "Epoch [80/100], Training loss: 0.6012\n",
            "Epoch [81/100], Training loss: 0.5995\n",
            "Epoch [82/100], Training loss: 0.5977\n",
            "Epoch [83/100], Training loss: 0.5959\n",
            "Epoch [84/100], Training loss: 0.5942\n",
            "Epoch [85/100], Training loss: 0.5924\n",
            "Epoch [86/100], Training loss: 0.5906\n",
            "Epoch [87/100], Training loss: 0.5888\n",
            "Epoch [88/100], Training loss: 0.5869\n",
            "Epoch [89/100], Training loss: 0.5851\n",
            "Epoch [90/100], Training loss: 0.5833\n",
            "Epoch [91/100], Training loss: 0.5814\n",
            "Epoch [92/100], Training loss: 0.5796\n",
            "Epoch [93/100], Training loss: 0.5777\n",
            "Epoch [94/100], Training loss: 0.5759\n",
            "Epoch [95/100], Training loss: 0.5740\n",
            "Epoch [96/100], Training loss: 0.5721\n",
            "Epoch [97/100], Training loss: 0.5702\n",
            "Epoch [98/100], Training loss: 0.5684\n",
            "Epoch [99/100], Training loss: 0.5665\n",
            "Epoch [100/100], Training loss: 0.5646\n",
            "Training complete.\n",
            "Epoch [1/100], Training loss: 0.6936\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-43-08011d4dd45a>:49: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  train_preds = model(torch.tensor(X_train, dtype=torch.float32))\n",
            "<ipython-input-43-08011d4dd45a>:55: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  val_preds = model(torch.tensor(X_val, dtype=torch.float32))\n",
            "<ipython-input-43-08011d4dd45a>:32: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  train_dataset = TensorDataset(torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.long))\n",
            "<ipython-input-43-08011d4dd45a>:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  val_dataset = TensorDataset(torch.tensor(X_val, dtype=torch.float32), torch.tensor(y_val, dtype=torch.long))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [2/100], Training loss: 0.6924\n",
            "Epoch [3/100], Training loss: 0.6912\n",
            "Epoch [4/100], Training loss: 0.6900\n",
            "Epoch [5/100], Training loss: 0.6887\n",
            "Epoch [6/100], Training loss: 0.6875\n",
            "Epoch [7/100], Training loss: 0.6863\n",
            "Epoch [8/100], Training loss: 0.6851\n",
            "Epoch [9/100], Training loss: 0.6838\n",
            "Epoch [10/100], Training loss: 0.6826\n",
            "Epoch [11/100], Training loss: 0.6814\n",
            "Epoch [12/100], Training loss: 0.6801\n",
            "Epoch [13/100], Training loss: 0.6789\n",
            "Epoch [14/100], Training loss: 0.6776\n",
            "Epoch [15/100], Training loss: 0.6764\n",
            "Epoch [16/100], Training loss: 0.6752\n",
            "Epoch [17/100], Training loss: 0.6740\n",
            "Epoch [18/100], Training loss: 0.6727\n",
            "Epoch [19/100], Training loss: 0.6715\n",
            "Epoch [20/100], Training loss: 0.6702\n",
            "Epoch [21/100], Training loss: 0.6690\n",
            "Epoch [22/100], Training loss: 0.6677\n",
            "Epoch [23/100], Training loss: 0.6665\n",
            "Epoch [24/100], Training loss: 0.6652\n",
            "Epoch [25/100], Training loss: 0.6639\n",
            "Epoch [26/100], Training loss: 0.6627\n",
            "Epoch [27/100], Training loss: 0.6614\n",
            "Epoch [28/100], Training loss: 0.6601\n",
            "Epoch [29/100], Training loss: 0.6588\n",
            "Epoch [30/100], Training loss: 0.6575\n",
            "Epoch [31/100], Training loss: 0.6562\n",
            "Epoch [32/100], Training loss: 0.6549\n",
            "Epoch [33/100], Training loss: 0.6536\n",
            "Epoch [34/100], Training loss: 0.6523\n",
            "Epoch [35/100], Training loss: 0.6509\n",
            "Epoch [36/100], Training loss: 0.6496\n",
            "Epoch [37/100], Training loss: 0.6482\n",
            "Epoch [38/100], Training loss: 0.6469\n",
            "Epoch [39/100], Training loss: 0.6455\n",
            "Epoch [40/100], Training loss: 0.6441\n",
            "Epoch [41/100], Training loss: 0.6427\n",
            "Epoch [42/100], Training loss: 0.6413\n",
            "Epoch [43/100], Training loss: 0.6399\n",
            "Epoch [44/100], Training loss: 0.6384\n",
            "Epoch [45/100], Training loss: 0.6370\n",
            "Epoch [46/100], Training loss: 0.6355\n",
            "Epoch [47/100], Training loss: 0.6341\n",
            "Epoch [48/100], Training loss: 0.6326\n",
            "Epoch [49/100], Training loss: 0.6312\n",
            "Epoch [50/100], Training loss: 0.6297\n",
            "Epoch [51/100], Training loss: 0.6282\n",
            "Epoch [52/100], Training loss: 0.6267\n",
            "Epoch [53/100], Training loss: 0.6252\n",
            "Epoch [54/100], Training loss: 0.6237\n",
            "Epoch [55/100], Training loss: 0.6222\n",
            "Epoch [56/100], Training loss: 0.6207\n",
            "Epoch [57/100], Training loss: 0.6192\n",
            "Epoch [58/100], Training loss: 0.6176\n",
            "Epoch [59/100], Training loss: 0.6161\n",
            "Epoch [60/100], Training loss: 0.6146\n",
            "Epoch [61/100], Training loss: 0.6130\n",
            "Epoch [62/100], Training loss: 0.6115\n",
            "Epoch [63/100], Training loss: 0.6099\n",
            "Epoch [64/100], Training loss: 0.6084\n",
            "Epoch [65/100], Training loss: 0.6068\n",
            "Epoch [66/100], Training loss: 0.6053\n",
            "Epoch [67/100], Training loss: 0.6037\n",
            "Epoch [68/100], Training loss: 0.6021\n",
            "Epoch [69/100], Training loss: 0.6005\n",
            "Epoch [70/100], Training loss: 0.5990\n",
            "Epoch [71/100], Training loss: 0.5974\n",
            "Epoch [72/100], Training loss: 0.5958\n",
            "Epoch [73/100], Training loss: 0.5942\n",
            "Epoch [74/100], Training loss: 0.5927\n",
            "Epoch [75/100], Training loss: 0.5911\n",
            "Epoch [76/100], Training loss: 0.5895\n",
            "Epoch [77/100], Training loss: 0.5879\n",
            "Epoch [78/100], Training loss: 0.5863\n",
            "Epoch [79/100], Training loss: 0.5847\n",
            "Epoch [80/100], Training loss: 0.5831\n",
            "Epoch [81/100], Training loss: 0.5815\n",
            "Epoch [82/100], Training loss: 0.5799\n",
            "Epoch [83/100], Training loss: 0.5783\n",
            "Epoch [84/100], Training loss: 0.5767\n",
            "Epoch [85/100], Training loss: 0.5751\n",
            "Epoch [86/100], Training loss: 0.5735\n",
            "Epoch [87/100], Training loss: 0.5719\n",
            "Epoch [88/100], Training loss: 0.5703\n",
            "Epoch [89/100], Training loss: 0.5687\n",
            "Epoch [90/100], Training loss: 0.5671\n",
            "Epoch [91/100], Training loss: 0.5655\n",
            "Epoch [92/100], Training loss: 0.5639\n",
            "Epoch [93/100], Training loss: 0.5623\n",
            "Epoch [94/100], Training loss: 0.5607\n",
            "Epoch [95/100], Training loss: 0.5591\n",
            "Epoch [96/100], Training loss: 0.5574\n",
            "Epoch [97/100], Training loss: 0.5558\n",
            "Epoch [98/100], Training loss: 0.5542\n",
            "Epoch [99/100], Training loss: 0.5526\n",
            "Epoch [100/100], Training loss: 0.5510\n",
            "Training complete.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-43-08011d4dd45a>:49: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  train_preds = model(torch.tensor(X_train, dtype=torch.float32))\n",
            "<ipython-input-43-08011d4dd45a>:55: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  val_preds = model(torch.tensor(X_val, dtype=torch.float32))\n",
            "<ipython-input-43-08011d4dd45a>:32: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  train_dataset = TensorDataset(torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.long))\n",
            "<ipython-input-43-08011d4dd45a>:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  val_dataset = TensorDataset(torch.tensor(X_val, dtype=torch.float32), torch.tensor(y_val, dtype=torch.long))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/100], Training loss: 0.6944\n",
            "Epoch [2/100], Training loss: 0.6930\n",
            "Epoch [3/100], Training loss: 0.6917\n",
            "Epoch [4/100], Training loss: 0.6905\n",
            "Epoch [5/100], Training loss: 0.6893\n",
            "Epoch [6/100], Training loss: 0.6881\n",
            "Epoch [7/100], Training loss: 0.6870\n",
            "Epoch [8/100], Training loss: 0.6859\n",
            "Epoch [9/100], Training loss: 0.6848\n",
            "Epoch [10/100], Training loss: 0.6838\n",
            "Epoch [11/100], Training loss: 0.6828\n",
            "Epoch [12/100], Training loss: 0.6818\n",
            "Epoch [13/100], Training loss: 0.6808\n",
            "Epoch [14/100], Training loss: 0.6797\n",
            "Epoch [15/100], Training loss: 0.6787\n",
            "Epoch [16/100], Training loss: 0.6777\n",
            "Epoch [17/100], Training loss: 0.6767\n",
            "Epoch [18/100], Training loss: 0.6757\n",
            "Epoch [19/100], Training loss: 0.6746\n",
            "Epoch [20/100], Training loss: 0.6736\n",
            "Epoch [21/100], Training loss: 0.6725\n",
            "Epoch [22/100], Training loss: 0.6715\n",
            "Epoch [23/100], Training loss: 0.6704\n",
            "Epoch [24/100], Training loss: 0.6693\n",
            "Epoch [25/100], Training loss: 0.6682\n",
            "Epoch [26/100], Training loss: 0.6670\n",
            "Epoch [27/100], Training loss: 0.6659\n",
            "Epoch [28/100], Training loss: 0.6647\n",
            "Epoch [29/100], Training loss: 0.6635\n",
            "Epoch [30/100], Training loss: 0.6623\n",
            "Epoch [31/100], Training loss: 0.6611\n",
            "Epoch [32/100], Training loss: 0.6599\n",
            "Epoch [33/100], Training loss: 0.6586\n",
            "Epoch [34/100], Training loss: 0.6574\n",
            "Epoch [35/100], Training loss: 0.6561\n",
            "Epoch [36/100], Training loss: 0.6548\n",
            "Epoch [37/100], Training loss: 0.6534\n",
            "Epoch [38/100], Training loss: 0.6521\n",
            "Epoch [39/100], Training loss: 0.6507\n",
            "Epoch [40/100], Training loss: 0.6493\n",
            "Epoch [41/100], Training loss: 0.6479\n",
            "Epoch [42/100], Training loss: 0.6465\n",
            "Epoch [43/100], Training loss: 0.6451\n",
            "Epoch [44/100], Training loss: 0.6436\n",
            "Epoch [45/100], Training loss: 0.6421\n",
            "Epoch [46/100], Training loss: 0.6406\n",
            "Epoch [47/100], Training loss: 0.6390\n",
            "Epoch [48/100], Training loss: 0.6375\n",
            "Epoch [49/100], Training loss: 0.6359\n",
            "Epoch [50/100], Training loss: 0.6343\n",
            "Epoch [51/100], Training loss: 0.6327\n",
            "Epoch [52/100], Training loss: 0.6310\n",
            "Epoch [53/100], Training loss: 0.6293\n",
            "Epoch [54/100], Training loss: 0.6276\n",
            "Epoch [55/100], Training loss: 0.6259\n",
            "Epoch [56/100], Training loss: 0.6242\n",
            "Epoch [57/100], Training loss: 0.6225\n",
            "Epoch [58/100], Training loss: 0.6207\n",
            "Epoch [59/100], Training loss: 0.6189\n",
            "Epoch [60/100], Training loss: 0.6171\n",
            "Epoch [61/100], Training loss: 0.6153\n",
            "Epoch [62/100], Training loss: 0.6135\n",
            "Epoch [63/100], Training loss: 0.6117\n",
            "Epoch [64/100], Training loss: 0.6098\n",
            "Epoch [65/100], Training loss: 0.6079\n",
            "Epoch [66/100], Training loss: 0.6061\n",
            "Epoch [67/100], Training loss: 0.6042\n",
            "Epoch [68/100], Training loss: 0.6022\n",
            "Epoch [69/100], Training loss: 0.6003\n",
            "Epoch [70/100], Training loss: 0.5984\n",
            "Epoch [71/100], Training loss: 0.5964\n",
            "Epoch [72/100], Training loss: 0.5945\n",
            "Epoch [73/100], Training loss: 0.5925\n",
            "Epoch [74/100], Training loss: 0.5906\n",
            "Epoch [75/100], Training loss: 0.5886\n",
            "Epoch [76/100], Training loss: 0.5866\n",
            "Epoch [77/100], Training loss: 0.5846\n",
            "Epoch [78/100], Training loss: 0.5826\n",
            "Epoch [79/100], Training loss: 0.5806\n",
            "Epoch [80/100], Training loss: 0.5787\n",
            "Epoch [81/100], Training loss: 0.5767\n",
            "Epoch [82/100], Training loss: 0.5747\n",
            "Epoch [83/100], Training loss: 0.5727\n",
            "Epoch [84/100], Training loss: 0.5707\n",
            "Epoch [85/100], Training loss: 0.5687\n",
            "Epoch [86/100], Training loss: 0.5667\n",
            "Epoch [87/100], Training loss: 0.5647\n",
            "Epoch [88/100], Training loss: 0.5627\n",
            "Epoch [89/100], Training loss: 0.5607\n",
            "Epoch [90/100], Training loss: 0.5587\n",
            "Epoch [91/100], Training loss: 0.5567\n",
            "Epoch [92/100], Training loss: 0.5547\n",
            "Epoch [93/100], Training loss: 0.5528\n",
            "Epoch [94/100], Training loss: 0.5508\n",
            "Epoch [95/100], Training loss: 0.5489\n",
            "Epoch [96/100], Training loss: 0.5469\n",
            "Epoch [97/100], Training loss: 0.5449\n",
            "Epoch [98/100], Training loss: 0.5430\n",
            "Epoch [99/100], Training loss: 0.5411\n",
            "Epoch [100/100], Training loss: 0.5392\n",
            "Training complete.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-43-08011d4dd45a>:49: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  train_preds = model(torch.tensor(X_train, dtype=torch.float32))\n",
            "<ipython-input-43-08011d4dd45a>:55: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  val_preds = model(torch.tensor(X_val, dtype=torch.float32))\n",
            "<ipython-input-43-08011d4dd45a>:32: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  train_dataset = TensorDataset(torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.long))\n",
            "<ipython-input-43-08011d4dd45a>:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  val_dataset = TensorDataset(torch.tensor(X_val, dtype=torch.float32), torch.tensor(y_val, dtype=torch.long))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/100], Training loss: 0.6923\n",
            "Epoch [2/100], Training loss: 0.6915\n",
            "Epoch [3/100], Training loss: 0.6908\n",
            "Epoch [4/100], Training loss: 0.6901\n",
            "Epoch [5/100], Training loss: 0.6894\n",
            "Epoch [6/100], Training loss: 0.6887\n",
            "Epoch [7/100], Training loss: 0.6881\n",
            "Epoch [8/100], Training loss: 0.6874\n",
            "Epoch [9/100], Training loss: 0.6868\n",
            "Epoch [10/100], Training loss: 0.6862\n",
            "Epoch [11/100], Training loss: 0.6856\n",
            "Epoch [12/100], Training loss: 0.6850\n",
            "Epoch [13/100], Training loss: 0.6844\n",
            "Epoch [14/100], Training loss: 0.6838\n",
            "Epoch [15/100], Training loss: 0.6832\n",
            "Epoch [16/100], Training loss: 0.6826\n",
            "Epoch [17/100], Training loss: 0.6820\n",
            "Epoch [18/100], Training loss: 0.6814\n",
            "Epoch [19/100], Training loss: 0.6808\n",
            "Epoch [20/100], Training loss: 0.6802\n",
            "Epoch [21/100], Training loss: 0.6796\n",
            "Epoch [22/100], Training loss: 0.6789\n",
            "Epoch [23/100], Training loss: 0.6783\n",
            "Epoch [24/100], Training loss: 0.6777\n",
            "Epoch [25/100], Training loss: 0.6770\n",
            "Epoch [26/100], Training loss: 0.6764\n",
            "Epoch [27/100], Training loss: 0.6757\n",
            "Epoch [28/100], Training loss: 0.6751\n",
            "Epoch [29/100], Training loss: 0.6744\n",
            "Epoch [30/100], Training loss: 0.6737\n",
            "Epoch [31/100], Training loss: 0.6730\n",
            "Epoch [32/100], Training loss: 0.6723\n",
            "Epoch [33/100], Training loss: 0.6716\n",
            "Epoch [34/100], Training loss: 0.6709\n",
            "Epoch [35/100], Training loss: 0.6701\n",
            "Epoch [36/100], Training loss: 0.6694\n",
            "Epoch [37/100], Training loss: 0.6686\n",
            "Epoch [38/100], Training loss: 0.6679\n",
            "Epoch [39/100], Training loss: 0.6671\n",
            "Epoch [40/100], Training loss: 0.6663\n",
            "Epoch [41/100], Training loss: 0.6655\n",
            "Epoch [42/100], Training loss: 0.6646\n",
            "Epoch [43/100], Training loss: 0.6638\n",
            "Epoch [44/100], Training loss: 0.6630\n",
            "Epoch [45/100], Training loss: 0.6621\n",
            "Epoch [46/100], Training loss: 0.6612\n",
            "Epoch [47/100], Training loss: 0.6603\n",
            "Epoch [48/100], Training loss: 0.6594\n",
            "Epoch [49/100], Training loss: 0.6585\n",
            "Epoch [50/100], Training loss: 0.6575\n",
            "Epoch [51/100], Training loss: 0.6566\n",
            "Epoch [52/100], Training loss: 0.6556\n",
            "Epoch [53/100], Training loss: 0.6546\n",
            "Epoch [54/100], Training loss: 0.6536\n",
            "Epoch [55/100], Training loss: 0.6526\n",
            "Epoch [56/100], Training loss: 0.6515\n",
            "Epoch [57/100], Training loss: 0.6504\n",
            "Epoch [58/100], Training loss: 0.6494\n",
            "Epoch [59/100], Training loss: 0.6483\n",
            "Epoch [60/100], Training loss: 0.6471\n",
            "Epoch [61/100], Training loss: 0.6460\n",
            "Epoch [62/100], Training loss: 0.6448\n",
            "Epoch [63/100], Training loss: 0.6437\n",
            "Epoch [64/100], Training loss: 0.6425\n",
            "Epoch [65/100], Training loss: 0.6412\n",
            "Epoch [66/100], Training loss: 0.6400\n",
            "Epoch [67/100], Training loss: 0.6387\n",
            "Epoch [68/100], Training loss: 0.6374\n",
            "Epoch [69/100], Training loss: 0.6361\n",
            "Epoch [70/100], Training loss: 0.6348\n",
            "Epoch [71/100], Training loss: 0.6335\n",
            "Epoch [72/100], Training loss: 0.6321\n",
            "Epoch [73/100], Training loss: 0.6307\n",
            "Epoch [74/100], Training loss: 0.6293\n",
            "Epoch [75/100], Training loss: 0.6279\n",
            "Epoch [76/100], Training loss: 0.6264\n",
            "Epoch [77/100], Training loss: 0.6249\n",
            "Epoch [78/100], Training loss: 0.6234\n",
            "Epoch [79/100], Training loss: 0.6219\n",
            "Epoch [80/100], Training loss: 0.6204\n",
            "Epoch [81/100], Training loss: 0.6188\n",
            "Epoch [82/100], Training loss: 0.6172\n",
            "Epoch [83/100], Training loss: 0.6156\n",
            "Epoch [84/100], Training loss: 0.6140\n",
            "Epoch [85/100], Training loss: 0.6124\n",
            "Epoch [86/100], Training loss: 0.6107\n",
            "Epoch [87/100], Training loss: 0.6090\n",
            "Epoch [88/100], Training loss: 0.6073\n",
            "Epoch [89/100], Training loss: 0.6056\n",
            "Epoch [90/100], Training loss: 0.6039\n",
            "Epoch [91/100], Training loss: 0.6021\n",
            "Epoch [92/100], Training loss: 0.6003\n",
            "Epoch [93/100], Training loss: 0.5985\n",
            "Epoch [94/100], Training loss: 0.5967\n",
            "Epoch [95/100], Training loss: 0.5949\n",
            "Epoch [96/100], Training loss: 0.5931\n",
            "Epoch [97/100], Training loss: 0.5912\n",
            "Epoch [98/100], Training loss: 0.5894\n",
            "Epoch [99/100], Training loss: 0.5875\n",
            "Epoch [100/100], Training loss: 0.5856\n",
            "Training complete.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-43-08011d4dd45a>:49: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  train_preds = model(torch.tensor(X_train, dtype=torch.float32))\n",
            "<ipython-input-43-08011d4dd45a>:55: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  val_preds = model(torch.tensor(X_val, dtype=torch.float32))\n",
            "<ipython-input-43-08011d4dd45a>:32: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  train_dataset = TensorDataset(torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.long))\n",
            "<ipython-input-43-08011d4dd45a>:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  val_dataset = TensorDataset(torch.tensor(X_val, dtype=torch.float32), torch.tensor(y_val, dtype=torch.long))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/100], Training loss: 0.6978\n",
            "Epoch [2/100], Training loss: 0.6967\n",
            "Epoch [3/100], Training loss: 0.6956\n",
            "Epoch [4/100], Training loss: 0.6946\n",
            "Epoch [5/100], Training loss: 0.6936\n",
            "Epoch [6/100], Training loss: 0.6927\n",
            "Epoch [7/100], Training loss: 0.6917\n",
            "Epoch [8/100], Training loss: 0.6908\n",
            "Epoch [9/100], Training loss: 0.6898\n",
            "Epoch [10/100], Training loss: 0.6889\n",
            "Epoch [11/100], Training loss: 0.6880\n",
            "Epoch [12/100], Training loss: 0.6871\n",
            "Epoch [13/100], Training loss: 0.6862\n",
            "Epoch [14/100], Training loss: 0.6853\n",
            "Epoch [15/100], Training loss: 0.6844\n",
            "Epoch [16/100], Training loss: 0.6836\n",
            "Epoch [17/100], Training loss: 0.6827\n",
            "Epoch [18/100], Training loss: 0.6818\n",
            "Epoch [19/100], Training loss: 0.6809\n",
            "Epoch [20/100], Training loss: 0.6800\n",
            "Epoch [21/100], Training loss: 0.6791\n",
            "Epoch [22/100], Training loss: 0.6782\n",
            "Epoch [23/100], Training loss: 0.6773\n",
            "Epoch [24/100], Training loss: 0.6764\n",
            "Epoch [25/100], Training loss: 0.6754\n",
            "Epoch [26/100], Training loss: 0.6745\n",
            "Epoch [27/100], Training loss: 0.6736\n",
            "Epoch [28/100], Training loss: 0.6726\n",
            "Epoch [29/100], Training loss: 0.6717\n",
            "Epoch [30/100], Training loss: 0.6707\n",
            "Epoch [31/100], Training loss: 0.6697\n",
            "Epoch [32/100], Training loss: 0.6688\n",
            "Epoch [33/100], Training loss: 0.6678\n",
            "Epoch [34/100], Training loss: 0.6668\n",
            "Epoch [35/100], Training loss: 0.6658\n",
            "Epoch [36/100], Training loss: 0.6647\n",
            "Epoch [37/100], Training loss: 0.6637\n",
            "Epoch [38/100], Training loss: 0.6626\n",
            "Epoch [39/100], Training loss: 0.6616\n",
            "Epoch [40/100], Training loss: 0.6605\n",
            "Epoch [41/100], Training loss: 0.6594\n",
            "Epoch [42/100], Training loss: 0.6584\n",
            "Epoch [43/100], Training loss: 0.6573\n",
            "Epoch [44/100], Training loss: 0.6561\n",
            "Epoch [45/100], Training loss: 0.6550\n",
            "Epoch [46/100], Training loss: 0.6539\n",
            "Epoch [47/100], Training loss: 0.6527\n",
            "Epoch [48/100], Training loss: 0.6516\n",
            "Epoch [49/100], Training loss: 0.6504\n",
            "Epoch [50/100], Training loss: 0.6492\n",
            "Epoch [51/100], Training loss: 0.6480\n",
            "Epoch [52/100], Training loss: 0.6467\n",
            "Epoch [53/100], Training loss: 0.6455\n",
            "Epoch [54/100], Training loss: 0.6442\n",
            "Epoch [55/100], Training loss: 0.6429\n",
            "Epoch [56/100], Training loss: 0.6416\n",
            "Epoch [57/100], Training loss: 0.6403\n",
            "Epoch [58/100], Training loss: 0.6390\n",
            "Epoch [59/100], Training loss: 0.6377\n",
            "Epoch [60/100], Training loss: 0.6363\n",
            "Epoch [61/100], Training loss: 0.6350\n",
            "Epoch [62/100], Training loss: 0.6336\n",
            "Epoch [63/100], Training loss: 0.6322\n",
            "Epoch [64/100], Training loss: 0.6308\n",
            "Epoch [65/100], Training loss: 0.6294\n",
            "Epoch [66/100], Training loss: 0.6279\n",
            "Epoch [67/100], Training loss: 0.6265\n",
            "Epoch [68/100], Training loss: 0.6250\n",
            "Epoch [69/100], Training loss: 0.6236\n",
            "Epoch [70/100], Training loss: 0.6221\n",
            "Epoch [71/100], Training loss: 0.6206\n",
            "Epoch [72/100], Training loss: 0.6191\n",
            "Epoch [73/100], Training loss: 0.6176\n",
            "Epoch [74/100], Training loss: 0.6161\n",
            "Epoch [75/100], Training loss: 0.6145\n",
            "Epoch [76/100], Training loss: 0.6130\n",
            "Epoch [77/100], Training loss: 0.6114\n",
            "Epoch [78/100], Training loss: 0.6099\n",
            "Epoch [79/100], Training loss: 0.6083\n",
            "Epoch [80/100], Training loss: 0.6067\n",
            "Epoch [81/100], Training loss: 0.6051\n",
            "Epoch [82/100], Training loss: 0.6035\n",
            "Epoch [83/100], Training loss: 0.6019\n",
            "Epoch [84/100], Training loss: 0.6003\n",
            "Epoch [85/100], Training loss: 0.5986\n",
            "Epoch [86/100], Training loss: 0.5970\n",
            "Epoch [87/100], Training loss: 0.5953\n",
            "Epoch [88/100], Training loss: 0.5937\n",
            "Epoch [89/100], Training loss: 0.5920\n",
            "Epoch [90/100], Training loss: 0.5904\n",
            "Epoch [91/100], Training loss: 0.5887\n",
            "Epoch [92/100], Training loss: 0.5870\n",
            "Epoch [93/100], Training loss: 0.5853\n",
            "Epoch [94/100], Training loss: 0.5836\n",
            "Epoch [95/100], Training loss: 0.5819\n",
            "Epoch [96/100], Training loss: 0.5802\n",
            "Epoch [97/100], Training loss: 0.5785\n",
            "Epoch [98/100], Training loss: 0.5768\n",
            "Epoch [99/100], Training loss: 0.5751\n",
            "Epoch [100/100], Training loss: 0.5734\n",
            "Training complete.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-43-08011d4dd45a>:49: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  train_preds = model(torch.tensor(X_train, dtype=torch.float32))\n",
            "<ipython-input-43-08011d4dd45a>:55: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  val_preds = model(torch.tensor(X_val, dtype=torch.float32))\n",
            "<ipython-input-43-08011d4dd45a>:32: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  train_dataset = TensorDataset(torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.long))\n",
            "<ipython-input-43-08011d4dd45a>:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  val_dataset = TensorDataset(torch.tensor(X_val, dtype=torch.float32), torch.tensor(y_val, dtype=torch.long))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/100], Training loss: 0.6974\n",
            "Epoch [2/100], Training loss: 0.6962\n",
            "Epoch [3/100], Training loss: 0.6951\n",
            "Epoch [4/100], Training loss: 0.6939\n",
            "Epoch [5/100], Training loss: 0.6928\n",
            "Epoch [6/100], Training loss: 0.6918\n",
            "Epoch [7/100], Training loss: 0.6907\n",
            "Epoch [8/100], Training loss: 0.6897\n",
            "Epoch [9/100], Training loss: 0.6887\n",
            "Epoch [10/100], Training loss: 0.6877\n",
            "Epoch [11/100], Training loss: 0.6868\n",
            "Epoch [12/100], Training loss: 0.6858\n",
            "Epoch [13/100], Training loss: 0.6848\n",
            "Epoch [14/100], Training loss: 0.6839\n",
            "Epoch [15/100], Training loss: 0.6829\n",
            "Epoch [16/100], Training loss: 0.6820\n",
            "Epoch [17/100], Training loss: 0.6810\n",
            "Epoch [18/100], Training loss: 0.6800\n",
            "Epoch [19/100], Training loss: 0.6791\n",
            "Epoch [20/100], Training loss: 0.6781\n",
            "Epoch [21/100], Training loss: 0.6771\n",
            "Epoch [22/100], Training loss: 0.6761\n",
            "Epoch [23/100], Training loss: 0.6751\n",
            "Epoch [24/100], Training loss: 0.6741\n",
            "Epoch [25/100], Training loss: 0.6731\n",
            "Epoch [26/100], Training loss: 0.6721\n",
            "Epoch [27/100], Training loss: 0.6711\n",
            "Epoch [28/100], Training loss: 0.6700\n",
            "Epoch [29/100], Training loss: 0.6690\n",
            "Epoch [30/100], Training loss: 0.6679\n",
            "Epoch [31/100], Training loss: 0.6669\n",
            "Epoch [32/100], Training loss: 0.6658\n",
            "Epoch [33/100], Training loss: 0.6647\n",
            "Epoch [34/100], Training loss: 0.6636\n",
            "Epoch [35/100], Training loss: 0.6625\n",
            "Epoch [36/100], Training loss: 0.6614\n",
            "Epoch [37/100], Training loss: 0.6603\n",
            "Epoch [38/100], Training loss: 0.6591\n",
            "Epoch [39/100], Training loss: 0.6580\n",
            "Epoch [40/100], Training loss: 0.6568\n",
            "Epoch [41/100], Training loss: 0.6556\n",
            "Epoch [42/100], Training loss: 0.6544\n",
            "Epoch [43/100], Training loss: 0.6532\n",
            "Epoch [44/100], Training loss: 0.6520\n",
            "Epoch [45/100], Training loss: 0.6508\n",
            "Epoch [46/100], Training loss: 0.6495\n",
            "Epoch [47/100], Training loss: 0.6483\n",
            "Epoch [48/100], Training loss: 0.6470\n",
            "Epoch [49/100], Training loss: 0.6457\n",
            "Epoch [50/100], Training loss: 0.6444\n",
            "Epoch [51/100], Training loss: 0.6431\n",
            "Epoch [52/100], Training loss: 0.6418\n",
            "Epoch [53/100], Training loss: 0.6404\n",
            "Epoch [54/100], Training loss: 0.6391\n",
            "Epoch [55/100], Training loss: 0.6377\n",
            "Epoch [56/100], Training loss: 0.6363\n",
            "Epoch [57/100], Training loss: 0.6349\n",
            "Epoch [58/100], Training loss: 0.6335\n",
            "Epoch [59/100], Training loss: 0.6320\n",
            "Epoch [60/100], Training loss: 0.6306\n",
            "Epoch [61/100], Training loss: 0.6291\n",
            "Epoch [62/100], Training loss: 0.6276\n",
            "Epoch [63/100], Training loss: 0.6262\n",
            "Epoch [64/100], Training loss: 0.6246\n",
            "Epoch [65/100], Training loss: 0.6231\n",
            "Epoch [66/100], Training loss: 0.6216\n",
            "Epoch [67/100], Training loss: 0.6201\n",
            "Epoch [68/100], Training loss: 0.6185\n",
            "Epoch [69/100], Training loss: 0.6170\n",
            "Epoch [70/100], Training loss: 0.6154\n",
            "Epoch [71/100], Training loss: 0.6138\n",
            "Epoch [72/100], Training loss: 0.6122\n",
            "Epoch [73/100], Training loss: 0.6106\n",
            "Epoch [74/100], Training loss: 0.6091\n",
            "Epoch [75/100], Training loss: 0.6074\n",
            "Epoch [76/100], Training loss: 0.6058\n",
            "Epoch [77/100], Training loss: 0.6042\n",
            "Epoch [78/100], Training loss: 0.6026\n",
            "Epoch [79/100], Training loss: 0.6010\n",
            "Epoch [80/100], Training loss: 0.5993\n",
            "Epoch [81/100], Training loss: 0.5977\n",
            "Epoch [82/100], Training loss: 0.5960\n",
            "Epoch [83/100], Training loss: 0.5944\n",
            "Epoch [84/100], Training loss: 0.5927\n",
            "Epoch [85/100], Training loss: 0.5911\n",
            "Epoch [86/100], Training loss: 0.5894\n",
            "Epoch [87/100], Training loss: 0.5878\n",
            "Epoch [88/100], Training loss: 0.5861\n",
            "Epoch [89/100], Training loss: 0.5844\n",
            "Epoch [90/100], Training loss: 0.5827\n",
            "Epoch [91/100], Training loss: 0.5810\n",
            "Epoch [92/100], Training loss: 0.5794\n",
            "Epoch [93/100], Training loss: 0.5777\n",
            "Epoch [94/100], Training loss: 0.5760\n",
            "Epoch [95/100], Training loss: 0.5743\n",
            "Epoch [96/100], Training loss: 0.5726\n",
            "Epoch [97/100], Training loss: 0.5709\n",
            "Epoch [98/100], Training loss: 0.5692\n",
            "Epoch [99/100], Training loss: 0.5675\n",
            "Epoch [100/100], Training loss: 0.5658\n",
            "Training complete.\n",
            "Epoch [1/100], Training loss: 0.6943\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-43-08011d4dd45a>:49: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  train_preds = model(torch.tensor(X_train, dtype=torch.float32))\n",
            "<ipython-input-43-08011d4dd45a>:55: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  val_preds = model(torch.tensor(X_val, dtype=torch.float32))\n",
            "<ipython-input-43-08011d4dd45a>:32: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  train_dataset = TensorDataset(torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.long))\n",
            "<ipython-input-43-08011d4dd45a>:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  val_dataset = TensorDataset(torch.tensor(X_val, dtype=torch.float32), torch.tensor(y_val, dtype=torch.long))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [2/100], Training loss: 0.6933\n",
            "Epoch [3/100], Training loss: 0.6924\n",
            "Epoch [4/100], Training loss: 0.6915\n",
            "Epoch [5/100], Training loss: 0.6905\n",
            "Epoch [6/100], Training loss: 0.6896\n",
            "Epoch [7/100], Training loss: 0.6887\n",
            "Epoch [8/100], Training loss: 0.6878\n",
            "Epoch [9/100], Training loss: 0.6869\n",
            "Epoch [10/100], Training loss: 0.6860\n",
            "Epoch [11/100], Training loss: 0.6851\n",
            "Epoch [12/100], Training loss: 0.6842\n",
            "Epoch [13/100], Training loss: 0.6834\n",
            "Epoch [14/100], Training loss: 0.6825\n",
            "Epoch [15/100], Training loss: 0.6817\n",
            "Epoch [16/100], Training loss: 0.6808\n",
            "Epoch [17/100], Training loss: 0.6799\n",
            "Epoch [18/100], Training loss: 0.6791\n",
            "Epoch [19/100], Training loss: 0.6782\n",
            "Epoch [20/100], Training loss: 0.6774\n",
            "Epoch [21/100], Training loss: 0.6765\n",
            "Epoch [22/100], Training loss: 0.6757\n",
            "Epoch [23/100], Training loss: 0.6749\n",
            "Epoch [24/100], Training loss: 0.6740\n",
            "Epoch [25/100], Training loss: 0.6732\n",
            "Epoch [26/100], Training loss: 0.6723\n",
            "Epoch [27/100], Training loss: 0.6715\n",
            "Epoch [28/100], Training loss: 0.6706\n",
            "Epoch [29/100], Training loss: 0.6698\n",
            "Epoch [30/100], Training loss: 0.6689\n",
            "Epoch [31/100], Training loss: 0.6680\n",
            "Epoch [32/100], Training loss: 0.6671\n",
            "Epoch [33/100], Training loss: 0.6663\n",
            "Epoch [34/100], Training loss: 0.6654\n",
            "Epoch [35/100], Training loss: 0.6644\n",
            "Epoch [36/100], Training loss: 0.6635\n",
            "Epoch [37/100], Training loss: 0.6626\n",
            "Epoch [38/100], Training loss: 0.6617\n",
            "Epoch [39/100], Training loss: 0.6607\n",
            "Epoch [40/100], Training loss: 0.6598\n",
            "Epoch [41/100], Training loss: 0.6588\n",
            "Epoch [42/100], Training loss: 0.6579\n",
            "Epoch [43/100], Training loss: 0.6569\n",
            "Epoch [44/100], Training loss: 0.6559\n",
            "Epoch [45/100], Training loss: 0.6548\n",
            "Epoch [46/100], Training loss: 0.6538\n",
            "Epoch [47/100], Training loss: 0.6528\n",
            "Epoch [48/100], Training loss: 0.6517\n",
            "Epoch [49/100], Training loss: 0.6507\n",
            "Epoch [50/100], Training loss: 0.6496\n",
            "Epoch [51/100], Training loss: 0.6485\n",
            "Epoch [52/100], Training loss: 0.6474\n",
            "Epoch [53/100], Training loss: 0.6462\n",
            "Epoch [54/100], Training loss: 0.6451\n",
            "Epoch [55/100], Training loss: 0.6439\n",
            "Epoch [56/100], Training loss: 0.6427\n",
            "Epoch [57/100], Training loss: 0.6415\n",
            "Epoch [58/100], Training loss: 0.6403\n",
            "Epoch [59/100], Training loss: 0.6391\n",
            "Epoch [60/100], Training loss: 0.6378\n",
            "Epoch [61/100], Training loss: 0.6366\n",
            "Epoch [62/100], Training loss: 0.6353\n",
            "Epoch [63/100], Training loss: 0.6340\n",
            "Epoch [64/100], Training loss: 0.6327\n",
            "Epoch [65/100], Training loss: 0.6313\n",
            "Epoch [66/100], Training loss: 0.6300\n",
            "Epoch [67/100], Training loss: 0.6286\n",
            "Epoch [68/100], Training loss: 0.6272\n",
            "Epoch [69/100], Training loss: 0.6258\n",
            "Epoch [70/100], Training loss: 0.6243\n",
            "Epoch [71/100], Training loss: 0.6229\n",
            "Epoch [72/100], Training loss: 0.6214\n",
            "Epoch [73/100], Training loss: 0.6199\n",
            "Epoch [74/100], Training loss: 0.6184\n",
            "Epoch [75/100], Training loss: 0.6169\n",
            "Epoch [76/100], Training loss: 0.6154\n",
            "Epoch [77/100], Training loss: 0.6138\n",
            "Epoch [78/100], Training loss: 0.6122\n",
            "Epoch [79/100], Training loss: 0.6106\n",
            "Epoch [80/100], Training loss: 0.6090\n",
            "Epoch [81/100], Training loss: 0.6074\n",
            "Epoch [82/100], Training loss: 0.6058\n",
            "Epoch [83/100], Training loss: 0.6041\n",
            "Epoch [84/100], Training loss: 0.6025\n",
            "Epoch [85/100], Training loss: 0.6008\n",
            "Epoch [86/100], Training loss: 0.5991\n",
            "Epoch [87/100], Training loss: 0.5974\n",
            "Epoch [88/100], Training loss: 0.5957\n",
            "Epoch [89/100], Training loss: 0.5940\n",
            "Epoch [90/100], Training loss: 0.5923\n",
            "Epoch [91/100], Training loss: 0.5906\n",
            "Epoch [92/100], Training loss: 0.5888\n",
            "Epoch [93/100], Training loss: 0.5871\n",
            "Epoch [94/100], Training loss: 0.5853\n",
            "Epoch [95/100], Training loss: 0.5836\n",
            "Epoch [96/100], Training loss: 0.5818\n",
            "Epoch [97/100], Training loss: 0.5801\n",
            "Epoch [98/100], Training loss: 0.5783\n",
            "Epoch [99/100], Training loss: 0.5765\n",
            "Epoch [100/100], Training loss: 0.5747\n",
            "Training complete.\n",
            "Epoch [1/100], Training loss: 0.6946\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-43-08011d4dd45a>:49: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  train_preds = model(torch.tensor(X_train, dtype=torch.float32))\n",
            "<ipython-input-43-08011d4dd45a>:55: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  val_preds = model(torch.tensor(X_val, dtype=torch.float32))\n",
            "<ipython-input-43-08011d4dd45a>:32: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  train_dataset = TensorDataset(torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.long))\n",
            "<ipython-input-43-08011d4dd45a>:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  val_dataset = TensorDataset(torch.tensor(X_val, dtype=torch.float32), torch.tensor(y_val, dtype=torch.long))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [2/100], Training loss: 0.6939\n",
            "Epoch [3/100], Training loss: 0.6931\n",
            "Epoch [4/100], Training loss: 0.6924\n",
            "Epoch [5/100], Training loss: 0.6917\n",
            "Epoch [6/100], Training loss: 0.6910\n",
            "Epoch [7/100], Training loss: 0.6903\n",
            "Epoch [8/100], Training loss: 0.6897\n",
            "Epoch [9/100], Training loss: 0.6890\n",
            "Epoch [10/100], Training loss: 0.6883\n",
            "Epoch [11/100], Training loss: 0.6877\n",
            "Epoch [12/100], Training loss: 0.6870\n",
            "Epoch [13/100], Training loss: 0.6863\n",
            "Epoch [14/100], Training loss: 0.6857\n",
            "Epoch [15/100], Training loss: 0.6850\n",
            "Epoch [16/100], Training loss: 0.6844\n",
            "Epoch [17/100], Training loss: 0.6837\n",
            "Epoch [18/100], Training loss: 0.6831\n",
            "Epoch [19/100], Training loss: 0.6824\n",
            "Epoch [20/100], Training loss: 0.6817\n",
            "Epoch [21/100], Training loss: 0.6810\n",
            "Epoch [22/100], Training loss: 0.6804\n",
            "Epoch [23/100], Training loss: 0.6797\n",
            "Epoch [24/100], Training loss: 0.6790\n",
            "Epoch [25/100], Training loss: 0.6783\n",
            "Epoch [26/100], Training loss: 0.6776\n",
            "Epoch [27/100], Training loss: 0.6768\n",
            "Epoch [28/100], Training loss: 0.6761\n",
            "Epoch [29/100], Training loss: 0.6753\n",
            "Epoch [30/100], Training loss: 0.6746\n",
            "Epoch [31/100], Training loss: 0.6738\n",
            "Epoch [32/100], Training loss: 0.6730\n",
            "Epoch [33/100], Training loss: 0.6722\n",
            "Epoch [34/100], Training loss: 0.6714\n",
            "Epoch [35/100], Training loss: 0.6706\n",
            "Epoch [36/100], Training loss: 0.6698\n",
            "Epoch [37/100], Training loss: 0.6690\n",
            "Epoch [38/100], Training loss: 0.6681\n",
            "Epoch [39/100], Training loss: 0.6672\n",
            "Epoch [40/100], Training loss: 0.6663\n",
            "Epoch [41/100], Training loss: 0.6654\n",
            "Epoch [42/100], Training loss: 0.6645\n",
            "Epoch [43/100], Training loss: 0.6635\n",
            "Epoch [44/100], Training loss: 0.6625\n",
            "Epoch [45/100], Training loss: 0.6616\n",
            "Epoch [46/100], Training loss: 0.6605\n",
            "Epoch [47/100], Training loss: 0.6595\n",
            "Epoch [48/100], Training loss: 0.6585\n",
            "Epoch [49/100], Training loss: 0.6574\n",
            "Epoch [50/100], Training loss: 0.6563\n",
            "Epoch [51/100], Training loss: 0.6552\n",
            "Epoch [52/100], Training loss: 0.6541\n",
            "Epoch [53/100], Training loss: 0.6529\n",
            "Epoch [54/100], Training loss: 0.6518\n",
            "Epoch [55/100], Training loss: 0.6506\n",
            "Epoch [56/100], Training loss: 0.6494\n",
            "Epoch [57/100], Training loss: 0.6481\n",
            "Epoch [58/100], Training loss: 0.6469\n",
            "Epoch [59/100], Training loss: 0.6457\n",
            "Epoch [60/100], Training loss: 0.6444\n",
            "Epoch [61/100], Training loss: 0.6431\n",
            "Epoch [62/100], Training loss: 0.6417\n",
            "Epoch [63/100], Training loss: 0.6404\n",
            "Epoch [64/100], Training loss: 0.6391\n",
            "Epoch [65/100], Training loss: 0.6377\n",
            "Epoch [66/100], Training loss: 0.6363\n",
            "Epoch [67/100], Training loss: 0.6349\n",
            "Epoch [68/100], Training loss: 0.6335\n",
            "Epoch [69/100], Training loss: 0.6320\n",
            "Epoch [70/100], Training loss: 0.6306\n",
            "Epoch [71/100], Training loss: 0.6291\n",
            "Epoch [72/100], Training loss: 0.6276\n",
            "Epoch [73/100], Training loss: 0.6261\n",
            "Epoch [74/100], Training loss: 0.6246\n",
            "Epoch [75/100], Training loss: 0.6231\n",
            "Epoch [76/100], Training loss: 0.6216\n",
            "Epoch [77/100], Training loss: 0.6200\n",
            "Epoch [78/100], Training loss: 0.6184\n",
            "Epoch [79/100], Training loss: 0.6169\n",
            "Epoch [80/100], Training loss: 0.6153\n",
            "Epoch [81/100], Training loss: 0.6137\n",
            "Epoch [82/100], Training loss: 0.6121\n",
            "Epoch [83/100], Training loss: 0.6104\n",
            "Epoch [84/100], Training loss: 0.6088\n",
            "Epoch [85/100], Training loss: 0.6072\n",
            "Epoch [86/100], Training loss: 0.6055\n",
            "Epoch [87/100], Training loss: 0.6039\n",
            "Epoch [88/100], Training loss: 0.6022\n",
            "Epoch [89/100], Training loss: 0.6005\n",
            "Epoch [90/100], Training loss: 0.5988\n",
            "Epoch [91/100], Training loss: 0.5971\n",
            "Epoch [92/100], Training loss: 0.5955\n",
            "Epoch [93/100], Training loss: 0.5937\n",
            "Epoch [94/100], Training loss: 0.5920\n",
            "Epoch [95/100], Training loss: 0.5903\n",
            "Epoch [96/100], Training loss: 0.5886\n",
            "Epoch [97/100], Training loss: 0.5869\n",
            "Epoch [98/100], Training loss: 0.5852\n",
            "Epoch [99/100], Training loss: 0.5834\n",
            "Epoch [100/100], Training loss: 0.5817\n",
            "Training complete.\n",
            "Epoch [1/100], Training loss: 0.6915\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-43-08011d4dd45a>:49: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  train_preds = model(torch.tensor(X_train, dtype=torch.float32))\n",
            "<ipython-input-43-08011d4dd45a>:55: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  val_preds = model(torch.tensor(X_val, dtype=torch.float32))\n",
            "<ipython-input-43-08011d4dd45a>:32: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  train_dataset = TensorDataset(torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.long))\n",
            "<ipython-input-43-08011d4dd45a>:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  val_dataset = TensorDataset(torch.tensor(X_val, dtype=torch.float32), torch.tensor(y_val, dtype=torch.long))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [2/100], Training loss: 0.6905\n",
            "Epoch [3/100], Training loss: 0.6895\n",
            "Epoch [4/100], Training loss: 0.6885\n",
            "Epoch [5/100], Training loss: 0.6875\n",
            "Epoch [6/100], Training loss: 0.6865\n",
            "Epoch [7/100], Training loss: 0.6855\n",
            "Epoch [8/100], Training loss: 0.6845\n",
            "Epoch [9/100], Training loss: 0.6835\n",
            "Epoch [10/100], Training loss: 0.6825\n",
            "Epoch [11/100], Training loss: 0.6815\n",
            "Epoch [12/100], Training loss: 0.6804\n",
            "Epoch [13/100], Training loss: 0.6794\n",
            "Epoch [14/100], Training loss: 0.6783\n",
            "Epoch [15/100], Training loss: 0.6773\n",
            "Epoch [16/100], Training loss: 0.6762\n",
            "Epoch [17/100], Training loss: 0.6751\n",
            "Epoch [18/100], Training loss: 0.6741\n",
            "Epoch [19/100], Training loss: 0.6730\n",
            "Epoch [20/100], Training loss: 0.6719\n",
            "Epoch [21/100], Training loss: 0.6708\n",
            "Epoch [22/100], Training loss: 0.6697\n",
            "Epoch [23/100], Training loss: 0.6686\n",
            "Epoch [24/100], Training loss: 0.6675\n",
            "Epoch [25/100], Training loss: 0.6664\n",
            "Epoch [26/100], Training loss: 0.6653\n",
            "Epoch [27/100], Training loss: 0.6641\n",
            "Epoch [28/100], Training loss: 0.6629\n",
            "Epoch [29/100], Training loss: 0.6618\n",
            "Epoch [30/100], Training loss: 0.6606\n",
            "Epoch [31/100], Training loss: 0.6594\n",
            "Epoch [32/100], Training loss: 0.6582\n",
            "Epoch [33/100], Training loss: 0.6570\n",
            "Epoch [34/100], Training loss: 0.6557\n",
            "Epoch [35/100], Training loss: 0.6544\n",
            "Epoch [36/100], Training loss: 0.6531\n",
            "Epoch [37/100], Training loss: 0.6518\n",
            "Epoch [38/100], Training loss: 0.6505\n",
            "Epoch [39/100], Training loss: 0.6492\n",
            "Epoch [40/100], Training loss: 0.6478\n",
            "Epoch [41/100], Training loss: 0.6465\n",
            "Epoch [42/100], Training loss: 0.6451\n",
            "Epoch [43/100], Training loss: 0.6436\n",
            "Epoch [44/100], Training loss: 0.6422\n",
            "Epoch [45/100], Training loss: 0.6408\n",
            "Epoch [46/100], Training loss: 0.6393\n",
            "Epoch [47/100], Training loss: 0.6378\n",
            "Epoch [48/100], Training loss: 0.6363\n",
            "Epoch [49/100], Training loss: 0.6348\n",
            "Epoch [50/100], Training loss: 0.6333\n",
            "Epoch [51/100], Training loss: 0.6318\n",
            "Epoch [52/100], Training loss: 0.6302\n",
            "Epoch [53/100], Training loss: 0.6287\n",
            "Epoch [54/100], Training loss: 0.6271\n",
            "Epoch [55/100], Training loss: 0.6255\n",
            "Epoch [56/100], Training loss: 0.6239\n",
            "Epoch [57/100], Training loss: 0.6223\n",
            "Epoch [58/100], Training loss: 0.6207\n",
            "Epoch [59/100], Training loss: 0.6190\n",
            "Epoch [60/100], Training loss: 0.6174\n",
            "Epoch [61/100], Training loss: 0.6157\n",
            "Epoch [62/100], Training loss: 0.6140\n",
            "Epoch [63/100], Training loss: 0.6124\n",
            "Epoch [64/100], Training loss: 0.6107\n",
            "Epoch [65/100], Training loss: 0.6090\n",
            "Epoch [66/100], Training loss: 0.6073\n",
            "Epoch [67/100], Training loss: 0.6055\n",
            "Epoch [68/100], Training loss: 0.6038\n",
            "Epoch [69/100], Training loss: 0.6021\n",
            "Epoch [70/100], Training loss: 0.6003\n",
            "Epoch [71/100], Training loss: 0.5986\n",
            "Epoch [72/100], Training loss: 0.5969\n",
            "Epoch [73/100], Training loss: 0.5951\n",
            "Epoch [74/100], Training loss: 0.5934\n",
            "Epoch [75/100], Training loss: 0.5916\n",
            "Epoch [76/100], Training loss: 0.5898\n",
            "Epoch [77/100], Training loss: 0.5881\n",
            "Epoch [78/100], Training loss: 0.5863\n",
            "Epoch [79/100], Training loss: 0.5845\n",
            "Epoch [80/100], Training loss: 0.5827\n",
            "Epoch [81/100], Training loss: 0.5809\n",
            "Epoch [82/100], Training loss: 0.5792\n",
            "Epoch [83/100], Training loss: 0.5774\n",
            "Epoch [84/100], Training loss: 0.5756\n",
            "Epoch [85/100], Training loss: 0.5738\n",
            "Epoch [86/100], Training loss: 0.5720\n",
            "Epoch [87/100], Training loss: 0.5702\n",
            "Epoch [88/100], Training loss: 0.5684\n",
            "Epoch [89/100], Training loss: 0.5666\n",
            "Epoch [90/100], Training loss: 0.5649\n",
            "Epoch [91/100], Training loss: 0.5631\n",
            "Epoch [92/100], Training loss: 0.5613\n",
            "Epoch [93/100], Training loss: 0.5595\n",
            "Epoch [94/100], Training loss: 0.5577\n",
            "Epoch [95/100], Training loss: 0.5559\n",
            "Epoch [96/100], Training loss: 0.5541\n",
            "Epoch [97/100], Training loss: 0.5524\n",
            "Epoch [98/100], Training loss: 0.5506\n",
            "Epoch [99/100], Training loss: 0.5488\n",
            "Epoch [100/100], Training loss: 0.5470\n",
            "Training complete.\n",
            "Epoch [1/100], Training loss: 0.6933\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-43-08011d4dd45a>:49: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  train_preds = model(torch.tensor(X_train, dtype=torch.float32))\n",
            "<ipython-input-43-08011d4dd45a>:55: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  val_preds = model(torch.tensor(X_val, dtype=torch.float32))\n",
            "<ipython-input-43-08011d4dd45a>:32: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  train_dataset = TensorDataset(torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.long))\n",
            "<ipython-input-43-08011d4dd45a>:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  val_dataset = TensorDataset(torch.tensor(X_val, dtype=torch.float32), torch.tensor(y_val, dtype=torch.long))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [2/100], Training loss: 0.6922\n",
            "Epoch [3/100], Training loss: 0.6911\n",
            "Epoch [4/100], Training loss: 0.6900\n",
            "Epoch [5/100], Training loss: 0.6889\n",
            "Epoch [6/100], Training loss: 0.6878\n",
            "Epoch [7/100], Training loss: 0.6868\n",
            "Epoch [8/100], Training loss: 0.6858\n",
            "Epoch [9/100], Training loss: 0.6848\n",
            "Epoch [10/100], Training loss: 0.6838\n",
            "Epoch [11/100], Training loss: 0.6828\n",
            "Epoch [12/100], Training loss: 0.6818\n",
            "Epoch [13/100], Training loss: 0.6808\n",
            "Epoch [14/100], Training loss: 0.6799\n",
            "Epoch [15/100], Training loss: 0.6789\n",
            "Epoch [16/100], Training loss: 0.6779\n",
            "Epoch [17/100], Training loss: 0.6769\n",
            "Epoch [18/100], Training loss: 0.6759\n",
            "Epoch [19/100], Training loss: 0.6749\n",
            "Epoch [20/100], Training loss: 0.6739\n",
            "Epoch [21/100], Training loss: 0.6729\n",
            "Epoch [22/100], Training loss: 0.6718\n",
            "Epoch [23/100], Training loss: 0.6708\n",
            "Epoch [24/100], Training loss: 0.6698\n",
            "Epoch [25/100], Training loss: 0.6688\n",
            "Epoch [26/100], Training loss: 0.6677\n",
            "Epoch [27/100], Training loss: 0.6666\n",
            "Epoch [28/100], Training loss: 0.6656\n",
            "Epoch [29/100], Training loss: 0.6645\n",
            "Epoch [30/100], Training loss: 0.6634\n",
            "Epoch [31/100], Training loss: 0.6623\n",
            "Epoch [32/100], Training loss: 0.6611\n",
            "Epoch [33/100], Training loss: 0.6600\n",
            "Epoch [34/100], Training loss: 0.6589\n",
            "Epoch [35/100], Training loss: 0.6577\n",
            "Epoch [36/100], Training loss: 0.6566\n",
            "Epoch [37/100], Training loss: 0.6554\n",
            "Epoch [38/100], Training loss: 0.6542\n",
            "Epoch [39/100], Training loss: 0.6530\n",
            "Epoch [40/100], Training loss: 0.6517\n",
            "Epoch [41/100], Training loss: 0.6505\n",
            "Epoch [42/100], Training loss: 0.6492\n",
            "Epoch [43/100], Training loss: 0.6479\n",
            "Epoch [44/100], Training loss: 0.6467\n",
            "Epoch [45/100], Training loss: 0.6454\n",
            "Epoch [46/100], Training loss: 0.6440\n",
            "Epoch [47/100], Training loss: 0.6427\n",
            "Epoch [48/100], Training loss: 0.6413\n",
            "Epoch [49/100], Training loss: 0.6400\n",
            "Epoch [50/100], Training loss: 0.6386\n",
            "Epoch [51/100], Training loss: 0.6372\n",
            "Epoch [52/100], Training loss: 0.6358\n",
            "Epoch [53/100], Training loss: 0.6343\n",
            "Epoch [54/100], Training loss: 0.6329\n",
            "Epoch [55/100], Training loss: 0.6314\n",
            "Epoch [56/100], Training loss: 0.6299\n",
            "Epoch [57/100], Training loss: 0.6284\n",
            "Epoch [58/100], Training loss: 0.6269\n",
            "Epoch [59/100], Training loss: 0.6254\n",
            "Epoch [60/100], Training loss: 0.6238\n",
            "Epoch [61/100], Training loss: 0.6222\n",
            "Epoch [62/100], Training loss: 0.6207\n",
            "Epoch [63/100], Training loss: 0.6191\n",
            "Epoch [64/100], Training loss: 0.6175\n",
            "Epoch [65/100], Training loss: 0.6158\n",
            "Epoch [66/100], Training loss: 0.6142\n",
            "Epoch [67/100], Training loss: 0.6126\n",
            "Epoch [68/100], Training loss: 0.6109\n",
            "Epoch [69/100], Training loss: 0.6092\n",
            "Epoch [70/100], Training loss: 0.6075\n",
            "Epoch [71/100], Training loss: 0.6058\n",
            "Epoch [72/100], Training loss: 0.6041\n",
            "Epoch [73/100], Training loss: 0.6024\n",
            "Epoch [74/100], Training loss: 0.6007\n",
            "Epoch [75/100], Training loss: 0.5989\n",
            "Epoch [76/100], Training loss: 0.5972\n",
            "Epoch [77/100], Training loss: 0.5954\n",
            "Epoch [78/100], Training loss: 0.5937\n",
            "Epoch [79/100], Training loss: 0.5919\n",
            "Epoch [80/100], Training loss: 0.5902\n",
            "Epoch [81/100], Training loss: 0.5884\n",
            "Epoch [82/100], Training loss: 0.5866\n",
            "Epoch [83/100], Training loss: 0.5848\n",
            "Epoch [84/100], Training loss: 0.5830\n",
            "Epoch [85/100], Training loss: 0.5812\n",
            "Epoch [86/100], Training loss: 0.5794\n",
            "Epoch [87/100], Training loss: 0.5776\n",
            "Epoch [88/100], Training loss: 0.5758\n",
            "Epoch [89/100], Training loss: 0.5740\n",
            "Epoch [90/100], Training loss: 0.5722\n",
            "Epoch [91/100], Training loss: 0.5704\n",
            "Epoch [92/100], Training loss: 0.5686\n",
            "Epoch [93/100], Training loss: 0.5667\n",
            "Epoch [94/100], Training loss: 0.5650\n",
            "Epoch [95/100], Training loss: 0.5631\n",
            "Epoch [96/100], Training loss: 0.5613\n",
            "Epoch [97/100], Training loss: 0.5595\n",
            "Epoch [98/100], Training loss: 0.5577\n",
            "Epoch [99/100], Training loss: 0.5559\n",
            "Epoch [100/100], Training loss: 0.5540\n",
            "Training complete.\n",
            "Epoch [1/100], Training loss: 0.6877\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-43-08011d4dd45a>:49: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  train_preds = model(torch.tensor(X_train, dtype=torch.float32))\n",
            "<ipython-input-43-08011d4dd45a>:55: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  val_preds = model(torch.tensor(X_val, dtype=torch.float32))\n",
            "<ipython-input-43-08011d4dd45a>:32: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  train_dataset = TensorDataset(torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.long))\n",
            "<ipython-input-43-08011d4dd45a>:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  val_dataset = TensorDataset(torch.tensor(X_val, dtype=torch.float32), torch.tensor(y_val, dtype=torch.long))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [2/100], Training loss: 0.6756\n",
            "Epoch [3/100], Training loss: 0.6635\n",
            "Epoch [4/100], Training loss: 0.6510\n",
            "Epoch [5/100], Training loss: 0.6374\n",
            "Epoch [6/100], Training loss: 0.6221\n",
            "Epoch [7/100], Training loss: 0.6057\n",
            "Epoch [8/100], Training loss: 0.5876\n",
            "Epoch [9/100], Training loss: 0.5686\n",
            "Epoch [10/100], Training loss: 0.5490\n",
            "Epoch [11/100], Training loss: 0.5297\n",
            "Epoch [12/100], Training loss: 0.5105\n",
            "Epoch [13/100], Training loss: 0.4921\n",
            "Epoch [14/100], Training loss: 0.4749\n",
            "Epoch [15/100], Training loss: 0.4580\n",
            "Epoch [16/100], Training loss: 0.4420\n",
            "Epoch [17/100], Training loss: 0.4266\n",
            "Epoch [18/100], Training loss: 0.4121\n",
            "Epoch [19/100], Training loss: 0.3988\n",
            "Epoch [20/100], Training loss: 0.3851\n",
            "Epoch [21/100], Training loss: 0.3728\n",
            "Epoch [22/100], Training loss: 0.3602\n",
            "Epoch [23/100], Training loss: 0.3464\n",
            "Epoch [24/100], Training loss: 0.3383\n",
            "Epoch [25/100], Training loss: 0.3253\n",
            "Epoch [26/100], Training loss: 0.3136\n",
            "Epoch [27/100], Training loss: 0.3030\n",
            "Epoch [28/100], Training loss: 0.2919\n",
            "Epoch [29/100], Training loss: 0.2822\n",
            "Epoch [30/100], Training loss: 0.2731\n",
            "Epoch [31/100], Training loss: 0.2651\n",
            "Epoch [32/100], Training loss: 0.2526\n",
            "Epoch [33/100], Training loss: 0.2433\n",
            "Epoch [34/100], Training loss: 0.2356\n",
            "Epoch [35/100], Training loss: 0.2252\n",
            "Epoch [36/100], Training loss: 0.2151\n",
            "Epoch [37/100], Training loss: 0.2078\n",
            "Epoch [38/100], Training loss: 0.2003\n",
            "Epoch [39/100], Training loss: 0.1926\n",
            "Epoch [40/100], Training loss: 0.1827\n",
            "Epoch [41/100], Training loss: 0.1735\n",
            "Epoch [42/100], Training loss: 0.1664\n",
            "Epoch [43/100], Training loss: 0.1589\n",
            "Epoch [44/100], Training loss: 0.1529\n",
            "Epoch [45/100], Training loss: 0.1461\n",
            "Epoch [46/100], Training loss: 0.1407\n",
            "Epoch [47/100], Training loss: 0.1342\n",
            "Epoch [48/100], Training loss: 0.1269\n",
            "Epoch [49/100], Training loss: 0.1186\n",
            "Epoch [50/100], Training loss: 0.1194\n",
            "Epoch [51/100], Training loss: 0.1124\n",
            "Epoch [52/100], Training loss: 0.1066\n",
            "Epoch [53/100], Training loss: 0.1041\n",
            "Epoch [54/100], Training loss: 0.0993\n",
            "Epoch [55/100], Training loss: 0.0929\n",
            "Epoch [56/100], Training loss: 0.0891\n",
            "Epoch [57/100], Training loss: 0.0873\n",
            "Epoch [58/100], Training loss: 0.0842\n",
            "Epoch [59/100], Training loss: 0.0796\n",
            "Epoch [60/100], Training loss: 0.0768\n",
            "Epoch [61/100], Training loss: 0.0719\n",
            "Epoch [62/100], Training loss: 0.0698\n",
            "Epoch [63/100], Training loss: 0.0653\n",
            "Epoch [64/100], Training loss: 0.0650\n",
            "Epoch [65/100], Training loss: 0.0637\n",
            "Epoch [66/100], Training loss: 0.0562\n",
            "Epoch [67/100], Training loss: 0.0574\n",
            "Epoch [68/100], Training loss: 0.0511\n",
            "Epoch [69/100], Training loss: 0.0485\n",
            "Epoch [70/100], Training loss: 0.0525\n",
            "Epoch [71/100], Training loss: 0.0536\n",
            "Epoch [72/100], Training loss: 0.0485\n",
            "Epoch [73/100], Training loss: 0.0457\n",
            "Epoch [74/100], Training loss: 0.0448\n",
            "Epoch [75/100], Training loss: 0.0444\n",
            "Epoch [76/100], Training loss: 0.0462\n",
            "Epoch [77/100], Training loss: 0.0402\n",
            "Epoch [78/100], Training loss: 0.0361\n",
            "Epoch [79/100], Training loss: 0.0366\n",
            "Epoch [80/100], Training loss: 0.0373\n",
            "Epoch [81/100], Training loss: 0.0395\n",
            "Epoch [82/100], Training loss: 0.0412\n",
            "Epoch [83/100], Training loss: 0.0332\n",
            "Epoch [84/100], Training loss: 0.0300\n",
            "Epoch [85/100], Training loss: 0.0335\n",
            "Epoch [86/100], Training loss: 0.0281\n",
            "Epoch [87/100], Training loss: 0.0325\n",
            "Epoch [88/100], Training loss: 0.0260\n",
            "Epoch [89/100], Training loss: 0.0281\n",
            "Epoch [90/100], Training loss: 0.0288\n",
            "Epoch [91/100], Training loss: 0.0288\n",
            "Epoch [92/100], Training loss: 0.0236\n",
            "Epoch [93/100], Training loss: 0.0269\n",
            "Epoch [94/100], Training loss: 0.0211\n",
            "Epoch [95/100], Training loss: 0.0273\n",
            "Epoch [96/100], Training loss: 0.0252\n",
            "Epoch [97/100], Training loss: 0.0233\n",
            "Epoch [98/100], Training loss: 0.0228\n",
            "Epoch [99/100], Training loss: 0.0215\n",
            "Epoch [100/100], Training loss: 0.0245\n",
            "Training complete.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-43-08011d4dd45a>:49: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  train_preds = model(torch.tensor(X_train, dtype=torch.float32))\n",
            "<ipython-input-43-08011d4dd45a>:55: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  val_preds = model(torch.tensor(X_val, dtype=torch.float32))\n",
            "<ipython-input-43-08011d4dd45a>:32: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  train_dataset = TensorDataset(torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.long))\n",
            "<ipython-input-43-08011d4dd45a>:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  val_dataset = TensorDataset(torch.tensor(X_val, dtype=torch.float32), torch.tensor(y_val, dtype=torch.long))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/100], Training loss: 0.6919\n",
            "Epoch [2/100], Training loss: 0.6828\n",
            "Epoch [3/100], Training loss: 0.6732\n",
            "Epoch [4/100], Training loss: 0.6629\n",
            "Epoch [5/100], Training loss: 0.6518\n",
            "Epoch [6/100], Training loss: 0.6397\n",
            "Epoch [7/100], Training loss: 0.6264\n",
            "Epoch [8/100], Training loss: 0.6124\n",
            "Epoch [9/100], Training loss: 0.5976\n",
            "Epoch [10/100], Training loss: 0.5823\n",
            "Epoch [11/100], Training loss: 0.5666\n",
            "Epoch [12/100], Training loss: 0.5510\n",
            "Epoch [13/100], Training loss: 0.5351\n",
            "Epoch [14/100], Training loss: 0.5196\n",
            "Epoch [15/100], Training loss: 0.5040\n",
            "Epoch [16/100], Training loss: 0.4892\n",
            "Epoch [17/100], Training loss: 0.4744\n",
            "Epoch [18/100], Training loss: 0.4606\n",
            "Epoch [19/100], Training loss: 0.4471\n",
            "Epoch [20/100], Training loss: 0.4335\n",
            "Epoch [21/100], Training loss: 0.4205\n",
            "Epoch [22/100], Training loss: 0.4081\n",
            "Epoch [23/100], Training loss: 0.3961\n",
            "Epoch [24/100], Training loss: 0.3832\n",
            "Epoch [25/100], Training loss: 0.3728\n",
            "Epoch [26/100], Training loss: 0.3614\n",
            "Epoch [27/100], Training loss: 0.3497\n",
            "Epoch [28/100], Training loss: 0.3397\n",
            "Epoch [29/100], Training loss: 0.3301\n",
            "Epoch [30/100], Training loss: 0.3196\n",
            "Epoch [31/100], Training loss: 0.3095\n",
            "Epoch [32/100], Training loss: 0.2997\n",
            "Epoch [33/100], Training loss: 0.2895\n",
            "Epoch [34/100], Training loss: 0.2802\n",
            "Epoch [35/100], Training loss: 0.2705\n",
            "Epoch [36/100], Training loss: 0.2611\n",
            "Epoch [37/100], Training loss: 0.2537\n",
            "Epoch [38/100], Training loss: 0.2451\n",
            "Epoch [39/100], Training loss: 0.2370\n",
            "Epoch [40/100], Training loss: 0.2275\n",
            "Epoch [41/100], Training loss: 0.2190\n",
            "Epoch [42/100], Training loss: 0.2144\n",
            "Epoch [43/100], Training loss: 0.2023\n",
            "Epoch [44/100], Training loss: 0.1952\n",
            "Epoch [45/100], Training loss: 0.1880\n",
            "Epoch [46/100], Training loss: 0.1804\n",
            "Epoch [47/100], Training loss: 0.1735\n",
            "Epoch [48/100], Training loss: 0.1681\n",
            "Epoch [49/100], Training loss: 0.1595\n",
            "Epoch [50/100], Training loss: 0.1539\n",
            "Epoch [51/100], Training loss: 0.1477\n",
            "Epoch [52/100], Training loss: 0.1419\n",
            "Epoch [53/100], Training loss: 0.1355\n",
            "Epoch [54/100], Training loss: 0.1297\n",
            "Epoch [55/100], Training loss: 0.1248\n",
            "Epoch [56/100], Training loss: 0.1199\n",
            "Epoch [57/100], Training loss: 0.1119\n",
            "Epoch [58/100], Training loss: 0.1089\n",
            "Epoch [59/100], Training loss: 0.1040\n",
            "Epoch [60/100], Training loss: 0.1017\n",
            "Epoch [61/100], Training loss: 0.0951\n",
            "Epoch [62/100], Training loss: 0.0932\n",
            "Epoch [63/100], Training loss: 0.0871\n",
            "Epoch [64/100], Training loss: 0.0824\n",
            "Epoch [65/100], Training loss: 0.0857\n",
            "Epoch [66/100], Training loss: 0.0770\n",
            "Epoch [67/100], Training loss: 0.0791\n",
            "Epoch [68/100], Training loss: 0.0700\n",
            "Epoch [69/100], Training loss: 0.0681\n",
            "Epoch [70/100], Training loss: 0.0715\n",
            "Epoch [71/100], Training loss: 0.0665\n",
            "Epoch [72/100], Training loss: 0.0641\n",
            "Epoch [73/100], Training loss: 0.0577\n",
            "Epoch [74/100], Training loss: 0.0517\n",
            "Epoch [75/100], Training loss: 0.0546\n",
            "Epoch [76/100], Training loss: 0.0637\n",
            "Epoch [77/100], Training loss: 0.0551\n",
            "Epoch [78/100], Training loss: 0.0456\n",
            "Epoch [79/100], Training loss: 0.0518\n",
            "Epoch [80/100], Training loss: 0.0567\n",
            "Epoch [81/100], Training loss: 0.0451\n",
            "Epoch [82/100], Training loss: 0.0412\n",
            "Epoch [83/100], Training loss: 0.0404\n",
            "Epoch [84/100], Training loss: 0.0395\n",
            "Epoch [85/100], Training loss: 0.0400\n",
            "Epoch [86/100], Training loss: 0.0348\n",
            "Epoch [87/100], Training loss: 0.0335\n",
            "Epoch [88/100], Training loss: 0.0323\n",
            "Epoch [89/100], Training loss: 0.0313\n",
            "Epoch [90/100], Training loss: 0.0285\n",
            "Epoch [91/100], Training loss: 0.0311\n",
            "Epoch [92/100], Training loss: 0.0281\n",
            "Epoch [93/100], Training loss: 0.0278\n",
            "Epoch [94/100], Training loss: 0.0319\n",
            "Epoch [95/100], Training loss: 0.0271\n",
            "Epoch [96/100], Training loss: 0.0263\n",
            "Epoch [97/100], Training loss: 0.0260\n",
            "Epoch [98/100], Training loss: 0.0252\n",
            "Epoch [99/100], Training loss: 0.0234\n",
            "Epoch [100/100], Training loss: 0.0221\n",
            "Training complete.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-43-08011d4dd45a>:49: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  train_preds = model(torch.tensor(X_train, dtype=torch.float32))\n",
            "<ipython-input-43-08011d4dd45a>:55: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  val_preds = model(torch.tensor(X_val, dtype=torch.float32))\n",
            "<ipython-input-43-08011d4dd45a>:32: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  train_dataset = TensorDataset(torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.long))\n",
            "<ipython-input-43-08011d4dd45a>:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  val_dataset = TensorDataset(torch.tensor(X_val, dtype=torch.float32), torch.tensor(y_val, dtype=torch.long))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/100], Training loss: 0.6873\n",
            "Epoch [2/100], Training loss: 0.6760\n",
            "Epoch [3/100], Training loss: 0.6644\n",
            "Epoch [4/100], Training loss: 0.6518\n",
            "Epoch [5/100], Training loss: 0.6373\n",
            "Epoch [6/100], Training loss: 0.6212\n",
            "Epoch [7/100], Training loss: 0.6036\n",
            "Epoch [8/100], Training loss: 0.5850\n",
            "Epoch [9/100], Training loss: 0.5663\n",
            "Epoch [10/100], Training loss: 0.5472\n",
            "Epoch [11/100], Training loss: 0.5288\n",
            "Epoch [12/100], Training loss: 0.5112\n",
            "Epoch [13/100], Training loss: 0.4941\n",
            "Epoch [14/100], Training loss: 0.4779\n",
            "Epoch [15/100], Training loss: 0.4627\n",
            "Epoch [16/100], Training loss: 0.4486\n",
            "Epoch [17/100], Training loss: 0.4349\n",
            "Epoch [18/100], Training loss: 0.4202\n",
            "Epoch [19/100], Training loss: 0.4078\n",
            "Epoch [20/100], Training loss: 0.3947\n",
            "Epoch [21/100], Training loss: 0.3827\n",
            "Epoch [22/100], Training loss: 0.3703\n",
            "Epoch [23/100], Training loss: 0.3593\n",
            "Epoch [24/100], Training loss: 0.3476\n",
            "Epoch [25/100], Training loss: 0.3368\n",
            "Epoch [26/100], Training loss: 0.3261\n",
            "Epoch [27/100], Training loss: 0.3158\n",
            "Epoch [28/100], Training loss: 0.3040\n",
            "Epoch [29/100], Training loss: 0.2945\n",
            "Epoch [30/100], Training loss: 0.2845\n",
            "Epoch [31/100], Training loss: 0.2740\n",
            "Epoch [32/100], Training loss: 0.2648\n",
            "Epoch [33/100], Training loss: 0.2563\n",
            "Epoch [34/100], Training loss: 0.2458\n",
            "Epoch [35/100], Training loss: 0.2383\n",
            "Epoch [36/100], Training loss: 0.2295\n",
            "Epoch [37/100], Training loss: 0.2203\n",
            "Epoch [38/100], Training loss: 0.2137\n",
            "Epoch [39/100], Training loss: 0.2040\n",
            "Epoch [40/100], Training loss: 0.1974\n",
            "Epoch [41/100], Training loss: 0.1898\n",
            "Epoch [42/100], Training loss: 0.1815\n",
            "Epoch [43/100], Training loss: 0.1743\n",
            "Epoch [44/100], Training loss: 0.1686\n",
            "Epoch [45/100], Training loss: 0.1598\n",
            "Epoch [46/100], Training loss: 0.1547\n",
            "Epoch [47/100], Training loss: 0.1491\n",
            "Epoch [48/100], Training loss: 0.1424\n",
            "Epoch [49/100], Training loss: 0.1372\n",
            "Epoch [50/100], Training loss: 0.1317\n",
            "Epoch [51/100], Training loss: 0.1260\n",
            "Epoch [52/100], Training loss: 0.1216\n",
            "Epoch [53/100], Training loss: 0.1132\n",
            "Epoch [54/100], Training loss: 0.1117\n",
            "Epoch [55/100], Training loss: 0.1076\n",
            "Epoch [56/100], Training loss: 0.1029\n",
            "Epoch [57/100], Training loss: 0.0969\n",
            "Epoch [58/100], Training loss: 0.0948\n",
            "Epoch [59/100], Training loss: 0.0900\n",
            "Epoch [60/100], Training loss: 0.0877\n",
            "Epoch [61/100], Training loss: 0.0843\n",
            "Epoch [62/100], Training loss: 0.0800\n",
            "Epoch [63/100], Training loss: 0.0769\n",
            "Epoch [64/100], Training loss: 0.0736\n",
            "Epoch [65/100], Training loss: 0.0701\n",
            "Epoch [66/100], Training loss: 0.0682\n",
            "Epoch [67/100], Training loss: 0.0652\n",
            "Epoch [68/100], Training loss: 0.0631\n",
            "Epoch [69/100], Training loss: 0.0591\n",
            "Epoch [70/100], Training loss: 0.0591\n",
            "Epoch [71/100], Training loss: 0.0562\n",
            "Epoch [72/100], Training loss: 0.0539\n",
            "Epoch [73/100], Training loss: 0.0527\n",
            "Epoch [74/100], Training loss: 0.0497\n",
            "Epoch [75/100], Training loss: 0.0500\n",
            "Epoch [76/100], Training loss: 0.0478\n",
            "Epoch [77/100], Training loss: 0.0425\n",
            "Epoch [78/100], Training loss: 0.0448\n",
            "Epoch [79/100], Training loss: 0.0428\n",
            "Epoch [80/100], Training loss: 0.0416\n",
            "Epoch [81/100], Training loss: 0.0418\n",
            "Epoch [82/100], Training loss: 0.0370\n",
            "Epoch [83/100], Training loss: 0.0369\n",
            "Epoch [84/100], Training loss: 0.0369\n",
            "Epoch [85/100], Training loss: 0.0355\n",
            "Epoch [86/100], Training loss: 0.0350\n",
            "Epoch [87/100], Training loss: 0.0324\n",
            "Epoch [88/100], Training loss: 0.0325\n",
            "Epoch [89/100], Training loss: 0.0323\n",
            "Epoch [90/100], Training loss: 0.0314\n",
            "Epoch [91/100], Training loss: 0.0312\n",
            "Epoch [92/100], Training loss: 0.0303\n",
            "Epoch [93/100], Training loss: 0.0284\n",
            "Epoch [94/100], Training loss: 0.0306\n",
            "Epoch [95/100], Training loss: 0.0267\n",
            "Epoch [96/100], Training loss: 0.0249\n",
            "Epoch [97/100], Training loss: 0.0270\n",
            "Epoch [98/100], Training loss: 0.0264\n",
            "Epoch [99/100], Training loss: 0.0262\n",
            "Epoch [100/100], Training loss: 0.0207\n",
            "Training complete.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-43-08011d4dd45a>:49: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  train_preds = model(torch.tensor(X_train, dtype=torch.float32))\n",
            "<ipython-input-43-08011d4dd45a>:55: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  val_preds = model(torch.tensor(X_val, dtype=torch.float32))\n",
            "<ipython-input-43-08011d4dd45a>:32: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  train_dataset = TensorDataset(torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.long))\n",
            "<ipython-input-43-08011d4dd45a>:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  val_dataset = TensorDataset(torch.tensor(X_val, dtype=torch.float32), torch.tensor(y_val, dtype=torch.long))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/100], Training loss: 0.6921\n",
            "Epoch [2/100], Training loss: 0.6828\n",
            "Epoch [3/100], Training loss: 0.6733\n",
            "Epoch [4/100], Training loss: 0.6625\n",
            "Epoch [5/100], Training loss: 0.6498\n",
            "Epoch [6/100], Training loss: 0.6352\n",
            "Epoch [7/100], Training loss: 0.6185\n",
            "Epoch [8/100], Training loss: 0.5999\n",
            "Epoch [9/100], Training loss: 0.5805\n",
            "Epoch [10/100], Training loss: 0.5603\n",
            "Epoch [11/100], Training loss: 0.5400\n",
            "Epoch [12/100], Training loss: 0.5206\n",
            "Epoch [13/100], Training loss: 0.5025\n",
            "Epoch [14/100], Training loss: 0.4846\n",
            "Epoch [15/100], Training loss: 0.4688\n",
            "Epoch [16/100], Training loss: 0.4529\n",
            "Epoch [17/100], Training loss: 0.4371\n",
            "Epoch [18/100], Training loss: 0.4230\n",
            "Epoch [19/100], Training loss: 0.4089\n",
            "Epoch [20/100], Training loss: 0.3952\n",
            "Epoch [21/100], Training loss: 0.3835\n",
            "Epoch [22/100], Training loss: 0.3704\n",
            "Epoch [23/100], Training loss: 0.3581\n",
            "Epoch [24/100], Training loss: 0.3468\n",
            "Epoch [25/100], Training loss: 0.3368\n",
            "Epoch [26/100], Training loss: 0.3250\n",
            "Epoch [27/100], Training loss: 0.3138\n",
            "Epoch [28/100], Training loss: 0.3039\n",
            "Epoch [29/100], Training loss: 0.2944\n",
            "Epoch [30/100], Training loss: 0.2848\n",
            "Epoch [31/100], Training loss: 0.2740\n",
            "Epoch [32/100], Training loss: 0.2637\n",
            "Epoch [33/100], Training loss: 0.2551\n",
            "Epoch [34/100], Training loss: 0.2469\n",
            "Epoch [35/100], Training loss: 0.2386\n",
            "Epoch [36/100], Training loss: 0.2271\n",
            "Epoch [37/100], Training loss: 0.2202\n",
            "Epoch [38/100], Training loss: 0.2109\n",
            "Epoch [39/100], Training loss: 0.2038\n",
            "Epoch [40/100], Training loss: 0.1964\n",
            "Epoch [41/100], Training loss: 0.1880\n",
            "Epoch [42/100], Training loss: 0.1836\n",
            "Epoch [43/100], Training loss: 0.1728\n",
            "Epoch [44/100], Training loss: 0.1679\n",
            "Epoch [45/100], Training loss: 0.1598\n",
            "Epoch [46/100], Training loss: 0.1546\n",
            "Epoch [47/100], Training loss: 0.1493\n",
            "Epoch [48/100], Training loss: 0.1443\n",
            "Epoch [49/100], Training loss: 0.1378\n",
            "Epoch [50/100], Training loss: 0.1307\n",
            "Epoch [51/100], Training loss: 0.1253\n",
            "Epoch [52/100], Training loss: 0.1192\n",
            "Epoch [53/100], Training loss: 0.1121\n",
            "Epoch [54/100], Training loss: 0.1113\n",
            "Epoch [55/100], Training loss: 0.1107\n",
            "Epoch [56/100], Training loss: 0.1044\n",
            "Epoch [57/100], Training loss: 0.0973\n",
            "Epoch [58/100], Training loss: 0.0940\n",
            "Epoch [59/100], Training loss: 0.0907\n",
            "Epoch [60/100], Training loss: 0.0885\n",
            "Epoch [61/100], Training loss: 0.0839\n",
            "Epoch [62/100], Training loss: 0.0827\n",
            "Epoch [63/100], Training loss: 0.0786\n",
            "Epoch [64/100], Training loss: 0.0738\n",
            "Epoch [65/100], Training loss: 0.0691\n",
            "Epoch [66/100], Training loss: 0.0666\n",
            "Epoch [67/100], Training loss: 0.0707\n",
            "Epoch [68/100], Training loss: 0.0672\n",
            "Epoch [69/100], Training loss: 0.0620\n",
            "Epoch [70/100], Training loss: 0.0614\n",
            "Epoch [71/100], Training loss: 0.0548\n",
            "Epoch [72/100], Training loss: 0.0550\n",
            "Epoch [73/100], Training loss: 0.0732\n",
            "Epoch [74/100], Training loss: 0.0501\n",
            "Epoch [75/100], Training loss: 0.0486\n",
            "Epoch [76/100], Training loss: 0.0493\n",
            "Epoch [77/100], Training loss: 0.0486\n",
            "Epoch [78/100], Training loss: 0.0438\n",
            "Epoch [79/100], Training loss: 0.0466\n",
            "Epoch [80/100], Training loss: 0.0489\n",
            "Epoch [81/100], Training loss: 0.0378\n",
            "Epoch [82/100], Training loss: 0.0368\n",
            "Epoch [83/100], Training loss: 0.0315\n",
            "Epoch [84/100], Training loss: 0.0370\n",
            "Epoch [85/100], Training loss: 0.0362\n",
            "Epoch [86/100], Training loss: 0.0331\n",
            "Epoch [87/100], Training loss: 0.0306\n",
            "Epoch [88/100], Training loss: 0.0393\n",
            "Epoch [89/100], Training loss: 0.0340\n",
            "Epoch [90/100], Training loss: 0.0279\n",
            "Epoch [91/100], Training loss: 0.0314\n",
            "Epoch [92/100], Training loss: 0.0277\n",
            "Epoch [93/100], Training loss: 0.0254\n",
            "Epoch [94/100], Training loss: 0.0261\n",
            "Epoch [95/100], Training loss: 0.0211\n",
            "Epoch [96/100], Training loss: 0.0270\n",
            "Epoch [97/100], Training loss: 0.0241\n",
            "Epoch [98/100], Training loss: 0.0216\n",
            "Epoch [99/100], Training loss: 0.0299\n",
            "Epoch [100/100], Training loss: 0.0248\n",
            "Training complete.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-43-08011d4dd45a>:49: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  train_preds = model(torch.tensor(X_train, dtype=torch.float32))\n",
            "<ipython-input-43-08011d4dd45a>:55: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  val_preds = model(torch.tensor(X_val, dtype=torch.float32))\n",
            "<ipython-input-43-08011d4dd45a>:32: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  train_dataset = TensorDataset(torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.long))\n",
            "<ipython-input-43-08011d4dd45a>:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  val_dataset = TensorDataset(torch.tensor(X_val, dtype=torch.float32), torch.tensor(y_val, dtype=torch.long))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/100], Training loss: 0.6928\n",
            "Epoch [2/100], Training loss: 0.6839\n",
            "Epoch [3/100], Training loss: 0.6743\n",
            "Epoch [4/100], Training loss: 0.6637\n",
            "Epoch [5/100], Training loss: 0.6519\n",
            "Epoch [6/100], Training loss: 0.6386\n",
            "Epoch [7/100], Training loss: 0.6242\n",
            "Epoch [8/100], Training loss: 0.6088\n",
            "Epoch [9/100], Training loss: 0.5931\n",
            "Epoch [10/100], Training loss: 0.5771\n",
            "Epoch [11/100], Training loss: 0.5611\n",
            "Epoch [12/100], Training loss: 0.5454\n",
            "Epoch [13/100], Training loss: 0.5297\n",
            "Epoch [14/100], Training loss: 0.5144\n",
            "Epoch [15/100], Training loss: 0.4992\n",
            "Epoch [16/100], Training loss: 0.4845\n",
            "Epoch [17/100], Training loss: 0.4700\n",
            "Epoch [18/100], Training loss: 0.4563\n",
            "Epoch [19/100], Training loss: 0.4432\n",
            "Epoch [20/100], Training loss: 0.4302\n",
            "Epoch [21/100], Training loss: 0.4182\n",
            "Epoch [22/100], Training loss: 0.4064\n",
            "Epoch [23/100], Training loss: 0.3933\n",
            "Epoch [24/100], Training loss: 0.3826\n",
            "Epoch [25/100], Training loss: 0.3714\n",
            "Epoch [26/100], Training loss: 0.3606\n",
            "Epoch [27/100], Training loss: 0.3506\n",
            "Epoch [28/100], Training loss: 0.3398\n",
            "Epoch [29/100], Training loss: 0.3299\n",
            "Epoch [30/100], Training loss: 0.3198\n",
            "Epoch [31/100], Training loss: 0.3102\n",
            "Epoch [32/100], Training loss: 0.2999\n",
            "Epoch [33/100], Training loss: 0.2906\n",
            "Epoch [34/100], Training loss: 0.2826\n",
            "Epoch [35/100], Training loss: 0.2730\n",
            "Epoch [36/100], Training loss: 0.2646\n",
            "Epoch [37/100], Training loss: 0.2556\n",
            "Epoch [38/100], Training loss: 0.2466\n",
            "Epoch [39/100], Training loss: 0.2382\n",
            "Epoch [40/100], Training loss: 0.2310\n",
            "Epoch [41/100], Training loss: 0.2234\n",
            "Epoch [42/100], Training loss: 0.2161\n",
            "Epoch [43/100], Training loss: 0.2086\n",
            "Epoch [44/100], Training loss: 0.1995\n",
            "Epoch [45/100], Training loss: 0.1932\n",
            "Epoch [46/100], Training loss: 0.1862\n",
            "Epoch [47/100], Training loss: 0.1790\n",
            "Epoch [48/100], Training loss: 0.1707\n",
            "Epoch [49/100], Training loss: 0.1666\n",
            "Epoch [50/100], Training loss: 0.1604\n",
            "Epoch [51/100], Training loss: 0.1541\n",
            "Epoch [52/100], Training loss: 0.1487\n",
            "Epoch [53/100], Training loss: 0.1429\n",
            "Epoch [54/100], Training loss: 0.1385\n",
            "Epoch [55/100], Training loss: 0.1310\n",
            "Epoch [56/100], Training loss: 0.1256\n",
            "Epoch [57/100], Training loss: 0.1216\n",
            "Epoch [58/100], Training loss: 0.1153\n",
            "Epoch [59/100], Training loss: 0.1129\n",
            "Epoch [60/100], Training loss: 0.1118\n",
            "Epoch [61/100], Training loss: 0.1039\n",
            "Epoch [62/100], Training loss: 0.0979\n",
            "Epoch [63/100], Training loss: 0.0944\n",
            "Epoch [64/100], Training loss: 0.0915\n",
            "Epoch [65/100], Training loss: 0.0879\n",
            "Epoch [66/100], Training loss: 0.0835\n",
            "Epoch [67/100], Training loss: 0.0815\n",
            "Epoch [68/100], Training loss: 0.0784\n",
            "Epoch [69/100], Training loss: 0.0727\n",
            "Epoch [70/100], Training loss: 0.0717\n",
            "Epoch [71/100], Training loss: 0.0689\n",
            "Epoch [72/100], Training loss: 0.0673\n",
            "Epoch [73/100], Training loss: 0.0632\n",
            "Epoch [74/100], Training loss: 0.0629\n",
            "Epoch [75/100], Training loss: 0.0611\n",
            "Epoch [76/100], Training loss: 0.0571\n",
            "Epoch [77/100], Training loss: 0.0565\n",
            "Epoch [78/100], Training loss: 0.0559\n",
            "Epoch [79/100], Training loss: 0.0566\n",
            "Epoch [80/100], Training loss: 0.0519\n",
            "Epoch [81/100], Training loss: 0.0470\n",
            "Epoch [82/100], Training loss: 0.0482\n",
            "Epoch [83/100], Training loss: 0.0444\n",
            "Epoch [84/100], Training loss: 0.0461\n",
            "Epoch [85/100], Training loss: 0.0424\n",
            "Epoch [86/100], Training loss: 0.0421\n",
            "Epoch [87/100], Training loss: 0.0442\n",
            "Epoch [88/100], Training loss: 0.0384\n",
            "Epoch [89/100], Training loss: 0.0365\n",
            "Epoch [90/100], Training loss: 0.0407\n",
            "Epoch [91/100], Training loss: 0.0357\n",
            "Epoch [92/100], Training loss: 0.0379\n",
            "Epoch [93/100], Training loss: 0.0310\n",
            "Epoch [94/100], Training loss: 0.0327\n",
            "Epoch [95/100], Training loss: 0.0330\n",
            "Epoch [96/100], Training loss: 0.0303\n",
            "Epoch [97/100], Training loss: 0.0323\n",
            "Epoch [98/100], Training loss: 0.0315\n",
            "Epoch [99/100], Training loss: 0.0358\n",
            "Epoch [100/100], Training loss: 0.0302\n",
            "Training complete.\n",
            "Epoch [1/100], Training loss: 0.6890\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-43-08011d4dd45a>:49: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  train_preds = model(torch.tensor(X_train, dtype=torch.float32))\n",
            "<ipython-input-43-08011d4dd45a>:55: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  val_preds = model(torch.tensor(X_val, dtype=torch.float32))\n",
            "<ipython-input-43-08011d4dd45a>:32: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  train_dataset = TensorDataset(torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.long))\n",
            "<ipython-input-43-08011d4dd45a>:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  val_dataset = TensorDataset(torch.tensor(X_val, dtype=torch.float32), torch.tensor(y_val, dtype=torch.long))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [2/100], Training loss: 0.6809\n",
            "Epoch [3/100], Training loss: 0.6721\n",
            "Epoch [4/100], Training loss: 0.6621\n",
            "Epoch [5/100], Training loss: 0.6507\n",
            "Epoch [6/100], Training loss: 0.6381\n",
            "Epoch [7/100], Training loss: 0.6245\n",
            "Epoch [8/100], Training loss: 0.6100\n",
            "Epoch [9/100], Training loss: 0.5952\n",
            "Epoch [10/100], Training loss: 0.5803\n",
            "Epoch [11/100], Training loss: 0.5652\n",
            "Epoch [12/100], Training loss: 0.5502\n",
            "Epoch [13/100], Training loss: 0.5348\n",
            "Epoch [14/100], Training loss: 0.5198\n",
            "Epoch [15/100], Training loss: 0.5047\n",
            "Epoch [16/100], Training loss: 0.4899\n",
            "Epoch [17/100], Training loss: 0.4755\n",
            "Epoch [18/100], Training loss: 0.4619\n",
            "Epoch [19/100], Training loss: 0.4478\n",
            "Epoch [20/100], Training loss: 0.4348\n",
            "Epoch [21/100], Training loss: 0.4223\n",
            "Epoch [22/100], Training loss: 0.4101\n",
            "Epoch [23/100], Training loss: 0.3984\n",
            "Epoch [24/100], Training loss: 0.3868\n",
            "Epoch [25/100], Training loss: 0.3769\n",
            "Epoch [26/100], Training loss: 0.3652\n",
            "Epoch [27/100], Training loss: 0.3554\n",
            "Epoch [28/100], Training loss: 0.3457\n",
            "Epoch [29/100], Training loss: 0.3354\n",
            "Epoch [30/100], Training loss: 0.3261\n",
            "Epoch [31/100], Training loss: 0.3168\n",
            "Epoch [32/100], Training loss: 0.3073\n",
            "Epoch [33/100], Training loss: 0.2985\n",
            "Epoch [34/100], Training loss: 0.2904\n",
            "Epoch [35/100], Training loss: 0.2820\n",
            "Epoch [36/100], Training loss: 0.2739\n",
            "Epoch [37/100], Training loss: 0.2648\n",
            "Epoch [38/100], Training loss: 0.2569\n",
            "Epoch [39/100], Training loss: 0.2475\n",
            "Epoch [40/100], Training loss: 0.2418\n",
            "Epoch [41/100], Training loss: 0.2361\n",
            "Epoch [42/100], Training loss: 0.2258\n",
            "Epoch [43/100], Training loss: 0.2201\n",
            "Epoch [44/100], Training loss: 0.2111\n",
            "Epoch [45/100], Training loss: 0.2066\n",
            "Epoch [46/100], Training loss: 0.1984\n",
            "Epoch [47/100], Training loss: 0.1931\n",
            "Epoch [48/100], Training loss: 0.1864\n",
            "Epoch [49/100], Training loss: 0.1800\n",
            "Epoch [50/100], Training loss: 0.1729\n",
            "Epoch [51/100], Training loss: 0.1690\n",
            "Epoch [52/100], Training loss: 0.1646\n",
            "Epoch [53/100], Training loss: 0.1568\n",
            "Epoch [54/100], Training loss: 0.1529\n",
            "Epoch [55/100], Training loss: 0.1471\n",
            "Epoch [56/100], Training loss: 0.1420\n",
            "Epoch [57/100], Training loss: 0.1372\n",
            "Epoch [58/100], Training loss: 0.1322\n",
            "Epoch [59/100], Training loss: 0.1293\n",
            "Epoch [60/100], Training loss: 0.1235\n",
            "Epoch [61/100], Training loss: 0.1192\n",
            "Epoch [62/100], Training loss: 0.1155\n",
            "Epoch [63/100], Training loss: 0.1137\n",
            "Epoch [64/100], Training loss: 0.1084\n",
            "Epoch [65/100], Training loss: 0.1045\n",
            "Epoch [66/100], Training loss: 0.1019\n",
            "Epoch [67/100], Training loss: 0.1003\n",
            "Epoch [68/100], Training loss: 0.0951\n",
            "Epoch [69/100], Training loss: 0.0933\n",
            "Epoch [70/100], Training loss: 0.0899\n",
            "Epoch [71/100], Training loss: 0.0889\n",
            "Epoch [72/100], Training loss: 0.0854\n",
            "Epoch [73/100], Training loss: 0.0814\n",
            "Epoch [74/100], Training loss: 0.0766\n",
            "Epoch [75/100], Training loss: 0.0753\n",
            "Epoch [76/100], Training loss: 0.0783\n",
            "Epoch [77/100], Training loss: 0.0759\n",
            "Epoch [78/100], Training loss: 0.0692\n",
            "Epoch [79/100], Training loss: 0.0703\n",
            "Epoch [80/100], Training loss: 0.0604\n",
            "Epoch [81/100], Training loss: 0.0654\n",
            "Epoch [82/100], Training loss: 0.0668\n",
            "Epoch [83/100], Training loss: 0.0620\n",
            "Epoch [84/100], Training loss: 0.0637\n",
            "Epoch [85/100], Training loss: 0.0686\n",
            "Epoch [86/100], Training loss: 0.0568\n",
            "Epoch [87/100], Training loss: 0.0572\n",
            "Epoch [88/100], Training loss: 0.0586\n",
            "Epoch [89/100], Training loss: 0.0535\n",
            "Epoch [90/100], Training loss: 0.0554\n",
            "Epoch [91/100], Training loss: 0.0501\n",
            "Epoch [92/100], Training loss: 0.0539\n",
            "Epoch [93/100], Training loss: 0.0501\n",
            "Epoch [94/100], Training loss: 0.0523\n",
            "Epoch [95/100], Training loss: 0.0512\n",
            "Epoch [96/100], Training loss: 0.0459\n",
            "Epoch [97/100], Training loss: 0.0520\n",
            "Epoch [98/100], Training loss: 0.0475\n",
            "Epoch [99/100], Training loss: 0.0470\n",
            "Epoch [100/100], Training loss: 0.0455\n",
            "Training complete.\n",
            "Epoch [1/100], Training loss: 0.6917\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-43-08011d4dd45a>:49: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  train_preds = model(torch.tensor(X_train, dtype=torch.float32))\n",
            "<ipython-input-43-08011d4dd45a>:55: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  val_preds = model(torch.tensor(X_val, dtype=torch.float32))\n",
            "<ipython-input-43-08011d4dd45a>:32: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  train_dataset = TensorDataset(torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.long))\n",
            "<ipython-input-43-08011d4dd45a>:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  val_dataset = TensorDataset(torch.tensor(X_val, dtype=torch.float32), torch.tensor(y_val, dtype=torch.long))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [2/100], Training loss: 0.6854\n",
            "Epoch [3/100], Training loss: 0.6792\n",
            "Epoch [4/100], Training loss: 0.6725\n",
            "Epoch [5/100], Training loss: 0.6651\n",
            "Epoch [6/100], Training loss: 0.6570\n",
            "Epoch [7/100], Training loss: 0.6481\n",
            "Epoch [8/100], Training loss: 0.6380\n",
            "Epoch [9/100], Training loss: 0.6267\n",
            "Epoch [10/100], Training loss: 0.6142\n",
            "Epoch [11/100], Training loss: 0.6007\n",
            "Epoch [12/100], Training loss: 0.5866\n",
            "Epoch [13/100], Training loss: 0.5720\n",
            "Epoch [14/100], Training loss: 0.5568\n",
            "Epoch [15/100], Training loss: 0.5422\n",
            "Epoch [16/100], Training loss: 0.5279\n",
            "Epoch [17/100], Training loss: 0.5133\n",
            "Epoch [18/100], Training loss: 0.4995\n",
            "Epoch [19/100], Training loss: 0.4859\n",
            "Epoch [20/100], Training loss: 0.4733\n",
            "Epoch [21/100], Training loss: 0.4607\n",
            "Epoch [22/100], Training loss: 0.4484\n",
            "Epoch [23/100], Training loss: 0.4368\n",
            "Epoch [24/100], Training loss: 0.4254\n",
            "Epoch [25/100], Training loss: 0.4137\n",
            "Epoch [26/100], Training loss: 0.4029\n",
            "Epoch [27/100], Training loss: 0.3923\n",
            "Epoch [28/100], Training loss: 0.3811\n",
            "Epoch [29/100], Training loss: 0.3712\n",
            "Epoch [30/100], Training loss: 0.3597\n",
            "Epoch [31/100], Training loss: 0.3494\n",
            "Epoch [32/100], Training loss: 0.3398\n",
            "Epoch [33/100], Training loss: 0.3288\n",
            "Epoch [34/100], Training loss: 0.3209\n",
            "Epoch [35/100], Training loss: 0.3100\n",
            "Epoch [36/100], Training loss: 0.3007\n",
            "Epoch [37/100], Training loss: 0.2919\n",
            "Epoch [38/100], Training loss: 0.2831\n",
            "Epoch [39/100], Training loss: 0.2737\n",
            "Epoch [40/100], Training loss: 0.2652\n",
            "Epoch [41/100], Training loss: 0.2555\n",
            "Epoch [42/100], Training loss: 0.2487\n",
            "Epoch [43/100], Training loss: 0.2391\n",
            "Epoch [44/100], Training loss: 0.2323\n",
            "Epoch [45/100], Training loss: 0.2245\n",
            "Epoch [46/100], Training loss: 0.2162\n",
            "Epoch [47/100], Training loss: 0.2090\n",
            "Epoch [48/100], Training loss: 0.2009\n",
            "Epoch [49/100], Training loss: 0.1937\n",
            "Epoch [50/100], Training loss: 0.1868\n",
            "Epoch [51/100], Training loss: 0.1817\n",
            "Epoch [52/100], Training loss: 0.1754\n",
            "Epoch [53/100], Training loss: 0.1696\n",
            "Epoch [54/100], Training loss: 0.1635\n",
            "Epoch [55/100], Training loss: 0.1578\n",
            "Epoch [56/100], Training loss: 0.1491\n",
            "Epoch [57/100], Training loss: 0.1460\n",
            "Epoch [58/100], Training loss: 0.1425\n",
            "Epoch [59/100], Training loss: 0.1364\n",
            "Epoch [60/100], Training loss: 0.1314\n",
            "Epoch [61/100], Training loss: 0.1265\n",
            "Epoch [62/100], Training loss: 0.1222\n",
            "Epoch [63/100], Training loss: 0.1176\n",
            "Epoch [64/100], Training loss: 0.1122\n",
            "Epoch [65/100], Training loss: 0.1096\n",
            "Epoch [66/100], Training loss: 0.1067\n",
            "Epoch [67/100], Training loss: 0.1030\n",
            "Epoch [68/100], Training loss: 0.0971\n",
            "Epoch [69/100], Training loss: 0.0950\n",
            "Epoch [70/100], Training loss: 0.0914\n",
            "Epoch [71/100], Training loss: 0.0886\n",
            "Epoch [72/100], Training loss: 0.0894\n",
            "Epoch [73/100], Training loss: 0.0861\n",
            "Epoch [74/100], Training loss: 0.0812\n",
            "Epoch [75/100], Training loss: 0.0797\n",
            "Epoch [76/100], Training loss: 0.0771\n",
            "Epoch [77/100], Training loss: 0.0724\n",
            "Epoch [78/100], Training loss: 0.0716\n",
            "Epoch [79/100], Training loss: 0.0688\n",
            "Epoch [80/100], Training loss: 0.0667\n",
            "Epoch [81/100], Training loss: 0.0651\n",
            "Epoch [82/100], Training loss: 0.0636\n",
            "Epoch [83/100], Training loss: 0.0626\n",
            "Epoch [84/100], Training loss: 0.0593\n",
            "Epoch [85/100], Training loss: 0.0574\n",
            "Epoch [86/100], Training loss: 0.0576\n",
            "Epoch [87/100], Training loss: 0.0547\n",
            "Epoch [88/100], Training loss: 0.0546\n",
            "Epoch [89/100], Training loss: 0.0522\n",
            "Epoch [90/100], Training loss: 0.0510\n",
            "Epoch [91/100], Training loss: 0.0500\n",
            "Epoch [92/100], Training loss: 0.0527\n",
            "Epoch [93/100], Training loss: 0.0476\n",
            "Epoch [94/100], Training loss: 0.0472\n",
            "Epoch [95/100], Training loss: 0.0462\n",
            "Epoch [96/100], Training loss: 0.0429\n",
            "Epoch [97/100], Training loss: 0.0420\n",
            "Epoch [98/100], Training loss: 0.0455\n",
            "Epoch [99/100], Training loss: 0.0425\n",
            "Epoch [100/100], Training loss: 0.0440\n",
            "Training complete.\n",
            "Epoch [1/100], Training loss: 0.6912\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-43-08011d4dd45a>:49: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  train_preds = model(torch.tensor(X_train, dtype=torch.float32))\n",
            "<ipython-input-43-08011d4dd45a>:55: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  val_preds = model(torch.tensor(X_val, dtype=torch.float32))\n",
            "<ipython-input-43-08011d4dd45a>:32: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  train_dataset = TensorDataset(torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.long))\n",
            "<ipython-input-43-08011d4dd45a>:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  val_dataset = TensorDataset(torch.tensor(X_val, dtype=torch.float32), torch.tensor(y_val, dtype=torch.long))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [2/100], Training loss: 0.6818\n",
            "Epoch [3/100], Training loss: 0.6723\n",
            "Epoch [4/100], Training loss: 0.6613\n",
            "Epoch [5/100], Training loss: 0.6487\n",
            "Epoch [6/100], Training loss: 0.6341\n",
            "Epoch [7/100], Training loss: 0.6184\n",
            "Epoch [8/100], Training loss: 0.6019\n",
            "Epoch [9/100], Training loss: 0.5849\n",
            "Epoch [10/100], Training loss: 0.5677\n",
            "Epoch [11/100], Training loss: 0.5508\n",
            "Epoch [12/100], Training loss: 0.5341\n",
            "Epoch [13/100], Training loss: 0.5178\n",
            "Epoch [14/100], Training loss: 0.5022\n",
            "Epoch [15/100], Training loss: 0.4866\n",
            "Epoch [16/100], Training loss: 0.4718\n",
            "Epoch [17/100], Training loss: 0.4576\n",
            "Epoch [18/100], Training loss: 0.4439\n",
            "Epoch [19/100], Training loss: 0.4303\n",
            "Epoch [20/100], Training loss: 0.4174\n",
            "Epoch [21/100], Training loss: 0.4047\n",
            "Epoch [22/100], Training loss: 0.3929\n",
            "Epoch [23/100], Training loss: 0.3813\n",
            "Epoch [24/100], Training loss: 0.3699\n",
            "Epoch [25/100], Training loss: 0.3582\n",
            "Epoch [26/100], Training loss: 0.3473\n",
            "Epoch [27/100], Training loss: 0.3370\n",
            "Epoch [28/100], Training loss: 0.3264\n",
            "Epoch [29/100], Training loss: 0.3164\n",
            "Epoch [30/100], Training loss: 0.3061\n",
            "Epoch [31/100], Training loss: 0.2966\n",
            "Epoch [32/100], Training loss: 0.2877\n",
            "Epoch [33/100], Training loss: 0.2786\n",
            "Epoch [34/100], Training loss: 0.2682\n",
            "Epoch [35/100], Training loss: 0.2601\n",
            "Epoch [36/100], Training loss: 0.2511\n",
            "Epoch [37/100], Training loss: 0.2441\n",
            "Epoch [38/100], Training loss: 0.2340\n",
            "Epoch [39/100], Training loss: 0.2273\n",
            "Epoch [40/100], Training loss: 0.2196\n",
            "Epoch [41/100], Training loss: 0.2120\n",
            "Epoch [42/100], Training loss: 0.2054\n",
            "Epoch [43/100], Training loss: 0.1981\n",
            "Epoch [44/100], Training loss: 0.1905\n",
            "Epoch [45/100], Training loss: 0.1861\n",
            "Epoch [46/100], Training loss: 0.1784\n",
            "Epoch [47/100], Training loss: 0.1718\n",
            "Epoch [48/100], Training loss: 0.1661\n",
            "Epoch [49/100], Training loss: 0.1596\n",
            "Epoch [50/100], Training loss: 0.1558\n",
            "Epoch [51/100], Training loss: 0.1493\n",
            "Epoch [52/100], Training loss: 0.1437\n",
            "Epoch [53/100], Training loss: 0.1387\n",
            "Epoch [54/100], Training loss: 0.1346\n",
            "Epoch [55/100], Training loss: 0.1286\n",
            "Epoch [56/100], Training loss: 0.1249\n",
            "Epoch [57/100], Training loss: 0.1209\n",
            "Epoch [58/100], Training loss: 0.1179\n",
            "Epoch [59/100], Training loss: 0.1109\n",
            "Epoch [60/100], Training loss: 0.1091\n",
            "Epoch [61/100], Training loss: 0.1051\n",
            "Epoch [62/100], Training loss: 0.1021\n",
            "Epoch [63/100], Training loss: 0.1000\n",
            "Epoch [64/100], Training loss: 0.0940\n",
            "Epoch [65/100], Training loss: 0.0923\n",
            "Epoch [66/100], Training loss: 0.0894\n",
            "Epoch [67/100], Training loss: 0.0859\n",
            "Epoch [68/100], Training loss: 0.0835\n",
            "Epoch [69/100], Training loss: 0.0798\n",
            "Epoch [70/100], Training loss: 0.0788\n",
            "Epoch [71/100], Training loss: 0.0764\n",
            "Epoch [72/100], Training loss: 0.0730\n",
            "Epoch [73/100], Training loss: 0.0723\n",
            "Epoch [74/100], Training loss: 0.0689\n",
            "Epoch [75/100], Training loss: 0.0677\n",
            "Epoch [76/100], Training loss: 0.0662\n",
            "Epoch [77/100], Training loss: 0.0628\n",
            "Epoch [78/100], Training loss: 0.0625\n",
            "Epoch [79/100], Training loss: 0.0623\n",
            "Epoch [80/100], Training loss: 0.0595\n",
            "Epoch [81/100], Training loss: 0.0579\n",
            "Epoch [82/100], Training loss: 0.0574\n",
            "Epoch [83/100], Training loss: 0.0543\n",
            "Epoch [84/100], Training loss: 0.0535\n",
            "Epoch [85/100], Training loss: 0.0531\n",
            "Epoch [86/100], Training loss: 0.0516\n",
            "Epoch [87/100], Training loss: 0.0501\n",
            "Epoch [88/100], Training loss: 0.0490\n",
            "Epoch [89/100], Training loss: 0.0491\n",
            "Epoch [90/100], Training loss: 0.0444\n",
            "Epoch [91/100], Training loss: 0.0472\n",
            "Epoch [92/100], Training loss: 0.0453\n",
            "Epoch [93/100], Training loss: 0.0437\n",
            "Epoch [94/100], Training loss: 0.0435\n",
            "Epoch [95/100], Training loss: 0.0398\n",
            "Epoch [96/100], Training loss: 0.0419\n",
            "Epoch [97/100], Training loss: 0.0408\n",
            "Epoch [98/100], Training loss: 0.0410\n",
            "Epoch [99/100], Training loss: 0.0367\n",
            "Epoch [100/100], Training loss: 0.0402\n",
            "Training complete.\n",
            "Epoch [1/100], Training loss: 0.6923\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-43-08011d4dd45a>:49: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  train_preds = model(torch.tensor(X_train, dtype=torch.float32))\n",
            "<ipython-input-43-08011d4dd45a>:55: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  val_preds = model(torch.tensor(X_val, dtype=torch.float32))\n",
            "<ipython-input-43-08011d4dd45a>:32: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  train_dataset = TensorDataset(torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.long))\n",
            "<ipython-input-43-08011d4dd45a>:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  val_dataset = TensorDataset(torch.tensor(X_val, dtype=torch.float32), torch.tensor(y_val, dtype=torch.long))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [2/100], Training loss: 0.6845\n",
            "Epoch [3/100], Training loss: 0.6765\n",
            "Epoch [4/100], Training loss: 0.6677\n",
            "Epoch [5/100], Training loss: 0.6578\n",
            "Epoch [6/100], Training loss: 0.6468\n",
            "Epoch [7/100], Training loss: 0.6347\n",
            "Epoch [8/100], Training loss: 0.6218\n",
            "Epoch [9/100], Training loss: 0.6081\n",
            "Epoch [10/100], Training loss: 0.5937\n",
            "Epoch [11/100], Training loss: 0.5789\n",
            "Epoch [12/100], Training loss: 0.5635\n",
            "Epoch [13/100], Training loss: 0.5480\n",
            "Epoch [14/100], Training loss: 0.5324\n",
            "Epoch [15/100], Training loss: 0.5164\n",
            "Epoch [16/100], Training loss: 0.5003\n",
            "Epoch [17/100], Training loss: 0.4847\n",
            "Epoch [18/100], Training loss: 0.4695\n",
            "Epoch [19/100], Training loss: 0.4549\n",
            "Epoch [20/100], Training loss: 0.4409\n",
            "Epoch [21/100], Training loss: 0.4276\n",
            "Epoch [22/100], Training loss: 0.4143\n",
            "Epoch [23/100], Training loss: 0.4022\n",
            "Epoch [24/100], Training loss: 0.3894\n",
            "Epoch [25/100], Training loss: 0.3779\n",
            "Epoch [26/100], Training loss: 0.3665\n",
            "Epoch [27/100], Training loss: 0.3555\n",
            "Epoch [28/100], Training loss: 0.3447\n",
            "Epoch [29/100], Training loss: 0.3345\n",
            "Epoch [30/100], Training loss: 0.3249\n",
            "Epoch [31/100], Training loss: 0.3143\n",
            "Epoch [32/100], Training loss: 0.3052\n",
            "Epoch [33/100], Training loss: 0.2954\n",
            "Epoch [34/100], Training loss: 0.2868\n",
            "Epoch [35/100], Training loss: 0.2783\n",
            "Epoch [36/100], Training loss: 0.2690\n",
            "Epoch [37/100], Training loss: 0.2632\n",
            "Epoch [38/100], Training loss: 0.2526\n",
            "Epoch [39/100], Training loss: 0.2442\n",
            "Epoch [40/100], Training loss: 0.2366\n",
            "Epoch [41/100], Training loss: 0.2275\n",
            "Epoch [42/100], Training loss: 0.2208\n",
            "Epoch [43/100], Training loss: 0.2139\n",
            "Epoch [44/100], Training loss: 0.2074\n",
            "Epoch [45/100], Training loss: 0.1995\n",
            "Epoch [46/100], Training loss: 0.1940\n",
            "Epoch [47/100], Training loss: 0.1861\n",
            "Epoch [48/100], Training loss: 0.1782\n",
            "Epoch [49/100], Training loss: 0.1757\n",
            "Epoch [50/100], Training loss: 0.1696\n",
            "Epoch [51/100], Training loss: 0.1640\n",
            "Epoch [52/100], Training loss: 0.1563\n",
            "Epoch [53/100], Training loss: 0.1537\n",
            "Epoch [54/100], Training loss: 0.1487\n",
            "Epoch [55/100], Training loss: 0.1433\n",
            "Epoch [56/100], Training loss: 0.1370\n",
            "Epoch [57/100], Training loss: 0.1363\n",
            "Epoch [58/100], Training loss: 0.1277\n",
            "Epoch [59/100], Training loss: 0.1277\n",
            "Epoch [60/100], Training loss: 0.1217\n",
            "Epoch [61/100], Training loss: 0.1180\n",
            "Epoch [62/100], Training loss: 0.1140\n",
            "Epoch [63/100], Training loss: 0.1106\n",
            "Epoch [64/100], Training loss: 0.1030\n",
            "Epoch [65/100], Training loss: 0.1015\n",
            "Epoch [66/100], Training loss: 0.1005\n",
            "Epoch [67/100], Training loss: 0.0968\n",
            "Epoch [68/100], Training loss: 0.0921\n",
            "Epoch [69/100], Training loss: 0.0940\n",
            "Epoch [70/100], Training loss: 0.0845\n",
            "Epoch [71/100], Training loss: 0.0918\n",
            "Epoch [72/100], Training loss: 0.0813\n",
            "Epoch [73/100], Training loss: 0.0802\n",
            "Epoch [74/100], Training loss: 0.0783\n",
            "Epoch [75/100], Training loss: 0.0739\n",
            "Epoch [76/100], Training loss: 0.0698\n",
            "Epoch [77/100], Training loss: 0.0713\n",
            "Epoch [78/100], Training loss: 0.0697\n",
            "Epoch [79/100], Training loss: 0.0732\n",
            "Epoch [80/100], Training loss: 0.0659\n",
            "Epoch [81/100], Training loss: 0.0662\n",
            "Epoch [82/100], Training loss: 0.0604\n",
            "Epoch [83/100], Training loss: 0.0605\n",
            "Epoch [84/100], Training loss: 0.0597\n",
            "Epoch [85/100], Training loss: 0.0623\n",
            "Epoch [86/100], Training loss: 0.0613\n",
            "Epoch [87/100], Training loss: 0.0594\n",
            "Epoch [88/100], Training loss: 0.0539\n",
            "Epoch [89/100], Training loss: 0.0572\n",
            "Epoch [90/100], Training loss: 0.0526\n",
            "Epoch [91/100], Training loss: 0.0514\n",
            "Epoch [92/100], Training loss: 0.0497\n",
            "Epoch [93/100], Training loss: 0.0516\n",
            "Epoch [94/100], Training loss: 0.0473\n",
            "Epoch [95/100], Training loss: 0.0487\n",
            "Epoch [96/100], Training loss: 0.0458\n",
            "Epoch [97/100], Training loss: 0.0495\n",
            "Epoch [98/100], Training loss: 0.0464\n",
            "Epoch [99/100], Training loss: 0.0418\n",
            "Epoch [100/100], Training loss: 0.0429\n",
            "Training complete.\n",
            "Epoch [1/100], Training loss: 0.6958\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-43-08011d4dd45a>:49: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  train_preds = model(torch.tensor(X_train, dtype=torch.float32))\n",
            "<ipython-input-43-08011d4dd45a>:55: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  val_preds = model(torch.tensor(X_val, dtype=torch.float32))\n",
            "<ipython-input-43-08011d4dd45a>:32: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  train_dataset = TensorDataset(torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.long))\n",
            "<ipython-input-43-08011d4dd45a>:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  val_dataset = TensorDataset(torch.tensor(X_val, dtype=torch.float32), torch.tensor(y_val, dtype=torch.long))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [2/100], Training loss: 0.6873\n",
            "Epoch [3/100], Training loss: 0.6788\n",
            "Epoch [4/100], Training loss: 0.6697\n",
            "Epoch [5/100], Training loss: 0.6595\n",
            "Epoch [6/100], Training loss: 0.6481\n",
            "Epoch [7/100], Training loss: 0.6356\n",
            "Epoch [8/100], Training loss: 0.6219\n",
            "Epoch [9/100], Training loss: 0.6075\n",
            "Epoch [10/100], Training loss: 0.5928\n",
            "Epoch [11/100], Training loss: 0.5779\n",
            "Epoch [12/100], Training loss: 0.5629\n",
            "Epoch [13/100], Training loss: 0.5480\n",
            "Epoch [14/100], Training loss: 0.5332\n",
            "Epoch [15/100], Training loss: 0.5192\n",
            "Epoch [16/100], Training loss: 0.5051\n",
            "Epoch [17/100], Training loss: 0.4908\n",
            "Epoch [18/100], Training loss: 0.4781\n",
            "Epoch [19/100], Training loss: 0.4650\n",
            "Epoch [20/100], Training loss: 0.4533\n",
            "Epoch [21/100], Training loss: 0.4412\n",
            "Epoch [22/100], Training loss: 0.4301\n",
            "Epoch [23/100], Training loss: 0.4186\n",
            "Epoch [24/100], Training loss: 0.4081\n",
            "Epoch [25/100], Training loss: 0.3974\n",
            "Epoch [26/100], Training loss: 0.3880\n",
            "Epoch [27/100], Training loss: 0.3779\n",
            "Epoch [28/100], Training loss: 0.3689\n",
            "Epoch [29/100], Training loss: 0.3595\n",
            "Epoch [30/100], Training loss: 0.3500\n",
            "Epoch [31/100], Training loss: 0.3408\n",
            "Epoch [32/100], Training loss: 0.3329\n",
            "Epoch [33/100], Training loss: 0.3239\n",
            "Epoch [34/100], Training loss: 0.3158\n",
            "Epoch [35/100], Training loss: 0.3071\n",
            "Epoch [36/100], Training loss: 0.2989\n",
            "Epoch [37/100], Training loss: 0.2911\n",
            "Epoch [38/100], Training loss: 0.2826\n",
            "Epoch [39/100], Training loss: 0.2760\n",
            "Epoch [40/100], Training loss: 0.2683\n",
            "Epoch [41/100], Training loss: 0.2611\n",
            "Epoch [42/100], Training loss: 0.2532\n",
            "Epoch [43/100], Training loss: 0.2471\n",
            "Epoch [44/100], Training loss: 0.2394\n",
            "Epoch [45/100], Training loss: 0.2312\n",
            "Epoch [46/100], Training loss: 0.2260\n",
            "Epoch [47/100], Training loss: 0.2187\n",
            "Epoch [48/100], Training loss: 0.2138\n",
            "Epoch [49/100], Training loss: 0.2069\n",
            "Epoch [50/100], Training loss: 0.1998\n",
            "Epoch [51/100], Training loss: 0.1941\n",
            "Epoch [52/100], Training loss: 0.1882\n",
            "Epoch [53/100], Training loss: 0.1827\n",
            "Epoch [54/100], Training loss: 0.1769\n",
            "Epoch [55/100], Training loss: 0.1713\n",
            "Epoch [56/100], Training loss: 0.1668\n",
            "Epoch [57/100], Training loss: 0.1600\n",
            "Epoch [58/100], Training loss: 0.1555\n",
            "Epoch [59/100], Training loss: 0.1508\n",
            "Epoch [60/100], Training loss: 0.1445\n",
            "Epoch [61/100], Training loss: 0.1426\n",
            "Epoch [62/100], Training loss: 0.1370\n",
            "Epoch [63/100], Training loss: 0.1329\n",
            "Epoch [64/100], Training loss: 0.1277\n",
            "Epoch [65/100], Training loss: 0.1241\n",
            "Epoch [66/100], Training loss: 0.1198\n",
            "Epoch [67/100], Training loss: 0.1163\n",
            "Epoch [68/100], Training loss: 0.1120\n",
            "Epoch [69/100], Training loss: 0.1092\n",
            "Epoch [70/100], Training loss: 0.1044\n",
            "Epoch [71/100], Training loss: 0.1022\n",
            "Epoch [72/100], Training loss: 0.0977\n",
            "Epoch [73/100], Training loss: 0.0940\n",
            "Epoch [74/100], Training loss: 0.0923\n",
            "Epoch [75/100], Training loss: 0.0886\n",
            "Epoch [76/100], Training loss: 0.0869\n",
            "Epoch [77/100], Training loss: 0.0848\n",
            "Epoch [78/100], Training loss: 0.0817\n",
            "Epoch [79/100], Training loss: 0.0808\n",
            "Epoch [80/100], Training loss: 0.0775\n",
            "Epoch [81/100], Training loss: 0.0752\n",
            "Epoch [82/100], Training loss: 0.0731\n",
            "Epoch [83/100], Training loss: 0.0707\n",
            "Epoch [84/100], Training loss: 0.0692\n",
            "Epoch [85/100], Training loss: 0.0674\n",
            "Epoch [86/100], Training loss: 0.0655\n",
            "Epoch [87/100], Training loss: 0.0633\n",
            "Epoch [88/100], Training loss: 0.0629\n",
            "Epoch [89/100], Training loss: 0.0606\n",
            "Epoch [90/100], Training loss: 0.0580\n",
            "Epoch [91/100], Training loss: 0.0585\n",
            "Epoch [92/100], Training loss: 0.0576\n",
            "Epoch [93/100], Training loss: 0.0558\n",
            "Epoch [94/100], Training loss: 0.0559\n",
            "Epoch [95/100], Training loss: 0.0530\n",
            "Epoch [96/100], Training loss: 0.0508\n",
            "Epoch [97/100], Training loss: 0.0480\n",
            "Epoch [98/100], Training loss: 0.0522\n",
            "Epoch [99/100], Training loss: 0.0495\n",
            "Epoch [100/100], Training loss: 0.0481\n",
            "Training complete.\n",
            "Epoch [1/100], Training loss: 0.6459\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-43-08011d4dd45a>:49: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  train_preds = model(torch.tensor(X_train, dtype=torch.float32))\n",
            "<ipython-input-43-08011d4dd45a>:55: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  val_preds = model(torch.tensor(X_val, dtype=torch.float32))\n",
            "<ipython-input-43-08011d4dd45a>:32: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  train_dataset = TensorDataset(torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.long))\n",
            "<ipython-input-43-08011d4dd45a>:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  val_dataset = TensorDataset(torch.tensor(X_val, dtype=torch.float32), torch.tensor(y_val, dtype=torch.long))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [2/100], Training loss: 0.4990\n",
            "Epoch [3/100], Training loss: 0.3922\n",
            "Epoch [4/100], Training loss: 0.3169\n",
            "Epoch [5/100], Training loss: 0.2717\n",
            "Epoch [6/100], Training loss: 0.1991\n",
            "Epoch [7/100], Training loss: 0.1620\n",
            "Epoch [8/100], Training loss: 0.1151\n",
            "Epoch [9/100], Training loss: 0.0824\n",
            "Epoch [10/100], Training loss: 0.0814\n",
            "Epoch [11/100], Training loss: 0.0467\n",
            "Epoch [12/100], Training loss: 0.0323\n",
            "Epoch [13/100], Training loss: 0.0280\n",
            "Epoch [14/100], Training loss: 0.0210\n",
            "Epoch [15/100], Training loss: 0.0202\n",
            "Epoch [16/100], Training loss: 0.0163\n",
            "Epoch [17/100], Training loss: 0.0150\n",
            "Epoch [18/100], Training loss: 0.0115\n",
            "Epoch [19/100], Training loss: 0.0118\n",
            "Epoch [20/100], Training loss: 0.0101\n",
            "Epoch [21/100], Training loss: 0.0101\n",
            "Epoch [22/100], Training loss: 0.0084\n",
            "Epoch [23/100], Training loss: 0.0073\n",
            "Epoch [24/100], Training loss: 0.0060\n",
            "Epoch [25/100], Training loss: 0.0051\n",
            "Epoch [26/100], Training loss: 0.0056\n",
            "Epoch [27/100], Training loss: 0.0039\n",
            "Epoch [28/100], Training loss: 0.0032\n",
            "Epoch [29/100], Training loss: 0.0045\n",
            "Epoch [30/100], Training loss: 0.0049\n",
            "Epoch [31/100], Training loss: 0.0051\n",
            "Epoch [32/100], Training loss: 0.0029\n",
            "Epoch [33/100], Training loss: 0.0025\n",
            "Epoch [34/100], Training loss: 0.0023\n",
            "Epoch [35/100], Training loss: 0.0030\n",
            "Epoch [36/100], Training loss: 0.0026\n",
            "Epoch [37/100], Training loss: 0.0023\n",
            "Epoch [38/100], Training loss: 0.0025\n",
            "Epoch [39/100], Training loss: 0.0030\n",
            "Epoch [40/100], Training loss: 0.0020\n",
            "Epoch [41/100], Training loss: 0.0019\n",
            "Epoch [42/100], Training loss: 0.0015\n",
            "Epoch [43/100], Training loss: 0.0014\n",
            "Epoch [44/100], Training loss: 0.0013\n",
            "Epoch [45/100], Training loss: 0.0012\n",
            "Epoch [46/100], Training loss: 0.0011\n",
            "Epoch [47/100], Training loss: 0.0013\n",
            "Epoch [48/100], Training loss: 0.0010\n",
            "Epoch [49/100], Training loss: 0.0011\n",
            "Epoch [50/100], Training loss: 0.0011\n",
            "Epoch [51/100], Training loss: 0.0010\n",
            "Epoch [52/100], Training loss: 0.0009\n",
            "Epoch [53/100], Training loss: 0.0008\n",
            "Epoch [54/100], Training loss: 0.0008\n",
            "Epoch [55/100], Training loss: 0.0007\n",
            "Epoch [56/100], Training loss: 0.0008\n",
            "Epoch [57/100], Training loss: 0.0007\n",
            "Epoch [58/100], Training loss: 0.0007\n",
            "Epoch [59/100], Training loss: 0.0006\n",
            "Epoch [60/100], Training loss: 0.0008\n",
            "Epoch [61/100], Training loss: 0.0007\n",
            "Epoch [62/100], Training loss: 0.0006\n",
            "Epoch [63/100], Training loss: 0.0006\n",
            "Epoch [64/100], Training loss: 0.0006\n",
            "Epoch [65/100], Training loss: 0.0006\n",
            "Epoch [66/100], Training loss: 0.0005\n",
            "Epoch [67/100], Training loss: 0.0006\n",
            "Epoch [68/100], Training loss: 0.0006\n",
            "Epoch [69/100], Training loss: 0.0005\n",
            "Epoch [70/100], Training loss: 0.0005\n",
            "Epoch [71/100], Training loss: 0.0005\n",
            "Epoch [72/100], Training loss: 0.0005\n",
            "Epoch [73/100], Training loss: 0.0005\n",
            "Epoch [74/100], Training loss: 0.0005\n",
            "Epoch [75/100], Training loss: 0.0005\n",
            "Epoch [76/100], Training loss: 0.0004\n",
            "Epoch [77/100], Training loss: 0.0004\n",
            "Epoch [78/100], Training loss: 0.0004\n",
            "Epoch [79/100], Training loss: 0.0004\n",
            "Epoch [80/100], Training loss: 0.0004\n",
            "Epoch [81/100], Training loss: 0.0004\n",
            "Epoch [82/100], Training loss: 0.0004\n",
            "Epoch [83/100], Training loss: 0.0004\n",
            "Epoch [84/100], Training loss: 0.0004\n",
            "Epoch [85/100], Training loss: 0.0004\n",
            "Epoch [86/100], Training loss: 0.0004\n",
            "Epoch [87/100], Training loss: 0.0004\n",
            "Epoch [88/100], Training loss: 0.0003\n",
            "Epoch [89/100], Training loss: 0.0004\n",
            "Epoch [90/100], Training loss: 0.0003\n",
            "Epoch [91/100], Training loss: 0.0003\n",
            "Epoch [92/100], Training loss: 0.0003\n",
            "Epoch [93/100], Training loss: 0.0003\n",
            "Epoch [94/100], Training loss: 0.0003\n",
            "Epoch [95/100], Training loss: 0.0003\n",
            "Epoch [96/100], Training loss: 0.0003\n",
            "Epoch [97/100], Training loss: 0.0003\n",
            "Epoch [98/100], Training loss: 0.0003\n",
            "Epoch [99/100], Training loss: 0.0003\n",
            "Epoch [100/100], Training loss: 0.0003\n",
            "Training complete.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-43-08011d4dd45a>:49: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  train_preds = model(torch.tensor(X_train, dtype=torch.float32))\n",
            "<ipython-input-43-08011d4dd45a>:55: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  val_preds = model(torch.tensor(X_val, dtype=torch.float32))\n",
            "<ipython-input-43-08011d4dd45a>:32: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  train_dataset = TensorDataset(torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.long))\n",
            "<ipython-input-43-08011d4dd45a>:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  val_dataset = TensorDataset(torch.tensor(X_val, dtype=torch.float32), torch.tensor(y_val, dtype=torch.long))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/100], Training loss: 0.6441\n",
            "Epoch [2/100], Training loss: 0.5225\n",
            "Epoch [3/100], Training loss: 0.4248\n",
            "Epoch [4/100], Training loss: 0.3521\n",
            "Epoch [5/100], Training loss: 0.3088\n",
            "Epoch [6/100], Training loss: 0.2364\n",
            "Epoch [7/100], Training loss: 0.1890\n",
            "Epoch [8/100], Training loss: 0.1496\n",
            "Epoch [9/100], Training loss: 0.1373\n",
            "Epoch [10/100], Training loss: 0.0896\n",
            "Epoch [11/100], Training loss: 0.0710\n",
            "Epoch [12/100], Training loss: 0.0599\n",
            "Epoch [13/100], Training loss: 0.0332\n",
            "Epoch [14/100], Training loss: 0.0243\n",
            "Epoch [15/100], Training loss: 0.0300\n",
            "Epoch [16/100], Training loss: 0.0225\n",
            "Epoch [17/100], Training loss: 0.0132\n",
            "Epoch [18/100], Training loss: 0.0121\n",
            "Epoch [19/100], Training loss: 0.0149\n",
            "Epoch [20/100], Training loss: 0.0125\n",
            "Epoch [21/100], Training loss: 0.0081\n",
            "Epoch [22/100], Training loss: 0.0046\n",
            "Epoch [23/100], Training loss: 0.0136\n",
            "Epoch [24/100], Training loss: 0.0103\n",
            "Epoch [25/100], Training loss: 0.0067\n",
            "Epoch [26/100], Training loss: 0.0062\n",
            "Epoch [27/100], Training loss: 0.0052\n",
            "Epoch [28/100], Training loss: 0.0060\n",
            "Epoch [29/100], Training loss: 0.0048\n",
            "Epoch [30/100], Training loss: 0.0041\n",
            "Epoch [31/100], Training loss: 0.0046\n",
            "Epoch [32/100], Training loss: 0.0024\n",
            "Epoch [33/100], Training loss: 0.0022\n",
            "Epoch [34/100], Training loss: 0.0020\n",
            "Epoch [35/100], Training loss: 0.0018\n",
            "Epoch [36/100], Training loss: 0.0015\n",
            "Epoch [37/100], Training loss: 0.0016\n",
            "Epoch [38/100], Training loss: 0.0013\n",
            "Epoch [39/100], Training loss: 0.0014\n",
            "Epoch [40/100], Training loss: 0.0012\n",
            "Epoch [41/100], Training loss: 0.0012\n",
            "Epoch [42/100], Training loss: 0.0011\n",
            "Epoch [43/100], Training loss: 0.0009\n",
            "Epoch [44/100], Training loss: 0.0011\n",
            "Epoch [45/100], Training loss: 0.0010\n",
            "Epoch [46/100], Training loss: 0.0010\n",
            "Epoch [47/100], Training loss: 0.0009\n",
            "Epoch [48/100], Training loss: 0.0008\n",
            "Epoch [49/100], Training loss: 0.0007\n",
            "Epoch [50/100], Training loss: 0.0007\n",
            "Epoch [51/100], Training loss: 0.0007\n",
            "Epoch [52/100], Training loss: 0.0006\n",
            "Epoch [53/100], Training loss: 0.0007\n",
            "Epoch [54/100], Training loss: 0.0006\n",
            "Epoch [55/100], Training loss: 0.0006\n",
            "Epoch [56/100], Training loss: 0.0006\n",
            "Epoch [57/100], Training loss: 0.0006\n",
            "Epoch [58/100], Training loss: 0.0005\n",
            "Epoch [59/100], Training loss: 0.0005\n",
            "Epoch [60/100], Training loss: 0.0005\n",
            "Epoch [61/100], Training loss: 0.0005\n",
            "Epoch [62/100], Training loss: 0.0005\n",
            "Epoch [63/100], Training loss: 0.0005\n",
            "Epoch [64/100], Training loss: 0.0005\n",
            "Epoch [65/100], Training loss: 0.0004\n",
            "Epoch [66/100], Training loss: 0.0004\n",
            "Epoch [67/100], Training loss: 0.0004\n",
            "Epoch [68/100], Training loss: 0.0004\n",
            "Epoch [69/100], Training loss: 0.0004\n",
            "Epoch [70/100], Training loss: 0.0004\n",
            "Epoch [71/100], Training loss: 0.0004\n",
            "Epoch [72/100], Training loss: 0.0004\n",
            "Epoch [73/100], Training loss: 0.0004\n",
            "Epoch [74/100], Training loss: 0.0004\n",
            "Epoch [75/100], Training loss: 0.0004\n",
            "Epoch [76/100], Training loss: 0.0003\n",
            "Epoch [77/100], Training loss: 0.0003\n",
            "Epoch [78/100], Training loss: 0.0003\n",
            "Epoch [79/100], Training loss: 0.0003\n",
            "Epoch [80/100], Training loss: 0.0003\n",
            "Epoch [81/100], Training loss: 0.0003\n",
            "Epoch [82/100], Training loss: 0.0003\n",
            "Epoch [83/100], Training loss: 0.0003\n",
            "Epoch [84/100], Training loss: 0.0003\n",
            "Epoch [85/100], Training loss: 0.0003\n",
            "Epoch [86/100], Training loss: 0.0003\n",
            "Epoch [87/100], Training loss: 0.0003\n",
            "Epoch [88/100], Training loss: 0.0003\n",
            "Epoch [89/100], Training loss: 0.0003\n",
            "Epoch [90/100], Training loss: 0.0003\n",
            "Epoch [91/100], Training loss: 0.0003\n",
            "Epoch [92/100], Training loss: 0.0003\n",
            "Epoch [93/100], Training loss: 0.0003\n",
            "Epoch [94/100], Training loss: 0.0003\n",
            "Epoch [95/100], Training loss: 0.0002\n",
            "Epoch [96/100], Training loss: 0.0002\n",
            "Epoch [97/100], Training loss: 0.0002\n",
            "Epoch [98/100], Training loss: 0.0002\n",
            "Epoch [99/100], Training loss: 0.0002\n",
            "Epoch [100/100], Training loss: 0.0002\n",
            "Training complete.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-43-08011d4dd45a>:49: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  train_preds = model(torch.tensor(X_train, dtype=torch.float32))\n",
            "<ipython-input-43-08011d4dd45a>:55: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  val_preds = model(torch.tensor(X_val, dtype=torch.float32))\n",
            "<ipython-input-43-08011d4dd45a>:32: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  train_dataset = TensorDataset(torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.long))\n",
            "<ipython-input-43-08011d4dd45a>:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  val_dataset = TensorDataset(torch.tensor(X_val, dtype=torch.float32), torch.tensor(y_val, dtype=torch.long))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/100], Training loss: 0.6482\n",
            "Epoch [2/100], Training loss: 0.5161\n",
            "Epoch [3/100], Training loss: 0.4128\n",
            "Epoch [4/100], Training loss: 0.3443\n",
            "Epoch [5/100], Training loss: 0.2869\n",
            "Epoch [6/100], Training loss: 0.2344\n",
            "Epoch [7/100], Training loss: 0.2044\n",
            "Epoch [8/100], Training loss: 0.1455\n",
            "Epoch [9/100], Training loss: 0.1139\n",
            "Epoch [10/100], Training loss: 0.0919\n",
            "Epoch [11/100], Training loss: 0.0645\n",
            "Epoch [12/100], Training loss: 0.0405\n",
            "Epoch [13/100], Training loss: 0.0409\n",
            "Epoch [14/100], Training loss: 0.0259\n",
            "Epoch [15/100], Training loss: 0.0205\n",
            "Epoch [16/100], Training loss: 0.0186\n",
            "Epoch [17/100], Training loss: 0.0158\n",
            "Epoch [18/100], Training loss: 0.0135\n",
            "Epoch [19/100], Training loss: 0.0117\n",
            "Epoch [20/100], Training loss: 0.0105\n",
            "Epoch [21/100], Training loss: 0.0085\n",
            "Epoch [22/100], Training loss: 0.0097\n",
            "Epoch [23/100], Training loss: 0.0073\n",
            "Epoch [24/100], Training loss: 0.0066\n",
            "Epoch [25/100], Training loss: 0.0063\n",
            "Epoch [26/100], Training loss: 0.0038\n",
            "Epoch [27/100], Training loss: 0.0058\n",
            "Epoch [28/100], Training loss: 0.0042\n",
            "Epoch [29/100], Training loss: 0.0043\n",
            "Epoch [30/100], Training loss: 0.0026\n",
            "Epoch [31/100], Training loss: 0.0023\n",
            "Epoch [32/100], Training loss: 0.0032\n",
            "Epoch [33/100], Training loss: 0.0029\n",
            "Epoch [34/100], Training loss: 0.0019\n",
            "Epoch [35/100], Training loss: 0.0019\n",
            "Epoch [36/100], Training loss: 0.0015\n",
            "Epoch [37/100], Training loss: 0.0015\n",
            "Epoch [38/100], Training loss: 0.0014\n",
            "Epoch [39/100], Training loss: 0.0015\n",
            "Epoch [40/100], Training loss: 0.0014\n",
            "Epoch [41/100], Training loss: 0.0012\n",
            "Epoch [42/100], Training loss: 0.0011\n",
            "Epoch [43/100], Training loss: 0.0011\n",
            "Epoch [44/100], Training loss: 0.0010\n",
            "Epoch [45/100], Training loss: 0.0011\n",
            "Epoch [46/100], Training loss: 0.0010\n",
            "Epoch [47/100], Training loss: 0.0009\n",
            "Epoch [48/100], Training loss: 0.0009\n",
            "Epoch [49/100], Training loss: 0.0009\n",
            "Epoch [50/100], Training loss: 0.0008\n",
            "Epoch [51/100], Training loss: 0.0008\n",
            "Epoch [52/100], Training loss: 0.0008\n",
            "Epoch [53/100], Training loss: 0.0008\n",
            "Epoch [54/100], Training loss: 0.0007\n",
            "Epoch [55/100], Training loss: 0.0007\n",
            "Epoch [56/100], Training loss: 0.0007\n",
            "Epoch [57/100], Training loss: 0.0007\n",
            "Epoch [58/100], Training loss: 0.0006\n",
            "Epoch [59/100], Training loss: 0.0007\n",
            "Epoch [60/100], Training loss: 0.0006\n",
            "Epoch [61/100], Training loss: 0.0006\n",
            "Epoch [62/100], Training loss: 0.0006\n",
            "Epoch [63/100], Training loss: 0.0005\n",
            "Epoch [64/100], Training loss: 0.0006\n",
            "Epoch [65/100], Training loss: 0.0005\n",
            "Epoch [66/100], Training loss: 0.0005\n",
            "Epoch [67/100], Training loss: 0.0005\n",
            "Epoch [68/100], Training loss: 0.0005\n",
            "Epoch [69/100], Training loss: 0.0005\n",
            "Epoch [70/100], Training loss: 0.0005\n",
            "Epoch [71/100], Training loss: 0.0005\n",
            "Epoch [72/100], Training loss: 0.0005\n",
            "Epoch [73/100], Training loss: 0.0004\n",
            "Epoch [74/100], Training loss: 0.0004\n",
            "Epoch [75/100], Training loss: 0.0004\n",
            "Epoch [76/100], Training loss: 0.0004\n",
            "Epoch [77/100], Training loss: 0.0004\n",
            "Epoch [78/100], Training loss: 0.0004\n",
            "Epoch [79/100], Training loss: 0.0004\n",
            "Epoch [80/100], Training loss: 0.0004\n",
            "Epoch [81/100], Training loss: 0.0004\n",
            "Epoch [82/100], Training loss: 0.0004\n",
            "Epoch [83/100], Training loss: 0.0004\n",
            "Epoch [84/100], Training loss: 0.0004\n",
            "Epoch [85/100], Training loss: 0.0003\n",
            "Epoch [86/100], Training loss: 0.0003\n",
            "Epoch [87/100], Training loss: 0.0003\n",
            "Epoch [88/100], Training loss: 0.0003\n",
            "Epoch [89/100], Training loss: 0.0003\n",
            "Epoch [90/100], Training loss: 0.0003\n",
            "Epoch [91/100], Training loss: 0.0003\n",
            "Epoch [92/100], Training loss: 0.0003\n",
            "Epoch [93/100], Training loss: 0.0003\n",
            "Epoch [94/100], Training loss: 0.0003\n",
            "Epoch [95/100], Training loss: 0.0003\n",
            "Epoch [96/100], Training loss: 0.0003\n",
            "Epoch [97/100], Training loss: 0.0003\n",
            "Epoch [98/100], Training loss: 0.0003\n",
            "Epoch [99/100], Training loss: 0.0003\n",
            "Epoch [100/100], Training loss: 0.0003\n",
            "Training complete.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-43-08011d4dd45a>:49: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  train_preds = model(torch.tensor(X_train, dtype=torch.float32))\n",
            "<ipython-input-43-08011d4dd45a>:55: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  val_preds = model(torch.tensor(X_val, dtype=torch.float32))\n",
            "<ipython-input-43-08011d4dd45a>:32: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  train_dataset = TensorDataset(torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.long))\n",
            "<ipython-input-43-08011d4dd45a>:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  val_dataset = TensorDataset(torch.tensor(X_val, dtype=torch.float32), torch.tensor(y_val, dtype=torch.long))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/100], Training loss: 0.6497\n",
            "Epoch [2/100], Training loss: 0.5260\n",
            "Epoch [3/100], Training loss: 0.4328\n",
            "Epoch [4/100], Training loss: 0.3503\n",
            "Epoch [5/100], Training loss: 0.3036\n",
            "Epoch [6/100], Training loss: 0.2561\n",
            "Epoch [7/100], Training loss: 0.2017\n",
            "Epoch [8/100], Training loss: 0.1909\n",
            "Epoch [9/100], Training loss: 0.1239\n",
            "Epoch [10/100], Training loss: 0.0936\n",
            "Epoch [11/100], Training loss: 0.1291\n",
            "Epoch [12/100], Training loss: 0.0792\n",
            "Epoch [13/100], Training loss: 0.0449\n",
            "Epoch [14/100], Training loss: 0.0300\n",
            "Epoch [15/100], Training loss: 0.0249\n",
            "Epoch [16/100], Training loss: 0.0218\n",
            "Epoch [17/100], Training loss: 0.0157\n",
            "Epoch [18/100], Training loss: 0.0172\n",
            "Epoch [19/100], Training loss: 0.0167\n",
            "Epoch [20/100], Training loss: 0.0151\n",
            "Epoch [21/100], Training loss: 0.0106\n",
            "Epoch [22/100], Training loss: 0.0091\n",
            "Epoch [23/100], Training loss: 0.0081\n",
            "Epoch [24/100], Training loss: 0.0073\n",
            "Epoch [25/100], Training loss: 0.0050\n",
            "Epoch [26/100], Training loss: 0.0048\n",
            "Epoch [27/100], Training loss: 0.0041\n",
            "Epoch [28/100], Training loss: 0.0051\n",
            "Epoch [29/100], Training loss: 0.2680\n",
            "Epoch [30/100], Training loss: 0.0887\n",
            "Epoch [31/100], Training loss: 0.0288\n",
            "Epoch [32/100], Training loss: 0.0145\n",
            "Epoch [33/100], Training loss: 0.0106\n",
            "Epoch [34/100], Training loss: 0.0072\n",
            "Epoch [35/100], Training loss: 0.0068\n",
            "Epoch [36/100], Training loss: 0.0063\n",
            "Epoch [37/100], Training loss: 0.0040\n",
            "Epoch [38/100], Training loss: 0.0037\n",
            "Epoch [39/100], Training loss: 0.0038\n",
            "Epoch [40/100], Training loss: 0.0049\n",
            "Epoch [41/100], Training loss: 0.0032\n",
            "Epoch [42/100], Training loss: 0.0024\n",
            "Epoch [43/100], Training loss: 0.0023\n",
            "Epoch [44/100], Training loss: 0.0021\n",
            "Epoch [45/100], Training loss: 0.0017\n",
            "Epoch [46/100], Training loss: 0.0021\n",
            "Epoch [47/100], Training loss: 0.0018\n",
            "Epoch [48/100], Training loss: 0.0016\n",
            "Epoch [49/100], Training loss: 0.0014\n",
            "Epoch [50/100], Training loss: 0.0013\n",
            "Epoch [51/100], Training loss: 0.0013\n",
            "Epoch [52/100], Training loss: 0.0012\n",
            "Epoch [53/100], Training loss: 0.0011\n",
            "Epoch [54/100], Training loss: 0.0011\n",
            "Epoch [55/100], Training loss: 0.0010\n",
            "Epoch [56/100], Training loss: 0.0010\n",
            "Epoch [57/100], Training loss: 0.0009\n",
            "Epoch [58/100], Training loss: 0.0009\n",
            "Epoch [59/100], Training loss: 0.0008\n",
            "Epoch [60/100], Training loss: 0.0008\n",
            "Epoch [61/100], Training loss: 0.0008\n",
            "Epoch [62/100], Training loss: 0.0008\n",
            "Epoch [63/100], Training loss: 0.0007\n",
            "Epoch [64/100], Training loss: 0.0007\n",
            "Epoch [65/100], Training loss: 0.0007\n",
            "Epoch [66/100], Training loss: 0.0006\n",
            "Epoch [67/100], Training loss: 0.0006\n",
            "Epoch [68/100], Training loss: 0.0006\n",
            "Epoch [69/100], Training loss: 0.0005\n",
            "Epoch [70/100], Training loss: 0.0005\n",
            "Epoch [71/100], Training loss: 0.0005\n",
            "Epoch [72/100], Training loss: 0.0005\n",
            "Epoch [73/100], Training loss: 0.0005\n",
            "Epoch [74/100], Training loss: 0.0005\n",
            "Epoch [75/100], Training loss: 0.0005\n",
            "Epoch [76/100], Training loss: 0.0005\n",
            "Epoch [77/100], Training loss: 0.0005\n",
            "Epoch [78/100], Training loss: 0.0004\n",
            "Epoch [79/100], Training loss: 0.0004\n",
            "Epoch [80/100], Training loss: 0.0004\n",
            "Epoch [81/100], Training loss: 0.0004\n",
            "Epoch [82/100], Training loss: 0.0004\n",
            "Epoch [83/100], Training loss: 0.0004\n",
            "Epoch [84/100], Training loss: 0.0004\n",
            "Epoch [85/100], Training loss: 0.0004\n",
            "Epoch [86/100], Training loss: 0.0004\n",
            "Epoch [87/100], Training loss: 0.0003\n",
            "Epoch [88/100], Training loss: 0.0004\n",
            "Epoch [89/100], Training loss: 0.0003\n",
            "Epoch [90/100], Training loss: 0.0003\n",
            "Epoch [91/100], Training loss: 0.0003\n",
            "Epoch [92/100], Training loss: 0.0003\n",
            "Epoch [93/100], Training loss: 0.0003\n",
            "Epoch [94/100], Training loss: 0.0003\n",
            "Epoch [95/100], Training loss: 0.0003\n",
            "Epoch [96/100], Training loss: 0.0003\n",
            "Epoch [97/100], Training loss: 0.0003\n",
            "Epoch [98/100], Training loss: 0.0003\n",
            "Epoch [99/100], Training loss: 0.0003\n",
            "Epoch [100/100], Training loss: 0.0003\n",
            "Training complete.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-43-08011d4dd45a>:49: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  train_preds = model(torch.tensor(X_train, dtype=torch.float32))\n",
            "<ipython-input-43-08011d4dd45a>:55: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  val_preds = model(torch.tensor(X_val, dtype=torch.float32))\n",
            "<ipython-input-43-08011d4dd45a>:32: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  train_dataset = TensorDataset(torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.long))\n",
            "<ipython-input-43-08011d4dd45a>:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  val_dataset = TensorDataset(torch.tensor(X_val, dtype=torch.float32), torch.tensor(y_val, dtype=torch.long))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/100], Training loss: 0.6483\n",
            "Epoch [2/100], Training loss: 0.5141\n",
            "Epoch [3/100], Training loss: 0.4127\n",
            "Epoch [4/100], Training loss: 0.3358\n",
            "Epoch [5/100], Training loss: 0.2957\n",
            "Epoch [6/100], Training loss: 0.2260\n",
            "Epoch [7/100], Training loss: 0.1828\n",
            "Epoch [8/100], Training loss: 0.1529\n",
            "Epoch [9/100], Training loss: 0.1198\n",
            "Epoch [10/100], Training loss: 0.0762\n",
            "Epoch [11/100], Training loss: 0.0536\n",
            "Epoch [12/100], Training loss: 0.0444\n",
            "Epoch [13/100], Training loss: 0.0343\n",
            "Epoch [14/100], Training loss: 0.0209\n",
            "Epoch [15/100], Training loss: 0.0210\n",
            "Epoch [16/100], Training loss: 0.0206\n",
            "Epoch [17/100], Training loss: 0.0150\n",
            "Epoch [18/100], Training loss: 0.0122\n",
            "Epoch [19/100], Training loss: 0.0127\n",
            "Epoch [20/100], Training loss: 0.0089\n",
            "Epoch [21/100], Training loss: 0.0156\n",
            "Epoch [22/100], Training loss: 0.0109\n",
            "Epoch [23/100], Training loss: 0.0071\n",
            "Epoch [24/100], Training loss: 0.0092\n",
            "Epoch [25/100], Training loss: 0.0057\n",
            "Epoch [26/100], Training loss: 0.0054\n",
            "Epoch [27/100], Training loss: 0.0040\n",
            "Epoch [28/100], Training loss: 0.0033\n",
            "Epoch [29/100], Training loss: 0.0037\n",
            "Epoch [30/100], Training loss: 0.0027\n",
            "Epoch [31/100], Training loss: 0.0053\n",
            "Epoch [32/100], Training loss: 0.0030\n",
            "Epoch [33/100], Training loss: 0.0028\n",
            "Epoch [34/100], Training loss: 0.0053\n",
            "Epoch [35/100], Training loss: 0.0032\n",
            "Epoch [36/100], Training loss: 0.0025\n",
            "Epoch [37/100], Training loss: 0.0029\n",
            "Epoch [38/100], Training loss: 0.0021\n",
            "Epoch [39/100], Training loss: 0.0017\n",
            "Epoch [40/100], Training loss: 0.0014\n",
            "Epoch [41/100], Training loss: 0.0012\n",
            "Epoch [42/100], Training loss: 0.0013\n",
            "Epoch [43/100], Training loss: 0.0016\n",
            "Epoch [44/100], Training loss: 0.0012\n",
            "Epoch [45/100], Training loss: 0.0011\n",
            "Epoch [46/100], Training loss: 0.0011\n",
            "Epoch [47/100], Training loss: 0.0010\n",
            "Epoch [48/100], Training loss: 0.0009\n",
            "Epoch [49/100], Training loss: 0.0009\n",
            "Epoch [50/100], Training loss: 0.0009\n",
            "Epoch [51/100], Training loss: 0.0008\n",
            "Epoch [52/100], Training loss: 0.0007\n",
            "Epoch [53/100], Training loss: 0.0008\n",
            "Epoch [54/100], Training loss: 0.0007\n",
            "Epoch [55/100], Training loss: 0.0007\n",
            "Epoch [56/100], Training loss: 0.0007\n",
            "Epoch [57/100], Training loss: 0.0006\n",
            "Epoch [58/100], Training loss: 0.0006\n",
            "Epoch [59/100], Training loss: 0.0006\n",
            "Epoch [60/100], Training loss: 0.0006\n",
            "Epoch [61/100], Training loss: 0.0006\n",
            "Epoch [62/100], Training loss: 0.0006\n",
            "Epoch [63/100], Training loss: 0.0005\n",
            "Epoch [64/100], Training loss: 0.0005\n",
            "Epoch [65/100], Training loss: 0.0005\n",
            "Epoch [66/100], Training loss: 0.0005\n",
            "Epoch [67/100], Training loss: 0.0005\n",
            "Epoch [68/100], Training loss: 0.0005\n",
            "Epoch [69/100], Training loss: 0.0005\n",
            "Epoch [70/100], Training loss: 0.0005\n",
            "Epoch [71/100], Training loss: 0.0004\n",
            "Epoch [72/100], Training loss: 0.0004\n",
            "Epoch [73/100], Training loss: 0.0004\n",
            "Epoch [74/100], Training loss: 0.0004\n",
            "Epoch [75/100], Training loss: 0.0004\n",
            "Epoch [76/100], Training loss: 0.0004\n",
            "Epoch [77/100], Training loss: 0.0004\n",
            "Epoch [78/100], Training loss: 0.0004\n",
            "Epoch [79/100], Training loss: 0.0004\n",
            "Epoch [80/100], Training loss: 0.0004\n",
            "Epoch [81/100], Training loss: 0.0004\n",
            "Epoch [82/100], Training loss: 0.0003\n",
            "Epoch [83/100], Training loss: 0.0003\n",
            "Epoch [84/100], Training loss: 0.0003\n",
            "Epoch [85/100], Training loss: 0.0003\n",
            "Epoch [86/100], Training loss: 0.0003\n",
            "Epoch [87/100], Training loss: 0.0003\n",
            "Epoch [88/100], Training loss: 0.0003\n",
            "Epoch [89/100], Training loss: 0.0003\n",
            "Epoch [90/100], Training loss: 0.0003\n",
            "Epoch [91/100], Training loss: 0.0003\n",
            "Epoch [92/100], Training loss: 0.0003\n",
            "Epoch [93/100], Training loss: 0.0003\n",
            "Epoch [94/100], Training loss: 0.0003\n",
            "Epoch [95/100], Training loss: 0.0003\n",
            "Epoch [96/100], Training loss: 0.0003\n",
            "Epoch [97/100], Training loss: 0.0003\n",
            "Epoch [98/100], Training loss: 0.0003\n",
            "Epoch [99/100], Training loss: 0.0003\n",
            "Epoch [100/100], Training loss: 0.0003\n",
            "Training complete.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-43-08011d4dd45a>:49: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  train_preds = model(torch.tensor(X_train, dtype=torch.float32))\n",
            "<ipython-input-43-08011d4dd45a>:55: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  val_preds = model(torch.tensor(X_val, dtype=torch.float32))\n",
            "<ipython-input-43-08011d4dd45a>:32: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  train_dataset = TensorDataset(torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.long))\n",
            "<ipython-input-43-08011d4dd45a>:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  val_dataset = TensorDataset(torch.tensor(X_val, dtype=torch.float32), torch.tensor(y_val, dtype=torch.long))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/100], Training loss: 0.6522\n",
            "Epoch [2/100], Training loss: 0.5300\n",
            "Epoch [3/100], Training loss: 0.4272\n",
            "Epoch [4/100], Training loss: 0.3533\n",
            "Epoch [5/100], Training loss: 0.2976\n",
            "Epoch [6/100], Training loss: 0.2520\n",
            "Epoch [7/100], Training loss: 0.2152\n",
            "Epoch [8/100], Training loss: 0.1634\n",
            "Epoch [9/100], Training loss: 0.1436\n",
            "Epoch [10/100], Training loss: 0.1327\n",
            "Epoch [11/100], Training loss: 0.0987\n",
            "Epoch [12/100], Training loss: 0.0714\n",
            "Epoch [13/100], Training loss: 0.0783\n",
            "Epoch [14/100], Training loss: 0.0462\n",
            "Epoch [15/100], Training loss: 0.0437\n",
            "Epoch [16/100], Training loss: 0.0361\n",
            "Epoch [17/100], Training loss: 0.0343\n",
            "Epoch [18/100], Training loss: 0.0320\n",
            "Epoch [19/100], Training loss: 0.0288\n",
            "Epoch [20/100], Training loss: 0.0299\n",
            "Epoch [21/100], Training loss: 0.0272\n",
            "Epoch [22/100], Training loss: 0.0303\n",
            "Epoch [23/100], Training loss: 0.0301\n",
            "Epoch [24/100], Training loss: 0.0264\n",
            "Epoch [25/100], Training loss: 0.0252\n",
            "Epoch [26/100], Training loss: 0.0235\n",
            "Epoch [27/100], Training loss: 0.0257\n",
            "Epoch [28/100], Training loss: 0.0284\n",
            "Epoch [29/100], Training loss: 0.0220\n",
            "Epoch [30/100], Training loss: 0.0207\n",
            "Epoch [31/100], Training loss: 0.0209\n",
            "Epoch [32/100], Training loss: 0.0215\n",
            "Epoch [33/100], Training loss: 0.0212\n",
            "Epoch [34/100], Training loss: 0.0240\n",
            "Epoch [35/100], Training loss: 0.2377\n",
            "Epoch [36/100], Training loss: 0.2223\n",
            "Epoch [37/100], Training loss: 0.0666\n",
            "Epoch [38/100], Training loss: 0.0399\n",
            "Epoch [39/100], Training loss: 0.0296\n",
            "Epoch [40/100], Training loss: 0.0266\n",
            "Epoch [41/100], Training loss: 0.0225\n",
            "Epoch [42/100], Training loss: 0.0240\n",
            "Epoch [43/100], Training loss: 0.0214\n",
            "Epoch [44/100], Training loss: 0.0236\n",
            "Epoch [45/100], Training loss: 0.1472\n",
            "Epoch [46/100], Training loss: 0.1245\n",
            "Epoch [47/100], Training loss: 0.0447\n",
            "Epoch [48/100], Training loss: 0.0432\n",
            "Epoch [49/100], Training loss: 0.0345\n",
            "Epoch [50/100], Training loss: 0.0204\n",
            "Epoch [51/100], Training loss: 0.0191\n",
            "Epoch [52/100], Training loss: 0.0196\n",
            "Epoch [53/100], Training loss: 0.0170\n",
            "Epoch [54/100], Training loss: 0.0340\n",
            "Epoch [55/100], Training loss: 0.0390\n",
            "Epoch [56/100], Training loss: 0.0233\n",
            "Epoch [57/100], Training loss: 0.0194\n",
            "Epoch [58/100], Training loss: 0.0211\n",
            "Epoch [59/100], Training loss: 0.0180\n",
            "Epoch [60/100], Training loss: 0.0185\n",
            "Epoch [61/100], Training loss: 0.0193\n",
            "Epoch [62/100], Training loss: 0.0195\n",
            "Epoch [63/100], Training loss: 0.0190\n",
            "Epoch [64/100], Training loss: 0.0181\n",
            "Epoch [65/100], Training loss: 0.0194\n",
            "Epoch [66/100], Training loss: 0.0174\n",
            "Epoch [67/100], Training loss: 0.0179\n",
            "Epoch [68/100], Training loss: 0.2202\n",
            "Epoch [69/100], Training loss: 0.0914\n",
            "Epoch [70/100], Training loss: 0.0371\n",
            "Epoch [71/100], Training loss: 0.0266\n",
            "Epoch [72/100], Training loss: 0.0238\n",
            "Epoch [73/100], Training loss: 0.0216\n",
            "Epoch [74/100], Training loss: 0.0197\n",
            "Epoch [75/100], Training loss: 0.0192\n",
            "Epoch [76/100], Training loss: 0.0185\n",
            "Epoch [77/100], Training loss: 0.0193\n",
            "Epoch [78/100], Training loss: 0.0240\n",
            "Epoch [79/100], Training loss: 0.0180\n",
            "Epoch [80/100], Training loss: 0.0194\n",
            "Epoch [81/100], Training loss: 0.0242\n",
            "Epoch [82/100], Training loss: 0.0184\n",
            "Epoch [83/100], Training loss: 0.0186\n",
            "Epoch [84/100], Training loss: 0.0174\n",
            "Epoch [85/100], Training loss: 0.0174\n",
            "Epoch [86/100], Training loss: 0.0190\n",
            "Epoch [87/100], Training loss: 0.0193\n",
            "Epoch [88/100], Training loss: 0.1508\n",
            "Epoch [89/100], Training loss: 0.1587\n",
            "Epoch [90/100], Training loss: 0.0361\n",
            "Epoch [91/100], Training loss: 0.0265\n",
            "Epoch [92/100], Training loss: 0.0221\n",
            "Epoch [93/100], Training loss: 0.0222\n",
            "Epoch [94/100], Training loss: 0.0200\n",
            "Epoch [95/100], Training loss: 0.0192\n",
            "Epoch [96/100], Training loss: 0.0183\n",
            "Epoch [97/100], Training loss: 0.0180\n",
            "Epoch [98/100], Training loss: 0.0182\n",
            "Epoch [99/100], Training loss: 0.1560\n",
            "Epoch [100/100], Training loss: 0.0949\n",
            "Training complete.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-43-08011d4dd45a>:49: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  train_preds = model(torch.tensor(X_train, dtype=torch.float32))\n",
            "<ipython-input-43-08011d4dd45a>:55: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  val_preds = model(torch.tensor(X_val, dtype=torch.float32))\n",
            "<ipython-input-43-08011d4dd45a>:32: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  train_dataset = TensorDataset(torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.long))\n",
            "<ipython-input-43-08011d4dd45a>:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  val_dataset = TensorDataset(torch.tensor(X_val, dtype=torch.float32), torch.tensor(y_val, dtype=torch.long))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/100], Training loss: 0.6552\n",
            "Epoch [2/100], Training loss: 0.5344\n",
            "Epoch [3/100], Training loss: 0.4424\n",
            "Epoch [4/100], Training loss: 0.3768\n",
            "Epoch [5/100], Training loss: 0.3233\n",
            "Epoch [6/100], Training loss: 0.2816\n",
            "Epoch [7/100], Training loss: 0.2359\n",
            "Epoch [8/100], Training loss: 0.1997\n",
            "Epoch [9/100], Training loss: 0.1757\n",
            "Epoch [10/100], Training loss: 0.1282\n",
            "Epoch [11/100], Training loss: 0.1137\n",
            "Epoch [12/100], Training loss: 0.0896\n",
            "Epoch [13/100], Training loss: 0.0840\n",
            "Epoch [14/100], Training loss: 0.0578\n",
            "Epoch [15/100], Training loss: 0.1957\n",
            "Epoch [16/100], Training loss: 0.0528\n",
            "Epoch [17/100], Training loss: 0.0398\n",
            "Epoch [18/100], Training loss: 0.0330\n",
            "Epoch [19/100], Training loss: 0.0327\n",
            "Epoch [20/100], Training loss: 0.0286\n",
            "Epoch [21/100], Training loss: 0.0288\n",
            "Epoch [22/100], Training loss: 0.0277\n",
            "Epoch [23/100], Training loss: 0.0234\n",
            "Epoch [24/100], Training loss: 0.0242\n",
            "Epoch [25/100], Training loss: 0.0230\n",
            "Epoch [26/100], Training loss: 0.0243\n",
            "Epoch [27/100], Training loss: 0.0237\n",
            "Epoch [28/100], Training loss: 0.0203\n",
            "Epoch [29/100], Training loss: 0.0286\n",
            "Epoch [30/100], Training loss: 0.0232\n",
            "Epoch [31/100], Training loss: 0.0217\n",
            "Epoch [32/100], Training loss: 0.4257\n",
            "Epoch [33/100], Training loss: 0.0697\n",
            "Epoch [34/100], Training loss: 0.0398\n",
            "Epoch [35/100], Training loss: 0.0301\n",
            "Epoch [36/100], Training loss: 0.0279\n",
            "Epoch [37/100], Training loss: 0.0231\n",
            "Epoch [38/100], Training loss: 0.0238\n",
            "Epoch [39/100], Training loss: 0.0246\n",
            "Epoch [40/100], Training loss: 0.0217\n",
            "Epoch [41/100], Training loss: 0.0215\n",
            "Epoch [42/100], Training loss: 0.0198\n",
            "Epoch [43/100], Training loss: 0.0209\n",
            "Epoch [44/100], Training loss: 0.0213\n",
            "Epoch [45/100], Training loss: 0.0203\n",
            "Epoch [46/100], Training loss: 0.0187\n",
            "Epoch [47/100], Training loss: 0.0233\n",
            "Epoch [48/100], Training loss: 0.0205\n",
            "Epoch [49/100], Training loss: 0.0200\n",
            "Epoch [50/100], Training loss: 0.0190\n",
            "Epoch [51/100], Training loss: 0.0195\n",
            "Epoch [52/100], Training loss: 0.2721\n",
            "Epoch [53/100], Training loss: 0.0798\n",
            "Epoch [54/100], Training loss: 0.0371\n",
            "Epoch [55/100], Training loss: 0.0291\n",
            "Epoch [56/100], Training loss: 0.0216\n",
            "Epoch [57/100], Training loss: 0.0236\n",
            "Epoch [58/100], Training loss: 0.0229\n",
            "Epoch [59/100], Training loss: 0.0203\n",
            "Epoch [60/100], Training loss: 0.0197\n",
            "Epoch [61/100], Training loss: 0.0196\n",
            "Epoch [62/100], Training loss: 0.0203\n",
            "Epoch [63/100], Training loss: 0.0210\n",
            "Epoch [64/100], Training loss: 0.0186\n",
            "Epoch [65/100], Training loss: 0.3718\n",
            "Epoch [66/100], Training loss: 0.1117\n",
            "Epoch [67/100], Training loss: 0.0470\n",
            "Epoch [68/100], Training loss: 0.0336\n",
            "Epoch [69/100], Training loss: 0.0254\n",
            "Epoch [70/100], Training loss: 0.0232\n",
            "Epoch [71/100], Training loss: 0.0217\n",
            "Epoch [72/100], Training loss: 0.0256\n",
            "Epoch [73/100], Training loss: 0.0198\n",
            "Epoch [74/100], Training loss: 0.0183\n",
            "Epoch [75/100], Training loss: 0.0180\n",
            "Epoch [76/100], Training loss: 0.0177\n",
            "Epoch [77/100], Training loss: 0.0189\n",
            "Epoch [78/100], Training loss: 0.0183\n",
            "Epoch [79/100], Training loss: 0.0182\n",
            "Epoch [80/100], Training loss: 0.0188\n",
            "Epoch [81/100], Training loss: 0.0202\n",
            "Epoch [82/100], Training loss: 0.0184\n",
            "Epoch [83/100], Training loss: 0.0210\n",
            "Epoch [84/100], Training loss: 0.0221\n",
            "Epoch [85/100], Training loss: 0.2316\n",
            "Epoch [86/100], Training loss: 0.0415\n",
            "Epoch [87/100], Training loss: 0.0247\n",
            "Epoch [88/100], Training loss: 0.0200\n",
            "Epoch [89/100], Training loss: 0.0184\n",
            "Epoch [90/100], Training loss: 0.0199\n",
            "Epoch [91/100], Training loss: 0.0183\n",
            "Epoch [92/100], Training loss: 0.0202\n",
            "Epoch [93/100], Training loss: 0.0186\n",
            "Epoch [94/100], Training loss: 0.0181\n",
            "Epoch [95/100], Training loss: 0.0172\n",
            "Epoch [96/100], Training loss: 0.0204\n",
            "Epoch [97/100], Training loss: 0.0170\n",
            "Epoch [98/100], Training loss: 0.0190\n",
            "Epoch [99/100], Training loss: 0.0195\n",
            "Epoch [100/100], Training loss: 0.0976\n",
            "Training complete.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-43-08011d4dd45a>:49: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  train_preds = model(torch.tensor(X_train, dtype=torch.float32))\n",
            "<ipython-input-43-08011d4dd45a>:55: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  val_preds = model(torch.tensor(X_val, dtype=torch.float32))\n",
            "<ipython-input-43-08011d4dd45a>:32: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  train_dataset = TensorDataset(torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.long))\n",
            "<ipython-input-43-08011d4dd45a>:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  val_dataset = TensorDataset(torch.tensor(X_val, dtype=torch.float32), torch.tensor(y_val, dtype=torch.long))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/100], Training loss: 0.6641\n",
            "Epoch [2/100], Training loss: 0.5506\n",
            "Epoch [3/100], Training loss: 0.4410\n",
            "Epoch [4/100], Training loss: 0.3642\n",
            "Epoch [5/100], Training loss: 0.3069\n",
            "Epoch [6/100], Training loss: 0.2655\n",
            "Epoch [7/100], Training loss: 0.2212\n",
            "Epoch [8/100], Training loss: 0.2049\n",
            "Epoch [9/100], Training loss: 0.1598\n",
            "Epoch [10/100], Training loss: 0.1100\n",
            "Epoch [11/100], Training loss: 0.1127\n",
            "Epoch [12/100], Training loss: 0.0726\n",
            "Epoch [13/100], Training loss: 0.0585\n",
            "Epoch [14/100], Training loss: 0.0546\n",
            "Epoch [15/100], Training loss: 0.0556\n",
            "Epoch [16/100], Training loss: 0.0509\n",
            "Epoch [17/100], Training loss: 0.0365\n",
            "Epoch [18/100], Training loss: 0.0384\n",
            "Epoch [19/100], Training loss: 0.0335\n",
            "Epoch [20/100], Training loss: 0.0312\n",
            "Epoch [21/100], Training loss: 0.0287\n",
            "Epoch [22/100], Training loss: 0.0253\n",
            "Epoch [23/100], Training loss: 0.0280\n",
            "Epoch [24/100], Training loss: 0.0244\n",
            "Epoch [25/100], Training loss: 0.0267\n",
            "Epoch [26/100], Training loss: 0.0282\n",
            "Epoch [27/100], Training loss: 0.0264\n",
            "Epoch [28/100], Training loss: 0.0298\n",
            "Epoch [29/100], Training loss: 0.0283\n",
            "Epoch [30/100], Training loss: 0.0264\n",
            "Epoch [31/100], Training loss: 0.0259\n",
            "Epoch [32/100], Training loss: 0.2108\n",
            "Epoch [33/100], Training loss: 0.0948\n",
            "Epoch [34/100], Training loss: 0.0473\n",
            "Epoch [35/100], Training loss: 0.0303\n",
            "Epoch [36/100], Training loss: 0.0295\n",
            "Epoch [37/100], Training loss: 0.0263\n",
            "Epoch [38/100], Training loss: 0.0230\n",
            "Epoch [39/100], Training loss: 0.0228\n",
            "Epoch [40/100], Training loss: 0.0228\n",
            "Epoch [41/100], Training loss: 0.0235\n",
            "Epoch [42/100], Training loss: 0.0233\n",
            "Epoch [43/100], Training loss: 0.0222\n",
            "Epoch [44/100], Training loss: 0.0233\n",
            "Epoch [45/100], Training loss: 0.0244\n",
            "Epoch [46/100], Training loss: 0.0234\n",
            "Epoch [47/100], Training loss: 0.0220\n",
            "Epoch [48/100], Training loss: 0.1277\n",
            "Epoch [49/100], Training loss: 0.0457\n",
            "Epoch [50/100], Training loss: 0.1540\n",
            "Epoch [51/100], Training loss: 0.0563\n",
            "Epoch [52/100], Training loss: 0.0261\n",
            "Epoch [53/100], Training loss: 0.0231\n",
            "Epoch [54/100], Training loss: 0.0223\n",
            "Epoch [55/100], Training loss: 0.0219\n",
            "Epoch [56/100], Training loss: 0.0232\n",
            "Epoch [57/100], Training loss: 0.0212\n",
            "Epoch [58/100], Training loss: 0.0207\n",
            "Epoch [59/100], Training loss: 0.0216\n",
            "Epoch [60/100], Training loss: 0.0200\n",
            "Epoch [61/100], Training loss: 0.0209\n",
            "Epoch [62/100], Training loss: 0.2462\n",
            "Epoch [63/100], Training loss: 0.0491\n",
            "Epoch [64/100], Training loss: 0.0274\n",
            "Epoch [65/100], Training loss: 0.0232\n",
            "Epoch [66/100], Training loss: 0.0208\n",
            "Epoch [67/100], Training loss: 0.0208\n",
            "Epoch [68/100], Training loss: 0.0207\n",
            "Epoch [69/100], Training loss: 0.0210\n",
            "Epoch [70/100], Training loss: 0.0187\n",
            "Epoch [71/100], Training loss: 0.0206\n",
            "Epoch [72/100], Training loss: 0.0198\n",
            "Epoch [73/100], Training loss: 0.0210\n",
            "Epoch [74/100], Training loss: 0.0218\n",
            "Epoch [75/100], Training loss: 0.0197\n",
            "Epoch [76/100], Training loss: 0.0202\n",
            "Epoch [77/100], Training loss: 0.0251\n",
            "Epoch [78/100], Training loss: 0.0202\n",
            "Epoch [79/100], Training loss: 0.0219\n",
            "Epoch [80/100], Training loss: 0.0189\n",
            "Epoch [81/100], Training loss: 0.0181\n",
            "Epoch [82/100], Training loss: 0.0228\n",
            "Epoch [83/100], Training loss: 0.0203\n",
            "Epoch [84/100], Training loss: 0.0269\n",
            "Epoch [85/100], Training loss: 0.0215\n",
            "Epoch [86/100], Training loss: 0.0210\n",
            "Epoch [87/100], Training loss: 0.0210\n",
            "Epoch [88/100], Training loss: 0.0350\n",
            "Epoch [89/100], Training loss: 0.3820\n",
            "Epoch [90/100], Training loss: 0.0780\n",
            "Epoch [91/100], Training loss: 0.0384\n",
            "Epoch [92/100], Training loss: 0.0288\n",
            "Epoch [93/100], Training loss: 0.0222\n",
            "Epoch [94/100], Training loss: 0.0207\n",
            "Epoch [95/100], Training loss: 0.0195\n",
            "Epoch [96/100], Training loss: 0.0211\n",
            "Epoch [97/100], Training loss: 0.0193\n",
            "Epoch [98/100], Training loss: 0.0191\n",
            "Epoch [99/100], Training loss: 0.0190\n",
            "Epoch [100/100], Training loss: 0.0190\n",
            "Training complete.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-43-08011d4dd45a>:49: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  train_preds = model(torch.tensor(X_train, dtype=torch.float32))\n",
            "<ipython-input-43-08011d4dd45a>:55: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  val_preds = model(torch.tensor(X_val, dtype=torch.float32))\n",
            "<ipython-input-43-08011d4dd45a>:32: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  train_dataset = TensorDataset(torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.long))\n",
            "<ipython-input-43-08011d4dd45a>:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  val_dataset = TensorDataset(torch.tensor(X_val, dtype=torch.float32), torch.tensor(y_val, dtype=torch.long))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/100], Training loss: 0.6592\n",
            "Epoch [2/100], Training loss: 0.5370\n",
            "Epoch [3/100], Training loss: 0.4286\n",
            "Epoch [4/100], Training loss: 0.3531\n",
            "Epoch [5/100], Training loss: 0.3014\n",
            "Epoch [6/100], Training loss: 0.2809\n",
            "Epoch [7/100], Training loss: 0.2324\n",
            "Epoch [8/100], Training loss: 0.1869\n",
            "Epoch [9/100], Training loss: 0.1428\n",
            "Epoch [10/100], Training loss: 0.1393\n",
            "Epoch [11/100], Training loss: 0.1020\n",
            "Epoch [12/100], Training loss: 0.0824\n",
            "Epoch [13/100], Training loss: 0.0635\n",
            "Epoch [14/100], Training loss: 0.0596\n",
            "Epoch [15/100], Training loss: 0.0492\n",
            "Epoch [16/100], Training loss: 0.0490\n",
            "Epoch [17/100], Training loss: 0.0347\n",
            "Epoch [18/100], Training loss: 0.0334\n",
            "Epoch [19/100], Training loss: 0.0303\n",
            "Epoch [20/100], Training loss: 0.2171\n",
            "Epoch [21/100], Training loss: 0.0637\n",
            "Epoch [22/100], Training loss: 0.0386\n",
            "Epoch [23/100], Training loss: 0.0322\n",
            "Epoch [24/100], Training loss: 0.0290\n",
            "Epoch [25/100], Training loss: 0.0281\n",
            "Epoch [26/100], Training loss: 0.0257\n",
            "Epoch [27/100], Training loss: 0.0253\n",
            "Epoch [28/100], Training loss: 0.0270\n",
            "Epoch [29/100], Training loss: 0.0255\n",
            "Epoch [30/100], Training loss: 0.1493\n",
            "Epoch [31/100], Training loss: 0.0503\n",
            "Epoch [32/100], Training loss: 0.0306\n",
            "Epoch [33/100], Training loss: 0.0222\n",
            "Epoch [34/100], Training loss: 0.0233\n",
            "Epoch [35/100], Training loss: 0.0237\n",
            "Epoch [36/100], Training loss: 0.0278\n",
            "Epoch [37/100], Training loss: 0.0216\n",
            "Epoch [38/100], Training loss: 0.0230\n",
            "Epoch [39/100], Training loss: 0.0205\n",
            "Epoch [40/100], Training loss: 0.0220\n",
            "Epoch [41/100], Training loss: 0.0276\n",
            "Epoch [42/100], Training loss: 0.0287\n",
            "Epoch [43/100], Training loss: 0.0205\n",
            "Epoch [44/100], Training loss: 0.0276\n",
            "Epoch [45/100], Training loss: 0.0208\n",
            "Epoch [46/100], Training loss: 0.0205\n",
            "Epoch [47/100], Training loss: 0.0215\n",
            "Epoch [48/100], Training loss: 0.0214\n",
            "Epoch [49/100], Training loss: 0.0200\n",
            "Epoch [50/100], Training loss: 0.0209\n",
            "Epoch [51/100], Training loss: 0.0192\n",
            "Epoch [52/100], Training loss: 0.0262\n",
            "Epoch [53/100], Training loss: 0.0210\n",
            "Epoch [54/100], Training loss: 0.0219\n",
            "Epoch [55/100], Training loss: 0.3026\n",
            "Epoch [56/100], Training loss: 0.0691\n",
            "Epoch [57/100], Training loss: 0.0355\n",
            "Epoch [58/100], Training loss: 0.0249\n",
            "Epoch [59/100], Training loss: 0.0225\n",
            "Epoch [60/100], Training loss: 0.0220\n",
            "Epoch [61/100], Training loss: 0.0210\n",
            "Epoch [62/100], Training loss: 0.0208\n",
            "Epoch [63/100], Training loss: 0.0195\n",
            "Epoch [64/100], Training loss: 0.0197\n",
            "Epoch [65/100], Training loss: 0.0218\n",
            "Epoch [66/100], Training loss: 0.0201\n",
            "Epoch [67/100], Training loss: 0.0204\n",
            "Epoch [68/100], Training loss: 0.0199\n",
            "Epoch [69/100], Training loss: 0.0184\n",
            "Epoch [70/100], Training loss: 0.3864\n",
            "Epoch [71/100], Training loss: 0.0841\n",
            "Epoch [72/100], Training loss: 0.0406\n",
            "Epoch [73/100], Training loss: 0.0333\n",
            "Epoch [74/100], Training loss: 0.0244\n",
            "Epoch [75/100], Training loss: 0.0219\n",
            "Epoch [76/100], Training loss: 0.0214\n",
            "Epoch [77/100], Training loss: 0.0234\n",
            "Epoch [78/100], Training loss: 0.0218\n",
            "Epoch [79/100], Training loss: 0.0202\n",
            "Epoch [80/100], Training loss: 0.0220\n",
            "Epoch [81/100], Training loss: 0.0194\n",
            "Epoch [82/100], Training loss: 0.0197\n",
            "Epoch [83/100], Training loss: 0.0220\n",
            "Epoch [84/100], Training loss: 0.0243\n",
            "Epoch [85/100], Training loss: 0.0214\n",
            "Epoch [86/100], Training loss: 0.0181\n",
            "Epoch [87/100], Training loss: 0.0186\n",
            "Epoch [88/100], Training loss: 0.0182\n",
            "Epoch [89/100], Training loss: 0.0193\n",
            "Epoch [90/100], Training loss: 0.0181\n",
            "Epoch [91/100], Training loss: 0.1350\n",
            "Epoch [92/100], Training loss: 0.0436\n",
            "Epoch [93/100], Training loss: 0.0335\n",
            "Epoch [94/100], Training loss: 0.0243\n",
            "Epoch [95/100], Training loss: 0.0202\n",
            "Epoch [96/100], Training loss: 0.0178\n",
            "Epoch [97/100], Training loss: 0.0182\n",
            "Epoch [98/100], Training loss: 0.0186\n",
            "Epoch [99/100], Training loss: 0.0179\n",
            "Epoch [100/100], Training loss: 0.0217\n",
            "Training complete.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-43-08011d4dd45a>:49: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  train_preds = model(torch.tensor(X_train, dtype=torch.float32))\n",
            "<ipython-input-43-08011d4dd45a>:55: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  val_preds = model(torch.tensor(X_val, dtype=torch.float32))\n",
            "<ipython-input-43-08011d4dd45a>:32: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  train_dataset = TensorDataset(torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.long))\n",
            "<ipython-input-43-08011d4dd45a>:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  val_dataset = TensorDataset(torch.tensor(X_val, dtype=torch.float32), torch.tensor(y_val, dtype=torch.long))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/100], Training loss: 0.6481\n",
            "Epoch [2/100], Training loss: 0.5242\n",
            "Epoch [3/100], Training loss: 0.4293\n",
            "Epoch [4/100], Training loss: 0.3625\n",
            "Epoch [5/100], Training loss: 0.2998\n",
            "Epoch [6/100], Training loss: 0.2432\n",
            "Epoch [7/100], Training loss: 0.2349\n",
            "Epoch [8/100], Training loss: 0.1759\n",
            "Epoch [9/100], Training loss: 0.1411\n",
            "Epoch [10/100], Training loss: 0.1384\n",
            "Epoch [11/100], Training loss: 0.0885\n",
            "Epoch [12/100], Training loss: 0.0725\n",
            "Epoch [13/100], Training loss: 0.0661\n",
            "Epoch [14/100], Training loss: 0.0541\n",
            "Epoch [15/100], Training loss: 0.0414\n",
            "Epoch [16/100], Training loss: 0.0416\n",
            "Epoch [17/100], Training loss: 0.0340\n",
            "Epoch [18/100], Training loss: 0.0311\n",
            "Epoch [19/100], Training loss: 0.0326\n",
            "Epoch [20/100], Training loss: 0.0357\n",
            "Epoch [21/100], Training loss: 0.0303\n",
            "Epoch [22/100], Training loss: 0.0279\n",
            "Epoch [23/100], Training loss: 0.0292\n",
            "Epoch [24/100], Training loss: 0.0323\n",
            "Epoch [25/100], Training loss: 0.0264\n",
            "Epoch [26/100], Training loss: 0.0256\n",
            "Epoch [27/100], Training loss: 0.0238\n",
            "Epoch [28/100], Training loss: 0.0517\n",
            "Epoch [29/100], Training loss: 0.2686\n",
            "Epoch [30/100], Training loss: 0.0841\n",
            "Epoch [31/100], Training loss: 0.0467\n",
            "Epoch [32/100], Training loss: 0.0305\n",
            "Epoch [33/100], Training loss: 0.0270\n",
            "Epoch [34/100], Training loss: 0.0261\n",
            "Epoch [35/100], Training loss: 0.0265\n",
            "Epoch [36/100], Training loss: 0.0265\n",
            "Epoch [37/100], Training loss: 0.0214\n",
            "Epoch [38/100], Training loss: 0.0213\n",
            "Epoch [39/100], Training loss: 0.0220\n",
            "Epoch [40/100], Training loss: 0.0215\n",
            "Epoch [41/100], Training loss: 0.0212\n",
            "Epoch [42/100], Training loss: 0.0228\n",
            "Epoch [43/100], Training loss: 0.0221\n",
            "Epoch [44/100], Training loss: 0.0218\n",
            "Epoch [45/100], Training loss: 0.0224\n",
            "Epoch [46/100], Training loss: 0.0207\n",
            "Epoch [47/100], Training loss: 0.0216\n",
            "Epoch [48/100], Training loss: 0.0219\n",
            "Epoch [49/100], Training loss: 0.0202\n",
            "Epoch [50/100], Training loss: 0.0244\n",
            "Epoch [51/100], Training loss: 0.0229\n",
            "Epoch [52/100], Training loss: 0.0224\n",
            "Epoch [53/100], Training loss: 0.0215\n",
            "Epoch [54/100], Training loss: 0.0221\n",
            "Epoch [55/100], Training loss: 0.0231\n",
            "Epoch [56/100], Training loss: 0.0234\n",
            "Epoch [57/100], Training loss: 0.0233\n",
            "Epoch [58/100], Training loss: 0.0282\n",
            "Epoch [59/100], Training loss: 0.0207\n",
            "Epoch [60/100], Training loss: 0.1098\n",
            "Epoch [61/100], Training loss: 0.0608\n",
            "Epoch [62/100], Training loss: 0.0389\n",
            "Epoch [63/100], Training loss: 0.0338\n",
            "Epoch [64/100], Training loss: 0.0225\n",
            "Epoch [65/100], Training loss: 0.0188\n",
            "Epoch [66/100], Training loss: 0.0221\n",
            "Epoch [67/100], Training loss: 0.0187\n",
            "Epoch [68/100], Training loss: 0.0202\n",
            "Epoch [69/100], Training loss: 0.0211\n",
            "Epoch [70/100], Training loss: 0.0181\n",
            "Epoch [71/100], Training loss: 0.0208\n",
            "Epoch [72/100], Training loss: 0.0204\n",
            "Epoch [73/100], Training loss: 0.0221\n",
            "Epoch [74/100], Training loss: 0.0193\n",
            "Epoch [75/100], Training loss: 0.0199\n",
            "Epoch [76/100], Training loss: 0.0220\n",
            "Epoch [77/100], Training loss: 0.0200\n",
            "Epoch [78/100], Training loss: 0.0211\n",
            "Epoch [79/100], Training loss: 0.0196\n",
            "Epoch [80/100], Training loss: 0.0204\n",
            "Epoch [81/100], Training loss: 0.0213\n",
            "Epoch [82/100], Training loss: 0.0212\n",
            "Epoch [83/100], Training loss: 0.0194\n",
            "Epoch [84/100], Training loss: 0.1375\n",
            "Epoch [85/100], Training loss: 0.1540\n",
            "Epoch [86/100], Training loss: 0.0375\n",
            "Epoch [87/100], Training loss: 0.0310\n",
            "Epoch [88/100], Training loss: 0.0205\n",
            "Epoch [89/100], Training loss: 0.0228\n",
            "Epoch [90/100], Training loss: 0.0184\n",
            "Epoch [91/100], Training loss: 0.0188\n",
            "Epoch [92/100], Training loss: 0.0192\n",
            "Epoch [93/100], Training loss: 0.0194\n",
            "Epoch [94/100], Training loss: 0.0194\n",
            "Epoch [95/100], Training loss: 0.0196\n",
            "Epoch [96/100], Training loss: 0.0171\n",
            "Epoch [97/100], Training loss: 0.0254\n",
            "Epoch [98/100], Training loss: 0.0193\n",
            "Epoch [99/100], Training loss: 0.0200\n",
            "Epoch [100/100], Training loss: 0.0214\n",
            "Training complete.\n",
            "LR: 0.001, WD: 0.0, Train Mean: 0.7820, Train SE: 0.0030, Val Mean: 0.7380, Val SE: 0.0115\n",
            "LR: 0.001, WD: 0.01, Train Mean: 0.7861, Train SE: 0.0038, Val Mean: 0.7510, Val SE: 0.0026\n",
            "LR: 0.01, WD: 0.0, Train Mean: 0.9984, Train SE: 0.0003, Val Mean: 0.8055, Val SE: 0.0070\n",
            "LR: 0.01, WD: 0.01, Train Mean: 0.9975, Train SE: 0.0004, Val Mean: 0.8095, Val SE: 0.0128\n",
            "LR: 0.1, WD: 0.0, Train Mean: 1.0000, Train SE: 0.0000, Val Mean: 0.8175, Val SE: 0.0069\n",
            "LR: 0.1, WD: 0.01, Train Mean: 0.9955, Train SE: 0.0030, Val Mean: 0.8230, Val SE: 0.0031\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-43-08011d4dd45a>:49: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  train_preds = model(torch.tensor(X_train, dtype=torch.float32))\n",
            "<ipython-input-43-08011d4dd45a>:55: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  val_preds = model(torch.tensor(X_val, dtype=torch.float32))\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import accuracy_score\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# Assuming MyMLP model is defined\n",
        "# train function as defined in the previous steps\n",
        "\n",
        "# Define hyperparameters\n",
        "learning_rates = [0.001, 0.01, 0.1]\n",
        "weight_decays = [0., 0.01]\n",
        "batch_size = 50\n",
        "n_epochs = 100\n",
        "n_folds = 5\n",
        "\n",
        "results = []\n",
        "\n",
        "for lr in learning_rates:\n",
        "    for wd in weight_decays:\n",
        "        val_accs = []  # store validation accuracy for each fold\n",
        "        train_accs = []  # store training accuracy for each fold\n",
        "\n",
        "        kf = KFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
        "\n",
        "        for train_index, val_index in kf.split(X):  # X and y should be defined\n",
        "            # Split data into train and validation sets\n",
        "            X_train, X_val = X[train_index], X[val_index]\n",
        "            y_train, y_val = y[train_index], y[val_index]\n",
        "\n",
        "            # Create data loaders for training and validation sets\n",
        "            train_dataset = TensorDataset(torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.long))\n",
        "            val_dataset = TensorDataset(torch.tensor(X_val, dtype=torch.float32), torch.tensor(y_val, dtype=torch.long))\n",
        "            train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "            val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "            # Initialize model, criterion (CrossEntropyLoss), and optimizer (SGD)\n",
        "            model = MyMLP()  # Using the previously defined MLP model\n",
        "            criterion = torch.nn.BCEWithLogitsLoss()  # Binary Cross-Entropy loss\n",
        "            optimizer = optim.SGD(model.parameters(), lr=lr, weight_decay=wd)\n",
        "\n",
        "            # Train the model using the defined train function\n",
        "            train(model, train_loader, val_loader, n_epochs, optimizer, criterion, verbose=False)\n",
        "\n",
        "            # Evaluate the model on the training data\n",
        "            model.eval()\n",
        "            with torch.no_grad():\n",
        "                # Training accuracy\n",
        "                train_preds = model(torch.tensor(X_train, dtype=torch.float32))\n",
        "                train_preds = torch.sigmoid(train_preds).round()  # Apply sigmoid and round to get binary predictions\n",
        "                train_acc = accuracy_score(y_train, train_preds.numpy())\n",
        "                train_accs.append(train_acc)\n",
        "\n",
        "                # Validation accuracy\n",
        "                val_preds = model(torch.tensor(X_val, dtype=torch.float32))\n",
        "                val_preds = torch.sigmoid(val_preds).round()  # Apply sigmoid and round to get binary predictions\n",
        "                val_acc = accuracy_score(y_val, val_preds.numpy())\n",
        "                val_accs.append(val_acc)\n",
        "\n",
        "        # For each hyper-parameter, calculate the mean and standard error of accuracy\n",
        "        train_std, train_mean = torch.std_mean(torch.tensor(train_accs))\n",
        "        val_std, val_mean = torch.std_mean(torch.tensor(val_accs))\n",
        "\n",
        "        # Standard error is standard deviation / sqrt(n), it is more typical to report this\n",
        "        rootn = torch.sqrt(torch.tensor(n_folds))  # n is number of folds\n",
        "        train_se, val_se = train_std / rootn, val_std / rootn\n",
        "\n",
        "        # Storing learning rate, weight decay value, train mean accuracy, standard error, val mean accuracy, standard error\n",
        "        results.append((lr, wd, train_mean.item(), train_se.item(), val_mean.item(), val_se.item()))\n",
        "\n",
        "# Display the results\n",
        "for result in results:\n",
        "    print(f\"LR: {result[0]}, WD: {result[1]}, Train Mean: {result[2]:.4f}, Train SE: {result[3]:.4f}, \"\n",
        "          f\"Val Mean: {result[4]:.4f}, Val SE: {result[5]:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GutMxucZM8ku"
      },
      "source": [
        "## Show result [3 points]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Assuming 'results' contains the results from the previous cross-validation code\n",
        "# Example: results = [(0.001, 0.0, 0.95, 0.02, 0.82, 0.05), ...]\n",
        "\n",
        "# Create a DataFrame from the list of tuples, with labeled columns\n",
        "column_names = ['learning_rate', 'weight_decay', 'train_mean', 'train_se','val_mean', 'val_se']\n",
        "df = pd.DataFrame(results, columns=column_names)\n",
        "\n",
        "# Make pretty printable strings, with standard error bars\n",
        "df['train_output'] = df.apply(lambda row: f\"{row['train_mean']:.3f} +/- {row['train_se']:.3f}\", axis=1)\n",
        "df['val_output'] = df.apply(lambda row: f\"{row['val_mean']:.3f} +/- {row['val_se']:.3f}\", axis=1)\n",
        "\n",
        "# Print the training results\n",
        "print('Training results')\n",
        "pivot_df = df.pivot(index='weight_decay', columns='learning_rate', values='train_output')\n",
        "display(pivot_df)\n",
        "\n",
        "# Print the validation results\n",
        "print('Validation results')\n",
        "pivot_df = df.pivot(index='weight_decay', columns='learning_rate', values='val_output')\n",
        "display(pivot_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "id": "YxHRr6PshQFs",
        "outputId": "29257370-7a4b-4c82-9f6f-b2aeb22c0661"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training results\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "learning_rate            0.001            0.010            0.100\n",
              "weight_decay                                                    \n",
              "0.00           0.782 +/- 0.003  0.998 +/- 0.000  1.000 +/- 0.000\n",
              "0.01           0.786 +/- 0.004  0.998 +/- 0.000  0.995 +/- 0.003"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f38ea201-9884-4b2c-843a-5b8a6340e8b5\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th>learning_rate</th>\n",
              "      <th>0.001</th>\n",
              "      <th>0.010</th>\n",
              "      <th>0.100</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>weight_decay</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0.00</th>\n",
              "      <td>0.782 +/- 0.003</td>\n",
              "      <td>0.998 +/- 0.000</td>\n",
              "      <td>1.000 +/- 0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0.01</th>\n",
              "      <td>0.786 +/- 0.004</td>\n",
              "      <td>0.998 +/- 0.000</td>\n",
              "      <td>0.995 +/- 0.003</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f38ea201-9884-4b2c-843a-5b8a6340e8b5')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-f38ea201-9884-4b2c-843a-5b8a6340e8b5 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-f38ea201-9884-4b2c-843a-5b8a6340e8b5');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-4f25ed72-9c44-4e5d-8f71-68cc544963f2\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-4f25ed72-9c44-4e5d-8f71-68cc544963f2')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-4f25ed72-9c44-4e5d-8f71-68cc544963f2 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_42004599-3543-4c3a-86db-c27392ead977\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('pivot_df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_42004599-3543-4c3a-86db-c27392ead977 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('pivot_df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "pivot_df",
              "summary": "{\n  \"name\": \"pivot_df\",\n  \"rows\": 2,\n  \"fields\": [\n    {\n      \"column\": \"weight_decay\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.007071067811865475,\n        \"min\": 0.0,\n        \"max\": 0.01,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.01,\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 0.001,\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"0.786 +/- 0.004\",\n          \"0.782 +/- 0.003\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 0.01,\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"0.998 +/- 0.000\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 0.1,\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"0.995 +/- 0.003\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation results\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "learning_rate            0.001            0.010            0.100\n",
              "weight_decay                                                    \n",
              "0.00           0.738 +/- 0.012  0.805 +/- 0.007  0.817 +/- 0.007\n",
              "0.01           0.751 +/- 0.003  0.809 +/- 0.013  0.823 +/- 0.003"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e62d48ce-42c5-40fa-9aeb-08f3452b03f9\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th>learning_rate</th>\n",
              "      <th>0.001</th>\n",
              "      <th>0.010</th>\n",
              "      <th>0.100</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>weight_decay</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0.00</th>\n",
              "      <td>0.738 +/- 0.012</td>\n",
              "      <td>0.805 +/- 0.007</td>\n",
              "      <td>0.817 +/- 0.007</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0.01</th>\n",
              "      <td>0.751 +/- 0.003</td>\n",
              "      <td>0.809 +/- 0.013</td>\n",
              "      <td>0.823 +/- 0.003</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e62d48ce-42c5-40fa-9aeb-08f3452b03f9')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-e62d48ce-42c5-40fa-9aeb-08f3452b03f9 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-e62d48ce-42c5-40fa-9aeb-08f3452b03f9');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-11a7fab6-e210-44c8-97fb-9721ddba19ce\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-11a7fab6-e210-44c8-97fb-9721ddba19ce')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-11a7fab6-e210-44c8-97fb-9721ddba19ce button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_98b01cd5-5284-40e8-b60f-127a3f044723\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('pivot_df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_98b01cd5-5284-40e8-b60f-127a3f044723 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('pivot_df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "pivot_df",
              "summary": "{\n  \"name\": \"pivot_df\",\n  \"rows\": 2,\n  \"fields\": [\n    {\n      \"column\": \"weight_decay\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.007071067811865475,\n        \"min\": 0.0,\n        \"max\": 0.01,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.01,\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 0.001,\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"0.751 +/- 0.003\",\n          \"0.738 +/- 0.012\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 0.01,\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"0.809 +/- 0.013\",\n          \"0.805 +/- 0.007\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 0.1,\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"0.823 +/- 0.003\",\n          \"0.817 +/- 0.007\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jrktjPMkM8ku"
      },
      "source": [
        "### Exporting the Notebook to PDF\n",
        "To generate a nice looking PDF of your completed notebook, either use \"Print\" as PDF from Google colab (using Chrome), or if you are running locally, run the following command in the last cell:\n",
        "```python\n",
        "!jupyter nbconvert --to pdf --output=yourname_submission.pdf hw1.ipynb\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_IGx4vkuM8ku"
      },
      "source": [
        "## Extra credit\n",
        "\n",
        "There are some nice opportunities for extra credit, though I will be fairly stingy with the points, so you should only try it if you're interested in learning more.\n",
        "Some examples of things you could try for 1 extra point (and if you do multiple there will be a maximum of 2 extra points).\n",
        "- Use t-SNE or UMAP to visualize a 2-d embedding of all the points, and see if the real and fake images are separable in the 2-d space.\n",
        "- Use a more complex vision backbone like a pretrained ResNet to first embed the images, then train your MLP. You'll have to be careful to transform the images before input into a ResNet, as they usually expect a specific resolution. You can use torchvision transforms library for this. Does this increase accuracy? I don't know, but I speculate it won't help much - these embeddings are trained for classification accuracy, so they have no reason to preserve differences that are useful for finding fakes.\n",
        "- Train a more complex vision backbone, instead of using the MLP. Again, a ResNet or a small vision transformer would be interesting. I think this would be the most typical and effective approach.\n",
        "- Being an expert at hyper-parameter tuning is a skill that will benefit you greatly. Try a more fancy way to do this, like https://docs.wandb.ai, and see how well you can do on this assigment if you also vary other hyper-parameters (architecture, n_epochs, maybe early stopping, more learning rate/weight decay settings, regularizers, etc.)\n",
        "\n",
        "Of course you can train your deepfake detector on my fakes, but how well will it do on ones from some other system? This is the fundamental research question in that field - how to build robust detectors that will work well even on new image generators."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}